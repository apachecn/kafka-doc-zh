<!--
 Licensed to the Apache Software Foundation (ASF) under one or more
 contributor license agreements.  See the NOTICE file distributed with
 this work for additional information regarding copyright ownership.
 The ASF licenses this file to You under the Apache License, Version 2.0
 (the "License"); you may not use this file except in compliance with
 the License.  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

 Unless required by applicable law or agreed to in writing, software
 distributed under the License is distributed on an "AS IS" BASIS,
 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 See the License for the specific language governing permissions and
 limitations under the License.
-->

<script id="security-template" type="text/x-handlebars-template">
    <h3><a id="security_overview" href="#security_overview">7.1 Security Overview</a></h3>
    In release 0.9.0.0, the Kafka community added a number of features that, used either separately or together, increases security in a Kafka cluster. The following security measures are currently supported:
    <ol>
        <li>Authentication of connections to brokers from clients (producers and consumers), other brokers and tools, using either SSL or SASL. Kafka supports the following SASL mechanisms:
        <ul>
            <li>SASL/GSSAPI (Kerberos) - starting at version 0.9.0.0</li>
            <li>SASL/PLAIN - starting at version 0.10.0.0</li>
            <li>SASL/SCRAM-SHA-256 and SASL/SCRAM-SHA-512 - starting at version 0.10.2.0</li>
        </ul></li>
        <li>Authentication of connections from brokers to ZooKeeper</li>
        <li>Encryption of data transferred between brokers and clients, between brokers, or between brokers and tools using SSL (Note that there is a performance degradation when SSL is enabled, the magnitude of which depends on the CPU type and the JVM implementation.)</li>
        <li>Authorization of read / write operations by clients</li>
        <li>Authorization is pluggable and integration with external authorization services is supported</li>
    </ol>

    It's worth noting that security is optional - non-secured clusters are supported, as well as a mix of authenticated, unauthenticated, encrypted and non-encrypted clients.

    The guides below explain how to configure and use the security features in both clients and brokers.

    <h3><a id="security_ssl" href="#security_ssl">7.2 Encryption and Authentication using SSL</a></h3>
    Apache Kafka allows clients to connect over SSL. By default, SSL is disabled but can be turned on as needed.

    <ol>
        <li><h4><a id="security_ssl_key" href="#security_ssl_key">Generate SSL key and certificate for each Kafka broker</a></h4>
            The first step of deploying one or more brokers with the SSL support is to generate the key and the certificate for each machine in the cluster. You can use Java's keytool utility to accomplish this task.
            We will generate the key into a temporary keystore initially so that we can export and sign it later with CA.
            <pre class="brush: bash;">
            keytool -keystore server.keystore.jks -alias localhost -validity {validity} -genkey -keyalg RSA</pre>

            You need to specify two parameters in the above command:
            <ol>
                <li>keystore: the keystore file that stores the certificate. The keystore file contains the private key of the certificate; therefore, it needs to be kept safely.</li>
                <li>validity: the valid time of the certificate in days.</li>
            </ol>
            <br>
        Note: By default the property <code>ssl.endpoint.identification.algorithm</code> is not defined, so hostname verification is not performed. In order to enable hostname verification, set the following property:

        <pre class="brush: text;">	ssl.endpoint.identification.algorithm=HTTPS </pre>

        Once enabled, clients will verify the server's fully qualified domain name (FQDN) against one of the following two fields:
        <ol>
            <li>Common Name (CN)
            <li>Subject Alternative Name (SAN)
        </ol>
        <br>
        Both fields are valid, RFC-2818 recommends the use of SAN however. SAN is also more flexible, allowing for multiple DNS entries to be declared. Another advantage is that the CN can be set to a more meaningful value for authorization purposes. To add a SAN field  append the following argument <code> -ext SAN=DNS:{FQDN} </code> to the keytool command:
        <pre class="brush: bash;">
        keytool -keystore server.keystore.jks -alias localhost -validity {validity} -genkey -keyalg RSA -ext SAN=DNS:{FQDN}
        </pre>
        The following command can be run afterwards to verify the contents of the generated certificate:
        <pre class="brush: bash;">
        keytool -list -v -keystore server.keystore.jks
        </pre>
        </li>
        <li><h4><a id="security_ssl_ca" href="#security_ssl_ca">创建你自己的证书管理机构 (CA)</a></h4>
            在完成第一步之后, 集群中的每一台机器都有一个公私密钥对, 和一个用于标识该机器的证书. 但是, 这个证书是未签名的, 也就是说攻击者也可以创造一个这样的证书, 假装是其中任何一台机器.<p>
            因此, 给集群中每个机器的证书签名, 来防止伪造的证书, 是非常重要的. 一个证书管理机构 (CA) 负责证书的签名. CA 的工作流程类似政府发放护照. 政府给每一个护照盖章(签名), 因此护照变得难以伪造. 其他政府通过验证这个盖章来确认护照是可信的. 同理, CA 给证书签名, 并且加密方法保证已经签名的证书是难以计算和伪造的. 所以, 只要 CA 是真实的可信的权威机构, 客户端能更好的保证他们连接到的是可信的机器.
            <pre class="brush: bash;">
            openssl req -new -x509 -keyout ca-key -out ca-cert -days 365</pre>

            生成的 CA 是一个简单的公私密钥对和证书, 然后, 它将对其他证书签名.<br>

            下一步是将生成的 CA 添加到 **客户端的信任存储区 (clients' truststore)**, 这样客户端才可以相信这个 CA:
            <pre class="brush: bash;">
            keytool -keystore client.truststore.jks -alias CARoot -import -file ca-cert</pre>

            <b>Note:</b> 如果你将 Kafka broker 配置成需要客户端认证, 可以在 <a href="#config_broker">Kafka broker 配置</a>中设置 setting ssl.client.auth 属性为 "requested" 或者 "required". 然后你也一定要给 Kafka broker 提供一个信任存储区, 并且这个信任存储区应该拥有所有给客户端密钥签名的 CA 证书.
            <pre class="brush: bash;">
            keytool -keystore server.truststore.jks -alias CARoot -import -file ca-cert</pre>

            相对于第一步中的密钥库 (keystore) 存储每个机器自己的标识, 客户端的信任存储区 (truststore) 保存所有客户端需要信任的证书. 导入一个证书到一台机器的信任存储区, 意味着这台机器将信任所有被这个证书签名的证书. 与上面相似的, 相信政府 (CA) 即相信政府发放的所有护照(证书). 这一特征称为信任链, 当在大型的 Kafka 集群中部署 SSL 时, 这一特征特别有用. 你可以用一个单一的 CA 给集群中的所有证书签名, 而且可以让所有机器共享信任该 CA 的信任存储区. 这样, 每个机器都可以认证除自己之外的全部机器.</li>

        <li><h4><a id="security_ssl_signing" href="#security_ssl_signing">给证书签名</a></h4>
            下一步是用第二步生成的 CA 给第一步生成的所有证书签名. 首先, 你需要从密钥库中把证书导出来: 
            <pre class="brush: bash;">
            keytool -keystore server.keystore.jks -alias localhost -certreq -file cert-file</pre>

            然后用 CA 给导出的证书签名:
            <pre class="brush: bash;">
            openssl x509 -req -CA ca-cert -CAkey ca-key -in cert-file -out cert-signed -days {validity} -CAcreateserial -passin pass:{ca-password}</pre>

            最后, 你要把 CA 的证书和被签名的证书一起导入密钥库中:
            <pre class="brush: bash;">
            keytool -keystore server.keystore.jks -alias CARoot -import -file ca-cert
            keytool -keystore server.keystore.jks -alias localhost -import -file cert-signed</pre>

            参数的定义如下所示:
            <ol>
                <li>keystore: 密钥库的地址</li>
                <li>ca-cert: CA 的证书</li>
                <li>ca-key: CA 的私钥</li>
                <li>ca-password: CA 的密码</li>
                <li>cert-file: 从服务器导出的未被签名的证书</li>
                <li>cert-signed: 已经签名的服务器的证书</li>
            </ol>

            这是一个集合上面所有步骤, 用 bash 写的例子. 注意其中一条命令假设密码是 `test1234`, 所以你要么使用这个密码, 要么在执行命令前修改该密码:
            <pre>
            #!/bin/bash
            #Step 1
            keytool -keystore server.keystore.jks -alias localhost -validity 365 -keyalg RSA -genkey
            #Step 2
            openssl req -new -x509 -keyout ca-key -out ca-cert -days 365
            keytool -keystore server.truststore.jks -alias CARoot -import -file ca-cert
            keytool -keystore client.truststore.jks -alias CARoot -import -file ca-cert
            #Step 3
            keytool -keystore server.keystore.jks -alias localhost -certreq -file cert-file
            openssl x509 -req -CA ca-cert -CAkey ca-key -in cert-file -out cert-signed -days 365 -CAcreateserial -passin pass:test1234
            keytool -keystore server.keystore.jks -alias CARoot -import -file ca-cert
            keytool -keystore server.keystore.jks -alias localhost -import -file cert-signed</pre></li>

        <li><h4><a id="security_configbroker" href="#security_configbroker">配置 Kafka Broker</a></h4>
            Kafka Broker 支持在多个端口上监听连接.
            我们需要在 server.properties 中配置以下属性, 必须有一个或多个用逗号隔开的值:
            <pre>listeners</pre>

            如果 broker 之间的通信没有启用 SSL (如何启用可参照下文). PLAINTEXT 和 SSL 两个协议都需要提供端口信息.
            <pre class="brush: text;">
            listeners=PLAINTEXT://host.name:port,SSL://host.name:port</pre>

            下面是 broker 端需要的 SSL 的配置:
            <pre class="brush: text;">
            ssl.keystore.location=/var/private/ssl/server.keystore.jks
            ssl.keystore.password=test1234
            ssl.key.password=test1234
            ssl.truststore.location=/var/private/ssl/server.truststore.jks
            ssl.truststore.password=test1234</pre>

            Note: 严格来讲 ssl.truststore.password 是可选配置的, 但是强烈建议配置. 如果没有配置 password, 依然可以访问信任存储区 (truststore), 但是不能进行真实性检查.

            值得考虑的可选配置:
            <ol>
                <li>ssl.client.auth=none ("required" => 客户端认证是必须的, "requested" => 客户端认证是需要的, 但是没有证书依然可以连接. 因为 "requested" 会造成错误的安全感, 而且在客户端配置错误的情况下依然可以连接成功, 所以不鼓励使用.)</li>
                <li>ssl.cipher.suites (可选的). 指定的密码套件, 由认证, 加密, MAC 和密钥交换算法组成, 用于给使用 TLS 或者 SSL 协议的网络协商安全设置. (默认是空的 list)</li>
                <li>ssl.enabled.protocols=TLSv1.2,TLSv1.1,TLSv1 (列出你将要从客户端接收的 SSL 协议. 注意, SSL 已经废弃, 支持 TLS, 并且不建议在生产环境中使用 SSL.)</li>
                <li>ssl.keystore.type=JKS</li>
                <li>ssl.truststore.type=JKS</li>
                <li>ssl.secure.random.implementation=SHA1PRNG</li>
            </ol>
            如果你想 broker 间的通信启用 SSL, 将下面内容添加到 server.properties 文件中(默认是 PLAINTEXT ):
            <pre>
            security.inter.broker.protocol=SSL</pre>

            <p>
            由于一些国家的进口规定, Oracle 的实现限制了默认加密算法的强度.
            如果需要更强的算法 (例如, 256 位的 AES 密钥), 必须获得 <a href="http://www.oracle.com/technetwork/java/javase/downloads/index.html">JCE Unlimited Strength Jurisdiction Policy 文件</a>, 并且在 JDK/JRE 中安装.
            参考 <a href="https://docs.oracle.com/javase/8/docs/technotes/guides/security/SunProviders.html">JCA Providers 文档</a> 获取更多新.
            </p>

            <p>
            JRE/JDK 将有一个默认的伪随机数生成器 (PRNG), 用于加密操作, 所以没有要求使用 <pre>ssl.secure.random.implementation</pre> 配置 PRNG 的实现.
            不过, 某些实现会造成性能问题 (尤其是, Linux 系统的默认选择, <pre>NativePRNG</pre>, 利用了一个全局锁.)
            万一 SSL 连接的性能变成一个问题, 可以考虑, 明确的设置使用的 PRNG 实现.
            <pre>SHA1PRNG</pre> 实现是非阻塞的, 在高负载 (加上复制消息的流量, 每个 broker 生产消息的速度是 50 MB/sec) 下有非常好的性能表现.
            </p>

            一旦你启动了 broker, 你应该能在 server.log 中看到如下内容:
            <pre>
            with addresses: PLAINTEXT -> EndPoint(192.168.64.1,9092,PLAINTEXT),SSL -> EndPoint(192.168.64.1,9093,SSL)</pre>

            用以下命令可以快速验证服务器的 keystore 和 truststore 是否设置正确:
            <pre>openssl s_client -debug -connect localhost:9093 -tls1</pre> (Note: TLSv1 需要在 ssl.enabled.protocols 中列出)<br>
            在命令的输出中, 你应该能看到服务器的证书:
            <pre>
            -----BEGIN CERTIFICATE-----
            {variable sized random bytes}
            -----END CERTIFICATE-----
            subject=/C=US/ST=CA/L=Santa Clara/O=org/OU=org/CN=Sriharsha Chintalapani
            issuer=/C=US/ST=CA/L=Santa Clara/O=org/OU=org/CN=kafka/emailAddress=test@test.com</pre>
            如果没有显示证书信息, 或者有任何其他错误信息, 那么你的 keystore 设置不正确。</li>

        <li><h4><a id="security_configclients" href="#security_configclients">配置 Kafka 客户端</a></h4>
            只有新的 Kafka Producer 和 Consumer 支持 SSL, 旧的 API 不支持 SSL. Producer 和 Consumer 的 SSL 的配置相同.
            <br>
            如果在 broker 中不需要客户端认证, 那么下面是最小的配置的例子:
            <pre class="brush: text;">
            security.protocol=SSL
            ssl.truststore.location=/var/private/ssl/client.truststore.jks
            ssl.truststore.password=test1234</pre>

            Note: ssl.truststore.password 严格来讲是可选配置的, 但是强烈建议配置. 如果没有配置 password, 依然可以访问 truststore, 但是不能进行真实性检查.

            如果要求客户端认证, 必须像第一步一样创建一个 keystore, 和配置下面这些信息:
            <pre class="brush: text;">
            ssl.keystore.location=/var/private/ssl/client.keystore.jks
            ssl.keystore.password=test1234
            ssl.key.password=test1234</pre>
			
            根据我们的需求和 broker 的配置, 也需要设置其他的配置:
                <ol>
                    <li>ssl.provider (可选的). 用于 SSL 连接的安全提供程序名称. 默认值是 JVM 的默认安全提供程序.</li>
                    <li>ssl.cipher.suites (可选的). 指定的密码套件, 由认证, 加密, MAC 和密钥交换算法组成, 用于给使用 TLS 或者 SSL 协议的网络协商安全设置.</li>
                    <li>ssl.enabled.protocols=TLSv1.2,TLSv1.1,TLSv1. 需要列出至少一个在 broker 端配置的协议</li>
                    <li>ssl.truststore.type=JKS</li>
                    <li>ssl.keystore.type=JKS</li>
                </ol>
    <br>
            使用 console-producer 和 console-consumer 的例子:
            <pre class="brush: bash;">
            kafka-console-producer.sh --broker-list localhost:9093 --topic test --producer.config client-ssl.properties
            kafka-console-consumer.sh --bootstrap-server localhost:9093 --topic test --consumer.config client-ssl.properties</pre>
        </li>
    </ol>
    <h3><a id="security_sasl" href="#security_sasl">7.3 使用SASL实现身份验证</a></h3>

    <ol>
    <li><h4><a id="security_sasl_jaasconfig" href="#security_sasl_jaasconfig">JAAS 的配置</a></h4>
    <p>Kafka 使用Java验证和授权API
    (<a href="https://docs.oracle.com/javase/8/docs/technotes/guides/security/jaas/JAASRefGuide.html">JAAS</a>)
    来完成 SASL 配置.</p>
        <ol>
        <li><h5><a id="security_jaas_broker"
            href="#security_jaas_broker">Kafka brokers 的 JAAS 的配置</a></h5>

            <p><tt>KafkaServer</tt> 是每一个 KafkaServer/Broker 的 JASS 文件里面的节点名称。
                在这个节点中，提供了用于所有 brokers 之间通信的 SASL 客户端连接的 SASL 配置选项。</p>

            <p><tt>Client</tt> 节点是用于认证 SASL 与 zookeeper 之间的连接。它也允许 brokers 通过设定 zookeeper 节点中 SASL ACL
                来锁定这些节点 从而确定这些节点不被其他 broker 修改。
                在所有的 broker 中都必须使用相同的名称。 如果您想使用 Client 以外名称, 请设置系统属性
                <tt>zookeeper.sasl.clientconfig</tt> 中填写合适的名称
                (<i>e.g.</i>, <tt>-Dzookeeper.sasl.clientconfig=ZkClient</tt>).</p>

            <p>ZooKeeper 使用 "zookeeper" 作为默认的名称. 如果您想改变它的话, 请设置系统属性
            <tt>zookeeper.sasl.client.username</tt> 中填写合适的名称
            (<i>e.g.</i>, <tt>-Dzookeeper.sasl.client.username=zk</tt>).</p></li>

        <li><h5><a id="security_jaas_client"
            href="#security_jaas_client">Kafka clients 的 JAAS 的配置</a></h5>

            <p>Client 通过配置客户端属性 <a href="#security_client_dynamicjaas">sasl.jaas.config</a>
                或者通过与 brokers 相似的方法来配置 JAAS （<a href="#security_client_staticjaas">static JAAS config file</a>）
            </p>

            <ol>
            <li><h6><a id="security_client_dynamicjaas"
                href="#security_client_dynamicjaas">通过客户端配置属性来配置JAAS</a></h6>
                <p>客户无需创建物理文件来配置角色 只需要在 JASS 配置中指定 producer 或者 consumer。
                    这种模式通过为每个客户端指定不同的属性来使用不同的凭证，从而确保可以在在同一个 JVM 中使用不同的 producers
                    和 consumers。如果静态JAAS配置系统属性的方法
                <code>java.security.auth.login.config</code> 和配置客户端属性 <code>sasl.jaas.config</code>的方法
              同时被使用, 客户端的配置将会被使用.</p>
                <p>请看 <a href="#security_sasl_kerberos_clientconfig">GSSAPI (Kerberos)</a>,
                <a href="#security_sasl_plain_clientconfig">PLAIN</a> or
                <a href="#security_sasl_scram_clientconfig">SCRAM</a> 中的示例配置。</p></li>

                <li><h6><a id="security_client_staticjaas" href="#security_client_staticjaas">通过静态配置文件来配置 JAAS</a></h6>
                    使用静态 JAAS 配置文件 来配置 SASL 的客户端认证服务：
                <ol>
                <li>添加一个名为 <tt>KafkaClient</tt> 客户端的登陆节点，同时为 <tt>KafkaClient</tt> 中所选机制来配置一个登陆模块
                    如设置<a href="#security_sasl_kerberos_clientconfig">GSSAPI (Kerberos)</a>,
                    <a href="#security_sasl_plain_clientconfig">PLAIN</a> 或者
                    <a href="#security_sasl_scram_clientconfig">SCRAM</a>。
                    例如, <a href="#security_sasl_gssapi_clientconfig">GSSAPI</a>
                    的凭证配置如下:
                    <pre class="brush: text;">
        KafkaClient {
        com.sun.security.auth.module.Krb5LoginModule required
        useKeyTab=true
        storeKey=true
        keyTab="/etc/security/keytabs/kafka_client.keytab"
        principal="kafka-client-1@EXAMPLE.COM";
    };</pre>
                </li>
                <li>将 JASS 的配置文件位置作为 JVM 的参数传递给每个客户端的 JVM。 例如:
                    <pre class="brush: bash;">    -Djava.security.auth.login.config=/etc/kafka/kafka_client_jaas.conf</pre></li>
	</ol>
                </li>
            </ol>
            </li>
        </ol>
    </li>
    <li><h4><a id="security_sasl_config"
               href="#security_sasl_config">SASL 的配置</a></h4>

        <p> SASL 可以使用 PLAINTEXT 或者 SSL 协议作为传输层，相对应的就是使用 SASL_PLAINTEXT 或者 SASL_SSL 安全协议。
            如果使用 SASL_SSL 安全协议，必须配置 <a href="#security_ssl">SSL证书</a>。</p>

        <ol>
            <li><h5><a id="security_sasl_mechanism"
                       href="#security_sasl_mechanism">SASL 安全机制</a></h5>
                Kafka 支持以下的四个 SASL 机制:
                <ul>
                    <li><a href="#security_sasl_kerberos">GSSAPI</a> (Kerberos)</li>
                    <li><a href="#security_sasl_plain">PLAIN</a></li>
                    <li><a href="#security_sasl_scram">SCRAM-SHA-256</a></li>
                    <li><a href="#security_sasl_scram">SCRAM-SHA-512</a></li>
                </ul>
            </li>
            <li><h5><a id="security_sasl_brokerconfig"
                       href="#security_sasl_brokerconfig">Kafka brokers 的SASL配置</a></h5>
                <ol>
                    <li>在 server.properties 文件中配置一个 SASL 端口， 要为 <i>listeners</i> 添加其中至少一个参数（SASL_PLAINTEXT 或者 SASL_SSL）：
                        <pre>    listeners=SASL_PLAINTEXT://host.name:port</pre>
                        如果您只配置一个 SASL 端口 （或者您只想希望在 Kafka brokersz 之间使用 SASL 协议相互认证的话）您需要确保您在 borker 之间通信中 使用了了相同的 SASL 协议。
                        <pre>    security.inter.broker.protocol=SASL_PLAINTEXT (or SASL_SSL)</pre></li>
                    <li>选择一个或者多个 <a href="#security_sasl_mechanism">支持的安全机制</a>
                        然后按照所给的步骤来为所选的安全机制配置 SASL 协议。 如果您想在 broker 中启用多个安全机制， 请根据<a href="#security_sasl_multimechanism">此处</a>步骤操作。
                </ol>
            </li>
            <li><h5><a id="security_sasl_clientconfig"
                       href="#security_sasl_clientconfig">Kafka 客户端配置 SASL</a></h5>
                <p> SASL 授权只在新的 Java Kafka producer and consumer API 中被支持。之前的API并不支持</p>

                <p>要在客户端配置 SASL 授权， 需在 broker 中选择一个已启用的<a href="#security_sasl_mechanism">SASL 机制</a>用于客户端授权
                    并且根据下面的步骤为所选的安全机制配置 SASL 协议。</p>
            </li>
        </ol>
    </li>
    <li><h4><a id="security_sasl_kerberos" href="#security_sasl_kerberos">Authentication using SASL/Kerberos</a></h4>
        <ol>
        <li><h5><a id="security_sasl_kerberos_prereq" href="#security_sasl_kerberos_prereq">Prerequisites</a></h5>
        <ol>
            <li><b>Kerberos</b><br>
            If your organization is already using a Kerberos server (for example, by using Active Directory), there is no need to install a new server just for Kafka. Otherwise you will need to install one, your Linux vendor likely has packages for Kerberos and a short guide on how to install and configure it (<a href="https://help.ubuntu.com/community/Kerberos">Ubuntu</a>, <a href="https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Managing_Smart_Cards/installing-kerberos.html">Redhat</a>). Note that if you are using Oracle Java, you will need to download JCE policy files for your Java version and copy them to $JAVA_HOME/jre/lib/security.</li>
            <li><b>Create Kerberos Principals</b><br>
            If you are using the organization's Kerberos or Active Directory server, ask your Kerberos administrator for a principal for each Kafka broker in your cluster and for every operating system user that will access Kafka with Kerberos authentication (via clients and tools).</br>
            If you have installed your own Kerberos, you will need to create these principals yourself using the following commands:
                <pre class="brush: bash;">
        sudo /usr/sbin/kadmin.local -q 'addprinc -randkey kafka/{hostname}@{REALM}'
        sudo /usr/sbin/kadmin.local -q "ktadd -k /etc/security/keytabs/{keytabname}.keytab kafka/{hostname}@{REALM}"</pre></li>
            <li><b>Make sure all hosts can be reachable using hostnames</b> - it is a Kerberos requirement that all your hosts can be resolved with their FQDNs.</li>
        </ol>
        <li><h5><a id="security_sasl_kerberos_brokerconfig" href="#security_sasl_kerberos_brokerconfig">Configuring Kafka Brokers</a></h5>
        <ol>
            <li>Add a suitably modified JAAS file similar to the one below to each Kafka broker's config directory, let's call it kafka_server_jaas.conf for this example (note that each broker should have its own keytab):
            <pre class="brush: text;">
        KafkaServer {
            com.sun.security.auth.module.Krb5LoginModule required
            useKeyTab=true
            storeKey=true
            keyTab="/etc/security/keytabs/kafka_server.keytab"
            principal="kafka/kafka1.hostname.com@EXAMPLE.COM";
        };

        // Zookeeper client authentication
        Client {
        com.sun.security.auth.module.Krb5LoginModule required
        useKeyTab=true
        storeKey=true
        keyTab="/etc/security/keytabs/kafka_server.keytab"
        principal="kafka/kafka1.hostname.com@EXAMPLE.COM";
        };</pre>

            </li>
            <tt>KafkaServer</tt> section in the JAAS file tells the broker which principal to use and the location of the keytab where this principal is stored. It
            allows the broker to login using the keytab specified in this section. See <a href="#security_sasl_brokernotes">notes</a> for more details on Zookeeper SASL configuration.
            <li>Pass the JAAS and optionally the krb5 file locations as JVM parameters to each Kafka broker (see <a href="https://docs.oracle.com/javase/8/docs/technotes/guides/security/jgss/tutorials/KerberosReq.html">here</a> for more details):
                <pre>    -Djava.security.krb5.conf=/etc/kafka/krb5.conf
        -Djava.security.auth.login.config=/etc/kafka/kafka_server_jaas.conf</pre>
            </li>
            <li>Make sure the keytabs configured in the JAAS file are readable by the operating system user who is starting kafka broker.</li>
            <li>Configure SASL port and SASL mechanisms in server.properties as described <a href="#security_sasl_brokerconfig">here</a>. For example:
            <pre>    listeners=SASL_PLAINTEXT://host.name:port
        security.inter.broker.protocol=SASL_PLAINTEXT
        sasl.mechanism.inter.broker.protocol=GSSAPI
        sasl.enabled.mechanisms=GSSAPI
            </pre>
            </li>We must also configure the service name in server.properties, which should match the principal name of the kafka brokers. In the above example, principal is "kafka/kafka1.hostname.com@EXAMPLE.com", so:
            <pre>    sasl.kerberos.service.name=kafka</pre>

        </ol></li>
        <li><h5><a id="security_sasl_kerberos_clientconfig" href="#security_kerberos_sasl_clientconfig">Configuring Kafka Clients</a></h5>
            To configure SASL authentication on the clients:
            <ol>
                <li>
                    Clients (producers, consumers, connect workers, etc) will authenticate to the cluster with their
                    own principal (usually with the same name as the user running the client), so obtain or create
                    these principals as needed. Then configure the JAAS configuration property for each client.
                    Different clients within a JVM may run as different users by specifiying different principals.
                    The property <code>sasl.jaas.config</code> in producer.properties or consumer.properties describes
                    how clients like producer and consumer can connect to the Kafka Broker. The following is an example
                    configuration for a client using a keytab (recommended for long-running processes):
                <pre>
    sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required \
        useKeyTab=true \
        storeKey=true  \
        keyTab="/etc/security/keytabs/kafka_client.keytab" \
        principal="kafka-client-1@EXAMPLE.COM";</pre>

                   For command-line utilities like kafka-console-consumer or kafka-console-producer, kinit can be used
                   along with "useTicketCache=true" as in:
                <pre>
    sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required \
        useTicketCache=true;</pre>

                   JAAS configuration for clients may alternatively be specified as a JVM parameter similar to brokers
                   as described <a href="#security_client_staticjaas">here</a>. Clients use the login section named
                   <tt>KafkaClient</tt>. This option allows only one user for all client connections from a JVM.</li>
                <li>Make sure the keytabs configured in the JAAS configuration are readable by the operating system user who is starting kafka client.</li>
                <li>Optionally pass the krb5 file locations as JVM parameters to each client JVM (see <a href="https://docs.oracle.com/javase/8/docs/technotes/guides/security/jgss/tutorials/KerberosReq.html">here</a> for more details):
                <pre>    -Djava.security.krb5.conf=/etc/kafka/krb5.conf</pre></li>
                <li>Configure the following properties in producer.properties or consumer.properties:
                <pre>
    security.protocol=SASL_PLAINTEXT (or SASL_SSL)
    sasl.mechanism=GSSAPI
    sasl.kerberos.service.name=kafka</pre></li>
            </ol>
        </li>
        </ol>
    </li>

    <li><h4><a id="security_sasl_plain" href="#security_sasl_plain">Authentication using SASL/PLAIN</a></h4>
        <p>SASL/PLAIN is a simple username/password authentication mechanism that is typically used with TLS for encryption to implement secure authentication.
        Kafka supports a default implementation for SASL/PLAIN which can be extended for production use as described <a href="#security_sasl_plain_production">here</a>.</p>
        The username is used as the authenticated <code>Principal</code> for configuration of ACLs etc.
        <ol>
        <li><h5><a id="security_sasl_plain_brokerconfig" href="#security_sasl_plain_brokerconfig">Configuring Kafka Brokers</a></h5>
            <ol>
            <li>Add a suitably modified JAAS file similar to the one below to each Kafka broker's config directory, let's call it kafka_server_jaas.conf for this example:
                <pre class="brush: text;">
        KafkaServer {
            org.apache.kafka.common.security.plain.PlainLoginModule required
            username="admin"
            password="admin-secret"
            user_admin="admin-secret"
            user_alice="alice-secret";
        };</pre>
                This configuration defines two users (<i>admin</i> and <i>alice</i>). The properties <tt>username</tt> and <tt>password</tt>
                in the <tt>KafkaServer</tt> section are used by the broker to initiate connections to other brokers. In this example,
                <i>admin</i> is the user for inter-broker communication. The set of properties <tt>user_<i>userName</i></tt> defines
                the passwords for all users that connect to the broker and the broker validates all client connections including
                those from other brokers using these properties.</li>
            <li>Pass the JAAS config file location as JVM parameter to each Kafka broker:
                <pre>    -Djava.security.auth.login.config=/etc/kafka/kafka_server_jaas.conf</pre></li>
            <li>Configure SASL port and SASL mechanisms in server.properties as described <a href="#security_sasl_brokerconfig">here</a>. For example:
                <pre>    listeners=SASL_SSL://host.name:port
        security.inter.broker.protocol=SASL_SSL
        sasl.mechanism.inter.broker.protocol=PLAIN
        sasl.enabled.mechanisms=PLAIN</pre></li>
            </ol>
        </li>

        <li><h5><a id="security_sasl_plain_clientconfig" href="#security_sasl_plain_clientconfig">Configuring Kafka Clients</a></h5>
            To configure SASL authentication on the clients:
            <ol>
            <li>Configure the JAAS configuration property for each client in producer.properties or consumer.properties.
                The login module describes how the clients like producer and consumer can connect to the Kafka Broker.
                The following is an example configuration for a client for the PLAIN mechanism:
                <pre class="brush: text;">
    sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required \
        username="alice" \
        password="alice-secret";</pre>
                <p>The options <tt>username</tt> and <tt>password</tt> are used by clients to configure
                the user for client connections. In this example, clients connect to the broker as user <i>alice</i>.
                Different clients within a JVM may connect as different users by specifying different user names
                and passwords in <code>sasl.jaas.config</code>.</p>

                <p>JAAS configuration for clients may alternatively be specified as a JVM parameter similar to brokers
                as described <a href="#security_client_staticjaas">here</a>. Clients use the login section named
                <tt>KafkaClient</tt>. This option allows only one user for all client connections from a JVM.</p></li>
            <li>Configure the following properties in producer.properties or consumer.properties:
                <pre>
    security.protocol=SASL_SSL
    sasl.mechanism=PLAIN</pre></li>
            </ol>
        </li>
        <li><h5><a id="security_sasl_plain_production" href="#security_sasl_plain_production">Use of SASL/PLAIN in production</a></h5>
            <ul>
            <li>SASL/PLAIN should be used only with SSL as transport layer to ensure that clear passwords are not transmitted on the wire without encryption.</li>
            <li>The default implementation of SASL/PLAIN in Kafka specifies usernames and passwords in the JAAS configuration file as shown
                <a href="#security_sasl_plain_brokerconfig">here</a>. To avoid storing passwords on disk, you can plug in your own implementation of
                <code>javax.security.auth.spi.LoginModule</code> that provides usernames and passwords from an external source. The login module implementation should
                provide username as the public credential and password as the private credential of the <code>Subject</code>. The default implementation
                <code>org.apache.kafka.common.security.plain.PlainLoginModule</code> can be used as an example.</li>
            <li>In production systems, external authentication servers may implement password authentication. Kafka brokers can be integrated with these servers by adding
                your own implementation of <code>javax.security.sasl.SaslServer</code>. The default implementation included in Kafka in the package
                <code>org.apache.kafka.common.security.plain</code> can be used as an example to get started.
                <ul>
                <li>New providers must be installed and registered in the JVM. Providers can be installed by adding provider classes to
                the normal <tt>CLASSPATH</tt> or bundled as a jar file and added to <tt><i>JAVA_HOME</i>/lib/ext</tt>.</li>
                <li>Providers can be registered statically by adding a provider to the security properties file
                <tt><i>JAVA_HOME</i>/lib/security/java.security</tt>.
                <pre>    security.provider.n=providerClassName</pre>
                where <i>providerClassName</i> is the fully qualified name of the new provider and <i>n</i> is the preference order with
                lower numbers indicating higher preference.</li>
                <li>Alternatively, you can register providers dynamically at runtime by invoking <code>Security.addProvider</code> at the beginning of the client
                application or in a static initializer in the login module. For example:
                <pre>    Security.addProvider(new PlainSaslServerProvider());</pre></li>
                <li>For more details, see <a href="http://docs.oracle.com/javase/8/docs/technotes/guides/security/crypto/CryptoSpec.html">JCA Reference</a>.</li>
                </ul>
            </li>
            </ul>
        </li>
        </ol>
    </li>

    <li><h4><a id="security_sasl_scram" href="#security_sasl_scram">Authentication using SASL/SCRAM</a></h4>
        <p>Salted Challenge Response Authentication Mechanism (SCRAM) is a family of SASL mechanisms that
        addresses the security concerns with traditional mechanisms that perform username/password authentication
        like PLAIN and DIGEST-MD5. The mechanism is defined in <a href="https://tools.ietf.org/html/rfc5802">RFC 5802</a>.
        Kafka supports <a href="https://tools.ietf.org/html/rfc7677">SCRAM-SHA-256</a> and SCRAM-SHA-512 which
        can be used with TLS to perform secure authentication. The username is used as the authenticated
        <code>Principal</code> for configuration of ACLs etc. The default SCRAM implementation in Kafka
        stores SCRAM credentials in Zookeeper and is suitable for use in Kafka installations where Zookeeper
        is on a private network. Refer to <a href="#security_sasl_scram_security">Security Considerations</a>
        for more details.</p>
        <ol>
        <li><h5><a id="security_sasl_scram_credentials" href="#security_sasl_scram_credentials">Creating SCRAM Credentials</a></h5>
        <p>The SCRAM implementation in Kafka uses Zookeeper as credential store. Credentials can be created in
        Zookeeper using <tt>kafka-configs.sh</tt>. For each SCRAM mechanism enabled, credentials must be created
        by adding a config with the mechanism name. Credentials for inter-broker communication must be created
        before Kafka brokers are started. Client credentials may be created and updated dynamically and updated
        credentials will be used to authenticate new connections.</p>
        <p>Create SCRAM credentials for user <i>alice</i> with password <i>alice-secret</i>:
        <pre class="brush: bash;">
    > bin/kafka-configs.sh --zookeeper localhost:2181 --alter --add-config 'SCRAM-SHA-256=[iterations=8192,password=alice-secret],SCRAM-SHA-512=[password=alice-secret]' --entity-type users --entity-name alice
        </pre>
        <p>The default iteration count of 4096 is used if iterations are not specified. A random salt is created
        and the SCRAM identity consisting of salt, iterations, StoredKey and ServerKey are stored in Zookeeper.
        See <a href="https://tools.ietf.org/html/rfc5802">RFC 5802</a> for details on SCRAM identity and the individual fields.
        <p>The following examples also require a user <i>admin</i> for inter-broker communication which can be created using:
        <pre class="brush: bash;">
    > bin/kafka-configs.sh --zookeeper localhost:2181 --alter --add-config 'SCRAM-SHA-256=[password=admin-secret],SCRAM-SHA-512=[password=admin-secret]' --entity-type users --entity-name admin
        </pre>
        <p>Existing credentials may be listed using the <i>--describe</i> option:
        <pre class="brush: bash;">
    > bin/kafka-configs.sh --zookeeper localhost:2181 --describe --entity-type users --entity-name alice
        </pre>
        <p>Credentials may be deleted for one or more SCRAM mechanisms using the <i>--delete</i> option:
        <pre class="brush: bash;">
    > bin/kafka-configs.sh --zookeeper localhost:2181 --alter --delete-config 'SCRAM-SHA-512' --entity-type users --entity-name alice
        </pre>
        </li>
        <li><h5><a id="security_sasl_scram_brokerconfig" href="#security_sasl_scram_brokerconfig">Configuring Kafka Brokers</a></h5>
            <ol>
            <li>Add a suitably modified JAAS file similar to the one below to each Kafka broker's config directory, let's call it kafka_server_jaas.conf for this example:
                <pre>
    KafkaServer {
        org.apache.kafka.common.security.scram.ScramLoginModule required
        username="admin"
        password="admin-secret";
    };</pre>
                The properties <tt>username</tt> and <tt>password</tt> in the <tt>KafkaServer</tt> section are used by
                the broker to initiate connections to other brokers. In this example, <i>admin</i> is the user for
                inter-broker communication.</li>
            <li>Pass the JAAS config file location as JVM parameter to each Kafka broker:
                <pre>    -Djava.security.auth.login.config=/etc/kafka/kafka_server_jaas.conf</pre></li>
            <li>Configure SASL port and SASL mechanisms in server.properties as described <a href="#security_sasl_brokerconfig">here</a>.</pre> For example:
                <pre>
    listeners=SASL_SSL://host.name:port
    security.inter.broker.protocol=SASL_SSL
    sasl.mechanism.inter.broker.protocol=SCRAM-SHA-256 (or SCRAM-SHA-512)
    sasl.enabled.mechanisms=SCRAM-SHA-256 (or SCRAM-SHA-512)</pre></li>
            </ol>
        </li>

        <li><h5><a id="security_sasl_scram_clientconfig" href="#security_sasl_scram_clientconfig">Configuring Kafka Clients</a></h5>
            To configure SASL authentication on the clients:
            <ol>
	    <li>Configure the JAAS configuration property for each client in producer.properties or consumer.properties.
                The login module describes how the clients like producer and consumer can connect to the Kafka Broker.
	        The following is an example configuration for a client for the SCRAM mechanisms:
                <pre class="brush: text;">
   sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required \
        username="alice" \
        password="alice-secret";</pre>

                <p>The options <tt>username</tt> and <tt>password</tt> are used by clients to configure
                the user for client connections. In this example, clients connect to the broker as user <i>alice</i>.
                Different clients within a JVM may connect as different users by specifying different user names
                and passwords in <code>sasl.jaas.config</code>.</p>

                <p>JAAS configuration for clients may alternatively be specified as a JVM parameter similar to brokers
                as described <a href="#security_client_staticjaas">here</a>. Clients use the login section named
                <tt>KafkaClient</tt>. This option allows only one user for all client connections from a JVM.</p></li>
            <li>Configure the following properties in producer.properties or consumer.properties:
                <pre>
    security.protocol=SASL_SSL
    sasl.mechanism=SCRAM-SHA-256 (or SCRAM-SHA-512)</pre></li>
            </ol>
        </li>
        <li><h5><a id="security_sasl_scram_security" href="#security_sasl_scram_security">Security Considerations for SASL/SCRAM</a></h5>
            <ul>
            <li>The default implementation of SASL/SCRAM in Kafka stores SCRAM credentials in Zookeeper. This
            is suitable for production use in installations where Zookeeper is secure and on a private network.</li>
            <li>Kafka supports only the strong hash functions SHA-256 and SHA-512 with a minimum iteration count
            of 4096. Strong hash functions combined with strong passwords and high iteration counts protect
            against brute force attacks if Zookeeper security is compromised.</li>
            <li>SCRAM should be used only with TLS-encryption to prevent interception of SCRAM exchanges. This
            protects against dictionary or brute force attacks and against impersonation if Zookeeper is compromised.</li>
            <li>The default SASL/SCRAM implementation may be overridden using custom login modules in installations
            where Zookeeper is not secure. See <a href="#security_sasl_plain_production">here</a> for details.</li>
            <li>For more details on security considerations, refer to
            <a href="https://tools.ietf.org/html/rfc5802#section-9">RFC 5802</a>.
            </ul>
        </li>
        </ol>
    </li>

    <li><h4><a id="security_sasl_multimechanism" href="#security_sasl_multimechanism">Enabling multiple SASL mechanisms in a broker</a></h4>
        <ol>
        <li>Specify configuration for the login modules of all enabled mechanisms in the <tt>KafkaServer</tt> section of the JAAS config file. For example:
            <pre>
        KafkaServer {
            com.sun.security.auth.module.Krb5LoginModule required
            useKeyTab=true
            storeKey=true
            keyTab="/etc/security/keytabs/kafka_server.keytab"
            principal="kafka/kafka1.hostname.com@EXAMPLE.COM";

            org.apache.kafka.common.security.plain.PlainLoginModule required
            username="admin"
            password="admin-secret"
            user_admin="admin-secret"
            user_alice="alice-secret";
        };</pre></li>
        <li>Enable the SASL mechanisms in server.properties: <pre>    sasl.enabled.mechanisms=GSSAPI,PLAIN,SCRAM-SHA-256,SCRAM-SHA-512</pre></li>
        <li>Specify the SASL security protocol and mechanism for inter-broker communication in server.properties if required:
            <pre>
    security.inter.broker.protocol=SASL_PLAINTEXT (or SASL_SSL)
    sasl.mechanism.inter.broker.protocol=GSSAPI (or one of the other enabled mechanisms)</pre></li>
        <li>Follow the mechanism-specific steps in <a href="#security_sasl_kerberos_brokerconfig">GSSAPI (Kerberos)</a>,
            <a href="#security_sasl_plain_brokerconfig">PLAIN</a> and <a href="#security_sasl_scram_brokerconfig">SCRAM</a>
            to configure SASL for the enabled mechanisms.</li>
        </ol>
    </li>
    <li><h4><a id="saslmechanism_rolling_upgrade" href="#saslmechanism_rolling_upgrade">Modifying SASL mechanism in a Running Cluster</a></h4>
        <p>SASL mechanism can be modified in a running cluster using the following sequence:</p>
        <ol>
        <li>Enable new SASL mechanism by adding the mechanism to <tt>sasl.enabled.mechanisms</tt> in server.properties for each broker. Update JAAS config file to include both
            mechanisms as described <a href="#security_sasl_multimechanism">here</a>. Incrementally bounce the cluster nodes.</li>
        <li>Restart clients using the new mechanism.</li>
        <li>To change the mechanism of inter-broker communication (if this is required), set <tt>sasl.mechanism.inter.broker.protocol</tt> in server.properties to the new mechanism and
            incrementally bounce the cluster again.</li>
        <li>To remove old mechanism (if this is required), remove the old mechanism from <tt>sasl.enabled.mechanisms</tt> in server.properties and remove the entries for the
            old mechanism from JAAS config file. Incrementally bounce the cluster again.</li>
        </ol>
    </li>
    </ol>

    <h3><a id="security_authz" href="#security_authz">7.4 Authorization and ACLs</a></h3>
    Kafka ships with a pluggable Authorizer and an out-of-box authorizer implementation that uses zookeeper to store all the acls. Kafka acls are defined in the general format of "Principal P is [Allowed/Denied] Operation O From Host H On Resource R". You can read more about the acl structure on KIP-11. In order to add, remove or list acls you can use the Kafka authorizer CLI. By default, if a Resource R has no associated acls, no one other than super users is allowed to access R. If you want to change that behavior, you can include the following in server.properties.
    <pre>allow.everyone.if.no.acl.found=true</pre>
    One can also add super users in server.properties like the following (note that the delimiter is semicolon since SSL user names may contain comma).
    <pre>super.users=User:Bob;User:Alice</pre>
    By default, the SSL user name will be of the form "CN=writeuser,OU=Unknown,O=Unknown,L=Unknown,ST=Unknown,C=Unknown". One can change that by setting a customized PrincipalBuilder in server.properties like the following.
    <pre>principal.builder.class=CustomizedPrincipalBuilderClass</pre>
    By default, the SASL user name will be the primary part of the Kerberos principal. One can change that by setting <code>sasl.kerberos.principal.to.local.rules</code> to a customized rule in server.properties.
    The format of <code>sasl.kerberos.principal.to.local.rules</code> is a list where each rule works in the same way as the auth_to_local in <a href="http://web.mit.edu/Kerberos/krb5-latest/doc/admin/conf_files/krb5_conf.html">Kerberos configuration file (krb5.conf)</a>. Each rules starts with RULE: and contains an expression in the format [n:string](regexp)s/pattern/replacement/g. See the kerberos documentation for more details. An example of adding a rule to properly translate user@MYDOMAIN.COM to user while also keeping the default rule in place is:
    <pre>sasl.kerberos.principal.to.local.rules=RULE:[1:$1@$0](.*@MYDOMAIN.COM)s/@.*//,DEFAULT</pre>

    <h4><a id="security_authz_cli" href="#security_authz_cli">Command Line Interface</a></h4>
    Kafka Authorization management CLI can be found under bin directory with all the other CLIs. The CLI script is called <b>kafka-acls.sh</b>. Following lists all the options that the script supports:
    <p></p>
    <table class="data-table">
        <tr>
            <th>Option</th>
            <th>Description</th>
            <th>Default</th>
            <th>Option type</th>
        </tr>
        <tr>
            <td>--add</td>
            <td>Indicates to the script that user is trying to add an acl.</td>
            <td></td>
            <td>Action</td>
        </tr>
        <tr>
            <td>--remove</td>
            <td>Indicates to the script that user is trying to remove an acl.</td>
            <td></td>
            <td>Action</td>
        </tr>
        <tr>
            <td>--list</td>
            <td>Indicates to the script that user is trying to list acls.</td>
            <td></td>
            <td>Action</td>
        </tr>
        <tr>
            <td>--authorizer</td>
            <td>Fully qualified class name of the authorizer.</td>
            <td>kafka.security.auth.SimpleAclAuthorizer</td>
            <td>Configuration</td>
        </tr>
        <tr>
            <td>--authorizer-properties</td>
            <td>key=val pairs that will be passed to authorizer for initialization. For the default authorizer the example values are: zookeeper.connect=localhost:2181</td>
            <td></td>
            <td>Configuration</td>
        </tr>
        <tr>
            <td>--cluster</td>
            <td>Specifies cluster as resource.</td>
            <td></td>
            <td>Resource</td>
        </tr>
        <tr>
            <td>--topic [topic-name]</td>
            <td>Specifies the topic as resource.</td>
            <td></td>
            <td>Resource</td>
        </tr>
        <tr>
            <td>--group [group-name]</td>
            <td>Specifies the consumer-group as resource.</td>
            <td></td>
            <td>Resource</td>
        </tr>
        <tr>
            <td>--allow-principal</td>
            <td>Principal is in PrincipalType:name format that will be added to ACL with Allow permission. <br>You can specify multiple --allow-principal in a single command.</td>
            <td></td>
            <td>Principal</td>
        </tr>
        <tr>
            <td>--deny-principal</td>
            <td>Principal is in PrincipalType:name format that will be added to ACL with Deny permission. <br>You can specify multiple --deny-principal in a single command.</td>
            <td></td>
            <td>Principal</td>
        </tr>
        <tr>
            <td>--allow-host</td>
            <td>IP address from which principals listed in --allow-principal will have access.</td>
            <td> if --allow-principal is specified defaults to * which translates to "all hosts"</td>
            <td>Host</td>
        </tr>
        <tr>
            <td>--deny-host</td>
            <td>IP address from which principals listed in --deny-principal will be denied access.</td>
            <td>if --deny-principal is specified defaults to * which translates to "all hosts"</td>
            <td>Host</td>
        </tr>
        <tr>
            <td>--operation</td>
            <td>Operation that will be allowed or denied.<br>
                Valid values are : Read, Write, Create, Delete, Alter, Describe, ClusterAction, All</td>
            <td>All</td>
            <td>Operation</td>
        </tr>
        <tr>
            <td>--producer</td>
            <td> Convenience option to add/remove acls for producer role. This will generate acls that allows WRITE,
                DESCRIBE on topic and CREATE on cluster.</td>
            <td></td>
            <td>Convenience</td>
        </tr>
        <tr>
            <td>--consumer</td>
            <td> Convenience option to add/remove acls for consumer role. This will generate acls that allows READ,
                DESCRIBE on topic and READ on consumer-group.</td>
            <td></td>
            <td>Convenience</td>
        </tr>
        <tr>
            <td>--force</td>
            <td> Convenience option to assume yes to all queries and do not prompt.</td>
            <td></td>
            <td>Convenience</td>
        </tr>
    </tbody></table>

    <h4><a id="security_authz_examples" href="#security_authz_examples">Examples</a></h4>
    <ul>
        <li><b>Adding Acls</b><br>
    Suppose you want to add an acl "Principals User:Bob and User:Alice are allowed to perform Operation Read and Write on Topic Test-Topic from IP 198.51.100.0 and IP 198.51.100.1". You can do that by executing the CLI with following options:
            <pre class="brush: bash;">bin/kafka-acls.sh --authorizer-properties zookeeper.connect=localhost:2181 --add --allow-principal User:Bob --allow-principal User:Alice --allow-host 198.51.100.0 --allow-host 198.51.100.1 --operation Read --operation Write --topic Test-topic</pre>
            By default, all principals that don't have an explicit acl that allows access for an operation to a resource are denied. In rare cases where an allow acl is defined that allows access to all but some principal we will have to use the --deny-principal and --deny-host option. For example, if we want to allow all users to Read from Test-topic but only deny User:BadBob from IP 198.51.100.3 we can do so using following commands:
            <pre class="brush: bash;">bin/kafka-acls.sh --authorizer-properties zookeeper.connect=localhost:2181 --add --allow-principal User:* --allow-host * --deny-principal User:BadBob --deny-host 198.51.100.3 --operation Read --topic Test-topic</pre>
            Note that ``--allow-host`` and ``deny-host`` only support IP addresses (hostnames are not supported).
            Above examples add acls to a topic by specifying --topic [topic-name] as the resource option. Similarly user can add acls to cluster by specifying --cluster and to a consumer group by specifying --group [group-name].</li>

        <li><b>Removing Acls</b><br>
                Removing acls is pretty much the same. The only difference is instead of --add option users will have to specify --remove option. To remove the acls added by the first example above we can execute the CLI with following options:
            <pre class="brush: bash;"> bin/kafka-acls.sh --authorizer-properties zookeeper.connect=localhost:2181 --remove --allow-principal User:Bob --allow-principal User:Alice --allow-host 198.51.100.0 --allow-host 198.51.100.1 --operation Read --operation Write --topic Test-topic </pre></li>

        <li><b>List Acls</b><br>
                We can list acls for any resource by specifying the --list option with the resource. To list all acls for Test-topic we can execute the CLI with following options:
                <pre class="brush: bash;">bin/kafka-acls.sh --authorizer-properties zookeeper.connect=localhost:2181 --list --topic Test-topic</pre></li>

        <li><b>Adding or removing a principal as producer or consumer</b><br>
                The most common use case for acl management are adding/removing a principal as producer or consumer so we added convenience options to handle these cases. In order to add User:Bob as a producer of  Test-topic we can execute the following command:
            <pre class="brush: bash;"> bin/kafka-acls.sh --authorizer-properties zookeeper.connect=localhost:2181 --add --allow-principal User:Bob --producer --topic Test-topic</pre>
                Similarly to add Alice as a consumer of Test-topic with consumer group Group-1 we just have to pass --consumer option:
            <pre class="brush: bash;"> bin/kafka-acls.sh --authorizer-properties zookeeper.connect=localhost:2181 --add --allow-principal User:Bob --consumer --topic Test-topic --group Group-1 </pre>
                Note that for consumer option we must also specify the consumer group.
                In order to remove a principal from producer or consumer role we just need to pass --remove option. </li>
        </ul>

    <h3><a id="security_rolling_upgrade" href="#security_rolling_upgrade">7.5 Incorporating Security Features in a Running Cluster</a></h3>
        You can secure a running cluster via one or more of the supported protocols discussed previously. This is done in phases:
        <p></p>
        <ul>
            <li>Incrementally bounce the cluster nodes to open additional secured port(s).</li>
            <li>Restart clients using the secured rather than PLAINTEXT port (assuming you are securing the client-broker connection).</li>
            <li>Incrementally bounce the cluster again to enable broker-to-broker security (if this is required)</li>
            <li>A final incremental bounce to close the PLAINTEXT port.</li>
        </ul>
        <p></p>
        The specific steps for configuring SSL and SASL are described in sections <a href="#security_ssl">7.2</a> and <a href="#security_sasl">7.3</a>.
        Follow these steps to enable security for your desired protocol(s).
        <p></p>
        The security implementation lets you configure different protocols for both broker-client and broker-broker communication.
        These must be enabled in separate bounces. A PLAINTEXT port must be left open throughout so brokers and/or clients can continue to communicate.
        <p></p>

        When performing an incremental bounce stop the brokers cleanly via a SIGTERM. It's also good practice to wait for restarted replicas to return to the ISR list before moving onto the next node.
        <p></p>
        As an example, say we wish to encrypt both broker-client and broker-broker communication with SSL. In the first incremental bounce, a SSL port is opened on each node:
            <pre>
            listeners=PLAINTEXT://broker1:9091,SSL://broker1:9092
            </pre>

        We then restart the clients, changing their config to point at the newly opened, secured port:

            <pre>
            bootstrap.servers = [broker1:9092,...]
            security.protocol = SSL
            ...etc
            </pre>

        In the second incremental server bounce we instruct Kafka to use SSL as the broker-broker protocol (which will use the same SSL port):

            <pre>
            listeners=PLAINTEXT://broker1:9091,SSL://broker1:9092
            security.inter.broker.protocol=SSL
            </pre>

        In the final bounce we secure the cluster by closing the PLAINTEXT port:

            <pre>
            listeners=SSL://broker1:9092
            security.inter.broker.protocol=SSL
            </pre>

        Alternatively we might choose to open multiple ports so that different protocols can be used for broker-broker and broker-client communication. Say we wished to use SSL encryption throughout (i.e. for broker-broker and broker-client communication) but we'd like to add SASL authentication to the broker-client connection also. We would achieve this by opening two additional ports during the first bounce:

            <pre>
            listeners=PLAINTEXT://broker1:9091,SSL://broker1:9092,SASL_SSL://broker1:9093
            </pre>

        We would then restart the clients, changing their config to point at the newly opened, SASL & SSL secured port:

            <pre>
            bootstrap.servers = [broker1:9093,...]
            security.protocol = SASL_SSL
            ...etc
            </pre>

        The second server bounce would switch the cluster to use encrypted broker-broker communication via the SSL port we previously opened on port 9092:

            <pre>
            listeners=PLAINTEXT://broker1:9091,SSL://broker1:9092,SASL_SSL://broker1:9093
            security.inter.broker.protocol=SSL
            </pre>

        The final bounce secures the cluster by closing the PLAINTEXT port.

            <pre>
        listeners=SSL://broker1:9092,SASL_SSL://broker1:9093
        security.inter.broker.protocol=SSL
            </pre>

        ZooKeeper can be secured independently of the Kafka cluster. The steps for doing this are covered in section <a href="#zk_authz_migration">7.6.2</a>.


    <h3><a id="zk_authz" href="#zk_authz">7.6 ZooKeeper Authentication</a></h3>
    <h4><a id="zk_authz_new" href="#zk_authz_new">7.6.1 New clusters</a></h4>
    To enable ZooKeeper authentication on brokers, there are two necessary steps:
    <ol>
        <li> Create a JAAS login file and set the appropriate system property to point to it as described above</li>
        <li> Set the configuration property <tt>zookeeper.set.acl</tt> in each broker to true</li>
    </ol>

    The metadata stored in ZooKeeper for the Kafka cluster is world-readable, but can only be modified by the brokers. The rationale behind this decision is that the data stored in ZooKeeper is not sensitive, but inappropriate manipulation of that data can cause cluster disruption. We also recommend limiting the access to ZooKeeper via network segmentation (only brokers and some admin tools need access to ZooKeeper if the new Java consumer and producer clients are used).

    <h4><a id="zk_authz_migration" href="#zk_authz_migration">7.6.2 Migrating clusters</a></h4>
    If you are running a version of Kafka that does not support security or simply with security disabled, and you want to make the cluster secure, then you need to execute the following steps to enable ZooKeeper authentication with minimal disruption to your operations:
    <ol>
        <li>Perform a rolling restart setting the JAAS login file, which enables brokers to authenticate. At the end of the rolling restart, brokers are able to manipulate znodes with strict ACLs, but they will not create znodes with those ACLs</li>
        <li>Perform a second rolling restart of brokers, this time setting the configuration parameter <tt>zookeeper.set.acl</tt> to true, which enables the use of secure ACLs when creating znodes</li>
        <li>Execute the ZkSecurityMigrator tool. To execute the tool, there is this script: <tt>./bin/zookeeper-security-migration.sh</tt> with <tt>zookeeper.acl</tt> set to secure. This tool traverses the corresponding sub-trees changing the ACLs of the znodes</li>
    </ol>
    <p>It is also possible to turn off authentication in a secure cluster. To do it, follow these steps:</p>
    <ol>
        <li>Perform a rolling restart of brokers setting the JAAS login file, which enables brokers to authenticate, but setting <tt>zookeeper.set.acl</tt> to false. At the end of the rolling restart, brokers stop creating znodes with secure ACLs, but are still able to authenticate and manipulate all znodes</li>
        <li>Execute the ZkSecurityMigrator tool. To execute the tool, run this script <tt>./bin/zookeeper-security-migration.sh</tt> with <tt>zookeeper.acl</tt> set to unsecure. This tool traverses the corresponding sub-trees changing the ACLs of the znodes</li>
        <li>Perform a second rolling restart of brokers, this time omitting the system property that sets the JAAS login file</li>
    </ol>
    Here is an example of how to run the migration tool:
    <pre class="brush: bash;">
    ./bin/zookeeper-security-migration.sh --zookeeper.acl=secure --zookeeper.connect=localhost:2181
    </pre>
    <p>Run this to see the full list of parameters:</p>
    <pre class="brush: bash;">
    ./bin/zookeeper-security-migration.sh --help
    </pre>
    <h4><a id="zk_authz_ensemble" href="#zk_authz_ensemble">7.6.3 Migrating the ZooKeeper ensemble</a></h4>
    It is also necessary to enable authentication on the ZooKeeper ensemble. To do it, we need to perform a rolling restart of the server and set a few properties. Please refer to the ZooKeeper documentation for more detail:
    <ol>
        <li><a href="http://zookeeper.apache.org/doc/r3.4.9/zookeeperProgrammers.html#sc_ZooKeeperAccessControl">Apache ZooKeeper documentation</a></li>
        <li><a href="https://cwiki.apache.org/confluence/display/ZOOKEEPER/Zookeeper+and+SASL">Apache ZooKeeper wiki</a></li>
    </ol>
</script>

<div class="p-security"></div>
