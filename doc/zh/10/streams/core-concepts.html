<!--
 Licensed to the Apache Software Foundation (ASF) under one or more
 contributor license agreements.  See the NOTICE file distributed with
 this work for additional information regarding copyright ownership.
 The ASF licenses this file to You under the Apache License, Version 2.0
 (the "License"); you may not use this file except in compliance with
 the License.  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

 Unless required by applicable law or agreed to in writing, software
 distributed under the License is distributed on an "AS IS" BASIS,
 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 See the License for the specific language governing permissions and
 limitations under the License.
-->

<script>
  @@include('../js/templateData.js')
<!--#include virtual="../js/templateData.js" --></script>

<script id="content-template" type="text/x-handlebars-template">
    <h1>核心思想</h1>
    <div class="sub-nav-sticky">
        <div class="sticky-top">
            <div style="height:35px">
                <a href="/{{version}}/documentation/streams/">简介</a>
                <a href="/{{version}}/documentation/streams/developer-guide">开发者指南</a>
                <a class="active-menu-item"  href="/{{version}}/documentation/streams/core-concepts">核心思想</a>
                <a href="/{{version}}/documentation/streams/quickstart">运行 demo 程序</a>
                <a href="/{{version}}/documentation/streams/tutorial">编写自己的流处理程序</a>
            </div>
        </div>
    </div>
    <p>
        Kafka Streams 是一个处理和分析保存在 Kafka 的数据的客户端库。
        它建立在重要的流处理概念之上，例如能够恰当地区分事件时间和处理时间、提供窗口功能以及简单有效、支持实时查询的应用程序状态管理。
    </p>
    <p>
        Kafka Streams 的<b>入门门槛很低</b>。我们可以在单节点环境上快速实现一个小规模的验证性的程序，只要程序能在多节点的集群环境成功运行即可部署到高负载的生产环境。
        Kafka Streams 通过利用 Kafka 的并行模型实现对相同应用程序的多个实例的负载平衡，这对于用户来说是透明的。
    </p>
    <p>
        Kafka Streams 的一些亮点：
    </p>

    <ul>
        <li>被设计为<b>简单且轻量级的客户端库</b>，可以轻松嵌入到任何Java应用程序中，也可轻松地集成到用户的任何流式应用程序的打包，部署和操作工具。</li>
        <li><b>除了使用 Apache Kafka 作为内部消息传递层之外， Kafka Streams 没有外部依赖关系</b>; 值得注意的是，它使用 Kafka 的分区模型来实现横向扩展处理，并同时保证强有力的有序性。</li>
        <li><b>支持容错的本地状态</b>，它支持非常快速和高效的有状态操作，如窗口连接和聚合。</li>
        <li>支持<b>一次处理语义</b>，以确保每个记录只被处理一次，即使错误发生在 Streams 客户端或 Kafka Broker 中也仅处理一次。</li>
        <li>采用<b>一次一个记录的处理方式</b>，以实现毫秒级的处理延迟，并支持对延迟的记录使用<b>基于事件时间的窗口化操作</b>。</li>
        <li>提供必要的流处理元操作的，包括<b>高度抽象的流 DSL 语言</b>和<b>底层流处理器的 API </b></li>。

    </ul>

    <p>
        以下是 Kafka Stream 的几个重要概念。
    </p>

    <h3><a id="streams_topology" href="#streams_topology">流处理拓扑结构</a></h3>

    <ul>
        <li><b>流（Stream）</b> 是 Kafka Stream 的一个非常重要的抽象概念，代表一个无界的、持续更新的数据集。 Stream 是一个有序、可重演、容错并且不可变的数据集，它的<b>数据</b> 是以键-值对的方式定义的。</li>
        <li><b>流处理程序（stream processing application）</b> 是指所有应用了 Kafka Streams library 的程序。 流处理程序通过一个以上的 <b>处理器拓扑结构（processor topology）</b> 定义计算逻辑，其中 <b>处理器拓扑结构</b> 是一个连接到流（的边界）的流处理器（节点）。</li>
        <li><b>流处理器（stream processor）</b> 是处理器拓扑结构的一个节点；它代表一个处理步骤：从拓扑结构中的前置流处理器接受输入数据并按逻辑转换数据，随后向拓扑结构的后续流处理器提供一个或者多个结果数据。 </li>
    </ul>

    拓扑结构中有两种特殊的处理器：

    <ul>
        <li><b>源处理器（Source Processor）</b> ： 源处理器是一种没有前置节点的特殊流处理器。它从一个或者多个 Kafka Topic 消费记录并产出一个输入流给到拓扑结构的后续处理节点。</li>
        <li><b>Sink Processor</b> ： sink processor 是一种特殊的流处理器，没有处理器需要依赖于它。 它从前置流处理器接受数据并传输给指定的 Kafka Topic 。 </li>
    </ul>

    注意：一个正常的处理器节点在处理记录的同时是可以访问其他远程系统。因此，它的处理结果既可以写入到其他远程系统，也可以回流到 Kafka 系统中。

    <img class="centered" src="/{{version}}/images/streams-architecture-topology.jpg" style="width:400px">

    <p>
        Kafka Streams 提供两种定义流处理拓扑结构的方式： <a href="/{{version}}/documentation/streams/developer-guide#streams_dsl"><b>Kafka Streams DSL</b></a> 提供
        了一些常用的、开箱即用的数据转换操作，比如： <code>map</code>， <code>filter</code>， <code>join</code> 和 <code>aggregations</code> ；而底层的 <a href="/{{version}}/documentation/streams/developer-guide#streams_processor"><b>Processor API</b></a> 则允许
        开发者定义和连接自定义的处理器，并且可以与 <a href="#streams_state">state stores</a> 交互。
    </p>

    <p>
        处理器拓扑结构仅仅是对流处理代码的抽象。
        在程序运行时，逻辑拓扑结构会实例化并在应用程序中复制以进行并行处理。（详细信息可参考 <a href="#streams_architecture_tasks"><b>Stream Partitions and Tasks</b></a> ）。
    </p>

    <h3><a id="streams_time" href="#streams_time">Time</a></h3>

    <p>
        流处理中很关键的一点是 <b>时间（time）</b> 的概念，以及它的模型设计、如何被整合到系统中。
        比如有些操作（如 <b>窗口（windowing）</b> ) 就是基于时间边界进行定义的。
    </p>
    <p>
        流处理中关于时间的一些常见概念：
    </p>

    <ul>
        <li><b>事件时间（Event time）</b> ： 事件或者数据记录产生的时间点，即事件在“源头”发生时的原始时间点。 <b>举个例子：</b> 如果是汽车GPS传感器产生的地理位置变化的事件，则事件时间就是GPS传感器捕获到位置发生变更的时间。</li>
        <li><b>处理事件（Processing time）</b> ： The point in time when the event or data record happens to be processed by the stream processing application, i.e. when the record is being consumed. The processing time may be milliseconds, hours, or days etc. later than the original event time. <b>Example:</b> Imagine an analytics application that reads and processes the geo-location data reported from car sensors to present it to a fleet management dashboard. Here, processing-time in the analytics application might be milliseconds or seconds (e.g. for real-time pipelines based on Apache Kafka and Kafka Streams) or hours (e.g. for batch pipelines based on Apache Hadoop or Apache Spark) after event-time.</li>
        <li><b>提取时间（Ingestion time）</b> ： The point in time when an event or data record is stored in a topic partition by a Kafka broker. The difference to event time is that this ingestion timestamp is generated when the record is appended to the target topic by the Kafka broker, not when the record is created "at the source". The difference to processing time is that processing time is when the stream processing application processes the record. <b>For example,</b> if a record is never processed, there is no notion of processing time for it, but it still has an ingestion time.</li>
    </ul>
    <p>
        The choice between event-time and ingestion-time is actually done through the configuration of Kafka (not Kafka Streams): From Kafka 0.10.x onwards, timestamps are automatically embedded into Kafka messages. Depending on Kafka's configuration these timestamps represent event-time or ingestion-time. The respective Kafka configuration setting can be specified on the broker level or per topic. The default timestamp extractor in Kafka Streams will retrieve these embedded timestamps as-is. Hence, the effective time semantics of your application depend on the effective Kafka configuration for these embedded timestamps.
    </p>
    <p>
        Kafka Streams assigns a <b>timestamp</b> to every data record via the <code>TimestampExtractor</code> interface.
        These per-record timestamps describe the progress of a stream with regards to time and are leveraged by	time-dependent operations such as window operations.
        As a result, this time will only advance when a new record arrives at the processor.
        We call this data-driven time the <b>stream time</b> of the application to differentiate with the <b>wall-clock time</b> when this application is actually executing.
        Concrete implementations of the <code>TimestampExtractor</code> interface will then provide different semantics to the stream time definition.
        For example retrieving or computing timestamps based on the actual contents of data records such as an embedded timestamp field to provide event time semantics,
        and returning the current wall-clock time thereby yield processing time semantics to stream time.
        Developers can thus enforce different notions of time depending on their business needs.
    </p>

    <p>
        Finally, whenever a Kafka Streams application writes records to Kafka, then it will also assign timestamps to these new records. The way the timestamps are assigned depends on the context:
    </p>

    <ul>
        <li> When new output records are generated via processing some input record, for example, <code>context.forward()</code> triggered in the <code>process()</code> function call, output record timestamps are inherited from input record timestamps directly.</li>
        <li> When new output records are generated via periodic functions such as <code>Punctuator#punctuate()</code>, the output record timestamp is defined as the current internal time (obtained through <code>context.timestamp()</code>) of the stream task.</li>
        <li> For aggregations, the timestamp of a resulting aggregate update record will be that of the latest arrived input record that triggered the update.</li>
    </ul>

    <h3><a id="streams_state" href="#streams_state">States</a></h3>

    <p>
        Some stream processing applications don't require state, which means the processing of a message is independent from
        the processing of all other messages.
        However, being able to maintain state opens up many possibilities for sophisticated stream processing applications: you
        can join input streams, or group and aggregate data records. Many such stateful operators are provided by the <a href="/{{version}}/documentation/streams/developer-guide#streams_dsl"><b>Kafka Streams DSL</b></a>.
    </p>
    <p>
        Kafka Streams provides so-called <b>state stores</b>, which can be used by stream processing applications to store and query data.
        This is an important capability when implementing stateful operations.
        Every task in Kafka Streams embeds one or more state stores that can be accessed via APIs to store and query data required for processing.
        These state stores can either be a persistent key-value store, an in-memory hashmap, or another convenient data structure.
        Kafka Streams offers fault-tolerance and automatic recovery for local state stores.
    </p>
    <p>
        Kafka Streams allows direct read-only queries of the state stores by methods, threads, processes or applications external to the stream processing application that created the state stores. This is provided through a feature called <b>Interactive Queries</b>. All stores are named and Interactive Queries exposes only the read operations of the underlying implementation.
    </p>
    <br>

    <h2><a id="streams_processing_guarantee" href="#streams_processing_guarantee">Processing Guarantees</a></h2>

    <p>
        In stream processing, one of the most frequently asked question is "does my stream processing system guarantee that each record is processed once and only once, even if some failures are encountered in the middle of processing?"
        Failing to guarantee exactly-once stream processing is a deal-breaker for many applications that cannot tolerate any data-loss or data duplicates, and in that case a batch-oriented framework is usually used in addition
        to the stream processing pipeline, known as the <a href="http://lambda-architecture.net/">Lambda Architecture</a>.
        Prior to 0.11.0.0, Kafka only provides at-least-once delivery guarantees and hence any stream processing systems that leverage it as the backend storage could not guarantee end-to-end exactly-once semantics.
        In fact, even for those stream processing systems that claim to support exactly-once processing, as long as they are reading from / writing to Kafka as the source / sink, their applications cannot actually guarantee that
        no duplicates will be generated throughout the pipeline.

        Since the 0.11.0.0 release, Kafka has added support to allow its producers to send messages to different topic partitions in a <a href="https://kafka.apache.org/documentation/#semantics">transactional and idempotent manner</a>,
        and Kafka Streams has hence added the end-to-end exactly-once processing semantics by leveraging these features.
        More specifically, it guarantees that for any record read from the source Kafka topics, its processing results will be reflected exactly once in the output Kafka topic as well as in the state stores for stateful operations.
        Note the key difference between Kafka Streams end-to-end exactly-once guarantee with other stream processing frameworks' claimed guarantees is that Kafka Streams tightly integrates with the underlying Kafka storage system and ensure that
        commits on the input topic offsets, updates on the state stores, and writes to the output topics will be completed atomically instead of treating Kafka as an external system that may have side-effects.
        To read more details on how this is done inside Kafka Streams, readers are recommended to read <a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-129%3A+Streams+Exactly-Once+Semantics">KIP-129</a>.

        In order to achieve exactly-once semantics when running Kafka Streams applications, users can simply set the <code>processing.guarantee</code> config value to <b>exactly_once</b> (default value is <b>at_least_once</b>).
        More details can be found in the <a href="/{{version}}/documentation#streamsconfigs"><b>Kafka Streams Configs</b></a> section.
    </p>

    <div class="pagination">
        <a href="/{{version}}/documentation/streams/developer-guide" class="pagination__btn pagination__btn__prev">Previous</a>
        <a href="/{{version}}/documentation/streams/architecture" class="pagination__btn pagination__btn__next">Next</a>
    </div>
</script>

@@include('../../includes/_header.htm')
<!--#include virtual="../../includes/_header.htm" -->
@@include('../../includes/_top.htm')
<!--#include virtual="../../includes/_top.htm" -->
<div class="content documentation documentation--current">
        @@include('../../includes/_nav.htm')
        <!--#include virtual="../../includes/_nav.htm" -->
        <div class="right">
                @@include('../../includes/_docs_banner.htm')
            <!--#include virtual="../../includes/_docs_banner.htm" -->
        <ul class="breadcrumbs">
            <li><a href="/documentation">Documentation</a></li>
            <li><a href="/documentation/streams">Kafka Streams API</a></li>
        </ul>
        <div class="p-content"></div>
    </div>
</div>
@@include('../../includes/_footer.htm')
<!--#include virtual="../../includes/_footer.htm" -->
<script>
$(function() {
          // Show selected style on nav item
          $('.b-nav__streams').addClass('selected');

   
          //sticky secondary nav
          var $navbar = $(".sub-nav-sticky"),
               y_pos = $navbar.offset().top,
               height = $navbar.height();
       
           $(window).scroll(function() {
               var scrollTop = $(window).scrollTop();
           
               if (scrollTop > y_pos - height) {
                   $navbar.addClass("navbar-fixed")
               } else if (scrollTop <= y_pos) {
                   $navbar.removeClass("navbar-fixed")
               }
           });
           // Display docs subnav items
           $('.b-nav__docs').parent().toggleClass('nav__item__with__subs--expanded');
});
</script>
