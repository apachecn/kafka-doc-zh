<!--
 Licensed to the Apache Software Foundation (ASF) under one or more
 contributor license agreements.  See the NOTICE file distributed with
 this work for additional information regarding copyright ownership.
 The ASF licenses this file to You under the Apache License, Version 2.0
 (the "License"); you may not use this file except in compliance with
 the License.  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

 Unless required by applicable law or agreed to in writing, software
 distributed under the License is distributed on an "AS IS" BASIS,
 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 See the License for the specific language governing permissions and
 limitations under the License.
-->
<script id="ops-template" type="text/x-handlebars-template">

  以下是基于LinkedIn使用Kafka作为生产系统的一些使用经验。如果您有其他好的技巧请告诉我们。

  <h3><a id="basic_ops" href="#basic_ops">6.1 基础的 Kafka 操作</a></h3>

  本节将回顾在Kafka集群上执行的最常见操作。所有在本节中看到的工具都可以在Kafka发行版的<code> bin / </ code>目录下找到，如果没有参数运行，每个工具都会打印所有可能的命令行选项的细节。

  <h4><a id="basic_ops_add_topic" href="#basic_ops_add_topic">添加和删除 topics</a></h4>

  您可以选择手动添加 topic ，或者在数据首次发布到不存在的 topic 时自动创建 topic 。如果 topic 是自动创建的，那么您可能需要调整用于自动创建 topic 的默认<a href="#topicconfigs"> topic 配置</a>。
  <p>
  使用 topic 工具来添加和修改 topic ：
  <pre class="brush: bash;">
  &gt; bin/kafka-topics.sh --zookeeper zk_host:port/chroot --create --topic my_topic_name
        --partitions 20 --replication-factor 3 --config x=y
  </pre>
  replication-factor 控制有多少服务器将复制每个写入的消息。如果您设置了3个复制因子，那么只能最多2个相关的服务器能出问题，否则您将无法访问数据。我们建议您使用2或3个复制因子，以便在不中断数据消费的情况下透明的调整集群。
  <p>
  partitions 参数控制 topic 将被分片到多少个日志里。partitions 会产生几个影响。首先，每个分区只属于一个台服务器，所以如果有20个分区，那么全部数据(包含读写负载)将由不超过20个服务器（不包含副本）处理。最后 partitions 还会影响 consumer 的最大并行度。这在<a href="#intro_consumers">概念部分</a>中有更详细的讨论。
  <p>
  每个分区日志都放在自己的Kafka日志目录下的文件夹中。这些文件夹的名称由主题名称，破折号（ -  ）和分区ID组成。由于典型的文件夹名称长度不能超过255个字符，所以主题名称的长度会受到限制。我们假设分区的数量不会超过10万。因此，主题名称不能超过249个字符。这在文件夹名称中留下了足够的空间以显示短划线和可能的5位长的分区ID。
  <p>
    在命令行上添加的配置会覆盖服务器的默认设置，例如数据应该保留的时间长度。<a href="#topicconfigs">此处</a>记录了完整的每个 topic 配置。

  <h4><a id="basic_ops_modify_topic" href="#basic_ops_modify_topic">修改 topics</a></h4>

  使用相同的 topic 工具，您可以修改 topic 的配置或分区。
  <p>
  要添加分区，你可以做如下操作
  <pre class="brush: bash;">
  &gt; bin/kafka-topics.sh --zookeeper zk_host:port/chroot --alter --topic my_topic_name
        --partitions 40
  </pre>
  请注意，分区的一个用处是对数据进行语义分区，并且添加分区不会更改现有数据的分区，因此如果依赖该分区，则可能会影响消费者。也就是说，如果数据是通过<code> hash（key）％number_of_partitions </ code>进行分区的，那么这个分区可能会通过添加分区进行混洗，但Kafka不会尝试以任何方式自动重新分配数据。
  <p>
  增加一个配置项:
  <pre class="brush: bash;">
  &gt; bin/kafka-configs.sh --zookeeper zk_host:port/chroot --entity-type topics --entity-name my_topic_name --alter --add-config x=y
  </pre>
  删除一个配置项:
  <pre class="brush: bash;">
  &gt; bin/kafka-configs.sh --zookeeper zk_host:port/chroot --entity-type topics --entity-name my_topic_name --alter --delete-config x
  </pre>
  删除一个 topic :
  <pre class="brush: bash;">
  &gt; bin/kafka-topics.sh --zookeeper zk_host:port/chroot --delete --topic my_topic_name
  </pre>
  <p>
  当前，Kafka 不支持减少一个 topic 的分区数。
  <p>
  有关更改 一个 topic 复制因子的说明，请参见<a href="#basic_ops_increase_replication_factor">此处</a>.

  <h4><a id="basic_ops_restarting" href="#basic_ops_restarting">优雅的关机</a></h4>

  Kafka集群将自动检测到任何 broker 关机或故障，并为该机器上的分区选择新的 leader。无论服务器出现故障还是因为维护或配置更改而故意停机，都会发生这种情况。 对于后一种情况，Kafka支持更优雅的停止服务器的机制，而不仅仅是杀死它。

  当一个服务器正常停止时，它将采取两种优化措施:
  <ol>
      <li>它将所有日志同步到磁盘，以避免在重新启动时需要进行任何日志恢复活动（即验证日志尾部的所有消息的校验和）。由于日志恢复需要时间，所以从侧面加速了重新启动操作。
      <li>它将在关闭之前将以该服务器为 leader 的任何分区迁移到其他副本。这将使 leader 角色传递更快，并将每个分区不可用的时间缩短到几毫秒。
  </ol>

  只要服务器的停止不是通过直接杀死，同步日志就会自动发生，但控制 leader 迁移需要使用特殊的设置：
  <pre class="brush: text;">
      controlled.shutdown.enable=true
  </pre>
  请注意，只有当 broker 托管的分区具有副本（即，复制因子大于1 <i>且</ i>至少其中一个副本处于活动状态）时，对关闭的控制才会成功。 这通常是你想要的，因为关闭最后一个副本会使 topic 分区不可用。

  <h4><a id="basic_ops_leader_balancing" href="#basic_ops_leader_balancing">Balancing leadership</a></h4>

  每当一个 borker 停止或崩溃时，该 borker 上的分区的leader 会转移到其他副本。这意味着，在 broker 重新启动时，默认情况下，它将只是所有分区的跟随者，这意味着它不会用于客户端的读取和写入。
  <p>
  为了避免这种不平衡，Kafka有一个首选副本的概念。如果分区的副本列表为1,5,9，则节点1首选为节点5或9的 leader ，因为它在副本列表中较早。您可以通过运行以下命令让Kafka集群尝试恢复已恢复副本的领导地位：
  <pre class="brush: bash;">
  &gt; bin/kafka-preferred-replica-election.sh --zookeeper zk_host:port/chroot
  </pre>

  由于运行此命令可能很乏味，您也可以通过以下配置来自动配置Kafka：
  <pre class="brush: text;">
      auto.leader.rebalance.enable=true
  </pre>

  <h4><a id="basic_ops_racks" href="#basic_ops_racks">垮机架均衡副本</a></h4>
  机架感知功能可以跨不同机架传播相同分区的副本。这扩展了 Kafka 为 broker 故障提供的容错担保，弥补了机架故障，如果机架上的所有 broker 都失败，则可以限制数据丢失的风险。该功能也可以应用于其他 broker 分组，例如EC2中的可用区域。
  <p></p>
  您可以通过向 broker 配置添加属性来指定 broker 属于的特定机架：
  <pre class="brush: text;">   broker.rack=my-rack-id</pre>
  当 topic <a href="#basic_ops_add_topic">创建</a>，<a href="#basic_ops_modify_topic">修改</a>或副本<a href="#basic_ops_cluster_expansion">重新分配</a>时， 机架约束将得到保证，确保副本跨越尽可能多的机架（一个分区将跨越 min(#racks，replication-factor) 个不同的机架）。
  <p></p>
  用于向 broker 分配副本的算法可确保每个 broker 的 leader 数量将保持不变，而不管 broker 在机架之间如何分布。这确保了均衡的吞吐量。
  <p></p>
  但是，如果 broker 在机架间分布不均 ，副本的分配将不均匀。具有较少 broker 的机架将获得更多复制副本，这意味着他们将使用更多存储并将更多资源投入复制。因此，每个机架配置相同数量的 broker 是明智的。

  <h4><a id="basic_ops_mirror_maker" href="#basic_ops_mirror_maker">集群之间镜像数据</a></h4>

  我们指的是通过“镜像”复制Kafka集群之间的数据<i>的过程，以避免与在单个集群中的节点之间发生的复制混淆。Kafka附带了一个在Kafka集群之间镜像数据的工具 mirror-maker。该工具从源集群中消费数据并产生数据到目标集群。

  这种镜像的常见用例是在另一个数据中心提供副本。这个场景将在下一节中详细的讨论。
  <p>
  您可以运行许多这样的镜像进程来提高吞吐量和容错能力（如果一个进程死亡，其他进程将承担额外的负载）。
    <p>
  从源群集中的 topic 中读取数据，并将其写入目标群集中具有相同名称的 topic。事实上，镜像只不过是把一个 Kafka 的 consumer 和 producer 联合在一起了。
  <p>
  源和目标集群是完全独立的实体：它们可以有不同数量的分区，偏移量也不会相同。由于这个原因，镜像集群并不是真正意义上的容错机制（因为 consumer 的偏移量将会不同）。为此，我们建议使用正常的群集内复制。然而，镜像制作进程将保留并使用消息 key 进行分区，所以在每个 key 的基础上保存顺序。
  <p>
  以下示例显示如何从输入群集中镜像单个 topic（名为<i> my-topic </i>）：
  <pre class="brush: bash;">
  &gt; bin/kafka-mirror-maker.sh
        --consumer.config consumer.properties
        --producer.config producer.properties --whitelist my-topic
  </pre>
  请注意，我们使用<code> --whitelist </code>选项指定 topic 列表。此选项允许使用任何<a href="http://docs.oracle.com/javase/7/docs/api/java/util/regex/Pattern.html"> Java风格的正则表达式</a> 因此，您可以使用<code>--whitelist 'A|B'</code>来镜像名为<i>A</i> 和 <i>B</i> 的两个 topic 。 或者您可以使用<code>--whitelist '*'</ code>来镜像<i>全部</i> topic。确保引用的任何正则表达式不会被 shell 尝试将其展开为文件路径。为了方便起见，我们允许使用','而不是'|' 指定 topic 列表。
  <p>
  有时，说出你不想要的东西比较容易。与使用<code>--whitelist</code> 来表示你想要的相反，通过镜像您可以使用 <code>--blacklist</code> 来表示要排除的内容。
  这也需要一个正则表达式的参数。但是，当启用新的 consumer 时，不支持 <code>--blacklist</code>（即<code> bootstrap.servers </code>）已在 consumer 配置中定义）。
  <p>
  将镜像与配置项<code> auto.create.topics.enable = true </code>结合使用，可以创建一个副本群集，即使添加了新的 topic，也可以自动创建和复制源群集中的所有数据。

  <h4><a id="basic_ops_consumer_lag" href="#basic_ops_consumer_lag">检查 consumer 位置</a></h4>
  有时观察到消费者的位置是有用的。我们有一个工具，可以显示 consumer 群体中所有 consumer 的位置，以及他们所在日志的结尾。要在名为<i>my-group</i>的 consumer 组上运行此工具，消费一个名为<i>my-topic</i>的 topic 将如下所示：
  <pre class="brush: bash;">
  &gt; bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group my-group

  注意：这将仅显示使用Java consumer API（基于非ZooKeeper的 consumer）的 consumer 的信息。
  
  TOPIC                          PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG        CONSUMER-ID                                       HOST                           CLIENT-ID
  my-topic                       0          2               4               2          consumer-1-029af89c-873c-4751-a720-cefd41a669d6   /127.0.0.1                     consumer-1
  my-topic                       1          2               3               1          consumer-1-029af89c-873c-4751-a720-cefd41a669d6   /127.0.0.1                     consumer-1
  my-topic                       2          2               3               1          consumer-2-42c1abd4-e3b2-425d-a8bb-e1ea49b29bb2   /127.0.0.1                     consumer-2
  </pre>

  这个工具也适用于基于ZooKeeper的 consumer：
  <pre class="brush: bash;">
  &gt; bin/kafka-consumer-groups.sh --zookeeper localhost:2181 --describe --group my-group

  注意：这只会显示关于使用ZooKeeper的 consumer 的信息（不是那些使用Java consumer API的消费者）。

  TOPIC                          PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG        CONSUMER-ID
  my-topic                       0          2               4               2          my-group_consumer-1
  my-topic                       1          2               3               1          my-group_consumer-1
  my-topic                       2          2               3               1          my-group_consumer-2
  </pre>

  <h4><a id="basic_ops_consumer_group" href="#basic_ops_consumer_group">管理 Consumer 组</a></h4>

    通过 ConsumerGroupCommand 工具，我们可以列出，描述或删除 consumer 组。请注意，删除仅在组元数据存储在ZooKeeper中时可用。当使用<a href="http://kafka.apache.org/documentation.html#newconsumerapi">新的 consumer API </a>（ broker 协调分区处理和重新平衡）时，
    当该组的最后一个提交偏移量过期时，该组将被删除。

  例如，要列出所有 topic 中的所有 consumer 组：

  <pre class="brush: bash;">
  &gt; bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --list

  test-consumer-group
  </pre>

  如前所述,为了查看偏移量，我们这样“describe”consumer 组：

  <pre class="brush: bash;">
  &gt; bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group test-consumer-group

  TOPIC                          PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG        CONSUMER-ID                                       HOST                           CLIENT-ID
  test-foo                       0          1               3               2          consumer-1-a5d61779-4d04-4c50-a6d6-fb35d942642d   /127.0.0.1                     consumer-1
  </pre>

  如果您正在使用老的高级 consumer 并在ZooKeeper中存储组元数据（即<code> offsets.storage = zookeeper </code>），则传递 <code>--zookeeper</code> 而不是<code>bootstrap-server</code>：

  <pre class="brush: bash;">
  &gt; bin/kafka-consumer-groups.sh --zookeeper localhost:2181 --list
  </pre>

  <h4><a id="basic_ops_cluster_expansion" href="#basic_ops_cluster_expansion">扩展您的群集</a></h4>

  将服务器添加到Kafka集群非常简单，只需为其分配唯一的 broker ID并在您的新服务器上启动Kafka即可。但是，这些新的服务器不会自动分配到任何数据分区，除非将分区移动到这些分区，否则直到创建新 topic 时才会提供服务。所以通常当您将机器添加到群集中时，您会希望将一些现有数据迁移到这些机器上。
  <p>
  迁移数据的过程是手动启动的，但是完全自动化。在迁移数据时，Kafka会将新服务器添加为正在迁移的分区的 follower，并允许它完全复制该分区中的现有数据。当新服务器完全复制了此分区的内容并加入了同步副本时，其中一个现有副本将删除其分区的数据。
  <p>
  分区重新分配工具可用于跨 broker 移动分区。理想的分区分布将确保所有 broker 的数据负载和分区大小比较均衡。分区重新分配工具不具备自动分析Kafka集群中的数据分布并移动分区以获得均匀负载的功能。因此，管理员必须找出哪些 topic 或分区应该移动。
  <p>
  分区重新分配工具可以以3种互斥方式运行：
  <ul>
  <li>--generate: 在此模式下，给定一个 topic 列表和一个 broker 列表，该工具会生成一个候选重新分配，以将指定的 topic 的所有分区移动到新的broker。此选项仅提供了一种便捷的方式，可以根据 tpoc 和目标 broker 列表生成分区重新分配计划。</li>
  <li>--execute: 在此模式下，该工具基于用户提供的重新分配计划启动分区重新分配。（使用--reassignment-json-file选项）。这可以是由管理员制作的自定义重新分配计划，也可以是使用--generate选项提供的自定义重新分配计划。</li>
  <li>--verify: 在此模式下，该工具将验证最近用 --execute 模式执行间的所有分区的重新分配状态。状态可以是成功完成，失败或正在进行</li>
  </ul>
  <h5><a id="basic_ops_automigrate" href="#basic_ops_automigrate">自动将数据迁移到新机器</a></h5>
  分区重新分配工具可用于将当前一组 broker 的一些 topic 移至新增的topic。这在扩展现有群集时通常很有用，因为将整个 topic 移动到新 broker 集比移动一个分区更容易。当这样做的时候，用户应该提供需要移动到新的 broker 集合的 topic 列表和新的目标broker列表。该工具然后会均匀分配新 broker 集中 topic 的所有分区。在此过程中，topic 的复制因子保持不变。实际上，所有输入 topic 的所有分区副本都将从旧的broker 组转移到新 broker中。
  <p>
  例如，以下示例将把名叫foo1，foo2的 topic 的所有分区移动到新的 broker 集5,6。最后，foo1和foo2的所有分区将只在<5,6> broker 上存在。
  <p>
  由于该工具接受由 topic 组成的输入列表作为json文件，因此首先需要确定要移动的 topic 并创建 json 文件，如下所示：
  <pre class="brush: bash;">
  > cat topics-to-move.json
  {"topics": [{"topic": "foo1"},
              {"topic": "foo2"}],
  "version":1
  }
  </pre>
  一旦json文件准备就绪，就可以使用分区重新分配工具来生成候选分配：
  <pre class="brush: bash;">
  > bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --topics-to-move-json-file topics-to-move.json --broker-list "5,6" --generate
  当前分区副本分配

  {"version":1,
  "partitions":[{"topic":"foo1","partition":2,"replicas":[1,2]},
                {"topic":"foo1","partition":0,"replicas":[3,4]},
                {"topic":"foo2","partition":2,"replicas":[1,2]},
                {"topic":"foo2","partition":0,"replicas":[3,4]},
                {"topic":"foo1","partition":1,"replicas":[2,3]},
                {"topic":"foo2","partition":1,"replicas":[2,3]}]
  }

  建议的分区重新分配配置

  {"version":1,
  "partitions":[{"topic":"foo1","partition":2,"replicas":[5,6]},
                {"topic":"foo1","partition":0,"replicas":[5,6]},
                {"topic":"foo2","partition":2,"replicas":[5,6]},
                {"topic":"foo2","partition":0,"replicas":[5,6]},
                {"topic":"foo1","partition":1,"replicas":[5,6]},
                {"topic":"foo2","partition":1,"replicas":[5,6]}]
  }
  </pre>
  <p>
  该工具会生成一个候选分配，将所有分区从topic foo1，foo2移动到brokers 5,6。但是，请注意，这个时候，分区操作还没有开始，它只是告诉你当前的任务和建议的新任务。应该保存当前的分配，以防您想要回滚到它。新的任务应该保存在一个json文件（例如expand-cluster-reassignment.json）中，并用--execute选项输入到工具中，如下所示：
  <pre class="brush: bash;">
  > bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file expand-cluster-reassignment.json --execute
  当前分区副本分配

  {"version":1,
  "partitions":[{"topic":"foo1","partition":2,"replicas":[1,2]},
                {"topic":"foo1","partition":0,"replicas":[3,4]},
                {"topic":"foo2","partition":2,"replicas":[1,2]},
                {"topic":"foo2","partition":0,"replicas":[3,4]},
                {"topic":"foo1","partition":1,"replicas":[2,3]},
                {"topic":"foo2","partition":1,"replicas":[2,3]}]
  }

  保存这个以在回滚期间用作--reassignment-json-file选项
  成功开始重新分配分区
  {"version":1,
  "partitions":[{"topic":"foo1","partition":2,"replicas":[5,6]},
                {"topic":"foo1","partition":0,"replicas":[5,6]},
                {"topic":"foo2","partition":2,"replicas":[5,6]},
                {"topic":"foo2","partition":0,"replicas":[5,6]},
                {"topic":"foo1","partition":1,"replicas":[5,6]},
                {"topic":"foo2","partition":1,"replicas":[5,6]}]
  }
  </pre>
  <p>
  最后，可以使用--verify选项来检查分区重新分配的状态。请注意，相同的expand-cluster-reassignment.json（与--execute选项一起使用）应与--verify选项一起使用：
  <pre class="brush: bash;">
  > bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file expand-cluster-reassignment.json --verify
  Status of partition reassignment:
  Reassignment of partition [foo1,0] completed successfully
  Reassignment of partition [foo1,1] is in progress
  Reassignment of partition [foo1,2] is in progress
  Reassignment of partition [foo2,0] completed successfully
  Reassignment of partition [foo2,1] completed successfully
  Reassignment of partition [foo2,2] completed successfully
  </pre>

  <h5><a id="basic_ops_partitionassignment" href="#basic_ops_partitionassignment">自定义分区分配和迁移</a></h5>
  分区重新分配工具也可用于选择性地将分区的副本移动到特定的一组 broker。当以这种方式使用时，假定用户知道重新分配计划并且不需要该工具产生候选的重新分配，有效地跳过 --generate 步骤并直接到 --execute步骤
  <p>
  例如，以下示例将 topic foo1的分区0 移到 broker 5,6中和将 topic foo2的分区1移到 broker 2,3中：
  <p>
  第一步是在json文件中定义重新分配计划：
  <pre class="brush: bash;">
  > cat custom-reassignment.json
  {"version":1,"partitions":[{"topic":"foo1","partition":0,"replicas":[5,6]},{"topic":"foo2","partition":1,"replicas":[2,3]}]}
  </pre>
  然后，使用带有 --execute 选项的 json 文件来启动重新分配过程：
  <pre class="brush: bash;">
  > bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file custom-reassignment.json --execute
 当前分区副本分配情况

  {"version":1,
  "partitions":[{"topic":"foo1","partition":0,"replicas":[1,2]},
                {"topic":"foo2","partition":1,"replicas":[3,4]}]
  }

  保存这个以在回滚期间用作 --reassignment-json-file 选项
  成功开始重新分配分区
  {"version":1,
  "partitions":[{"topic":"foo1","partition":0,"replicas":[5,6]},
                {"topic":"foo2","partition":1,"replicas":[2,3]}]
  }
  </pre>
  <p>
  可以使用--verify选项来检查分区重新分配的状态。 请注意，相同的expand-cluster-reassignment.json（与--execute选项一起使用）应与--verify选项一起使用：
  <pre class="brush: bash;">
  > bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file custom-reassignment.json --verify
  Status of partition reassignment:
  Reassignment of partition [foo1,0] completed successfully
  Reassignment of partition [foo2,1] completed successfully
  </pre>

  <h4><a id="basic_ops_decommissioning_brokers" href="#basic_ops_decommissioning_brokers">下线 brokers</a></h4>

  分区重新分配工具不具备为下线 broker 自动生成重新分配计划的功能。因此，管理员必须自己整理重新分配计划，将托管在即将下线的 broker 上的所有分区的副本移动到其他 broker。这可能比较单调，因为重新分配需要确保所有副本不会从将下线的 broker 只转移到唯一的 broker。 为了使这一过程毫不费力，我们计划在未来为下线 broker 添加工具支持。
  <h4><a id="basic_ops_increase_replication_factor" href="#basic_ops_increase_replication_factor">增加复制因子</a></h4>
  增加现有分区的复制因子很容易。只需在自定义重新分配json文件中指定额外的副本，并将其与--execute选项一起使用，以增加指定分区的复制因子。
  <p>
  例如，以下示例将foo的分区0的复制因子从1增加到3。在增加复制因子之前，该分区的唯一副本存在于 broker 5上。作为增加复制因子的一部分，我们将添加更多副本到 broker 6和7。
  <p>
  第一步是在json文件中自定义重新分配计划：
  <pre class="brush: bash;">
  > cat increase-replication-factor.json
  {"version":1,
  "partitions":[{"topic":"foo","partition":0,"replicas":[5,6,7]}]}
  </pre>
  然后，使用带有--execute选项的json文件来启动重新分配过程：
  <pre class="brush: bash;">
  > bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file increase-replication-factor.json --execute
  当前分区副本分配

  {"version":1,
  "partitions":[{"topic":"foo","partition":0,"replicas":[5]}]}

  保存这个以在回滚期间用作--reassignment-json-file选项
  成功开始重新分配分区
  {"version":1,
  "partitions":[{"topic":"foo","partition":0,"replicas":[5,6,7]}]}
  </pre>
  <p>
  可以使用--verify选项来检查分区重新分配的状态。请注意，与--verify选项使用的increase-replication-factor.json要与--execute选项一起使用的相同：
  <pre class="brush: bash;">
  > bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file increase-replication-factor.json --verify
  Status of partition reassignment:
  Reassignment of partition [foo,0] completed successfully
  </pre>
  您还可以使用kafka-topics工具验证复制因子的增加情况：
  <pre class="brush: bash;">
  > bin/kafka-topics.sh --zookeeper localhost:2181 --topic foo --describe
  Topic:foo	PartitionCount:1	ReplicationFactor:3	Configs:
    Topic: foo	Partition: 0	Leader: 5	Replicas: 5,6,7	Isr: 5,6,7
  </pre>

  <h4><a id="rep-throttle" href="#rep-throttle">限制数据迁移过程中的带宽使用</a></h4>
  Kafka允许您设置复制流量的阈值，设置用于将副本从机器移动到另一台机器上的带宽上限。在重新平衡群集，引导新 broker 或添加或删除 broker 时，这非常有用，因为它限制了这些数据密集型操作对用户的影响。
  <p></p>
  有两个接口可以用来调节阈值。最简单也是最安全的是在调用kafka-reassign-partitions.sh时调节，但也可以使用kafka-configs.sh直接查看和更改流量阀值。
  <p></p>
  例如，如果要使用下面的命令执行重新平衡，它将以不超过 50MB/s 的速度移动分区。
  <pre class="brush: bash;">$ bin/kafka-reassign-partitions.sh --zookeeper myhost:2181--execute --reassignment-json-file bigger-cluster.json —throttle 50000000</pre>
  当你执行这个脚本时，你会看到：
  <pre class="brush: bash;">
  The throttle limit was set to 50000000 B/s
  Successfully started reassignment of partitions.</pre>
  <p>如果你想改变阈值，在重新平衡期间，比如增加吞吐量以便更快地完成，你可以通过重新运行execute命令来传递同样的reassignment-json-file：</p>
  <pre class="brush: bash;">$ bin/kafka-reassign-partitions.sh --zookeeper localhost:2181  --execute --reassignment-json-file bigger-cluster.json --throttle 700000000
  There is an existing assignment running.
  The throttle limit was set to 700000000 B/s</pre>

  <p>当重新平衡完成后，管理员可以使用--verify选项检查重新平衡的状态。如果重新平衡完成，流量阈值将通过--verify命令删除。
    一旦重新平衡完成后，管理员必须及时通过 --verify 选项删除节流阀，如果不这样做可能会导致正常的复制流量受到限制。
      </p>
  <p>当执行--verify选项并且重新分配完成时，脚本将确认节流阀已被移除：</p>

  <pre class="brush: bash;">
  > bin/kafka-reassign-partitions.sh --zookeeper localhost:2181  --verify --reassignment-json-file bigger-cluster.json
  Status of partition reassignment:
  Reassignment of partition [my-topic,1] completed successfully
  Reassignment of partition [mytopic,0] completed successfully
  Throttle was removed.</pre>

  <p>管理员还可以使用kafka-configs.sh验证分配的配置。有两对节流阀配置用于管理节流过程。节流阈值本身在 broker 级别使用动态属性进行配置的： </p>

  <pre class="brush: text;">leader.replication.throttled.rate
  follower.replication.throttled.rate</pre>

  <p>还有一组枚举类型的复制节流配置： </p>

  <pre class="brush: text;">leader.replication.throttled.replicas
  follower.replication.throttled.replicas</pre>

  <p>每个 topic 配置了哪些。所有四个配置值都由kafka-reassign-partitions.sh自动分配(下面讨论). </p>
  <p>查看流量限制配置：</p>

  <pre class="brush: bash;">
  > bin/kafka-configs.sh --describe --zookeeper localhost:2181 --entity-type brokers
  Configs for brokers '2' are leader.replication.throttled.rate=700000000,follower.replication.throttled.rate=700000000
  Configs for brokers '1' are leader.replication.throttled.rate=700000000,follower.replication.throttled.rate=700000000</pre>

  <p>这显示应用于复制协议的 leader 和 follower 的节流阀。默认情况下，双方都被分配相同的限制吞吐量值。 </p>

  <p>要查看节流副本的列表，请执行以下操作：</p>

  <pre class="brush: bash;">
  > bin/kafka-configs.sh --describe --zookeeper localhost:2181 --entity-type topics
  Configs for topic 'my-topic' are leader.replication.throttled.replicas=1:102,0:101,
      follower.replication.throttled.replicas=1:101,0:102</pre>

  <p>这里我们看到 leader 节流阀被应用于 broker 102上的分区1和 broker 101上的分区0。follower节流阀同样被应用于 broker 101上的分区1和 broker 102上的分区0。</p>

  <p>默认情况下，kafka-reassign-partitions.sh将把 leader 的节流阀应用于重新平衡之前存在的所有副本，其中任何一个都可能是 leader。它会将 follower 应用到所有的目的地。
      因此，如果 broker 101,102上有副本的分区，被重新分配到102,103，那么该分区的 leader 节流阀将被应用于101,102，并且 follower 节流阀将仅被应用于103。</p>


  <p>如果需要，还可以使用kafka-configs.sh上的--alter开关手动更改节流阀配置。
  </p>

  <h5>安全使用节流复制</h5>

  <p>在使用节流复制时应该小心。 尤其是：</p>

  <p><i>(1) 移除节流阀</i></p>
  一旦重新分配完成，应及时移除节流阀（通过运行kafka-reassign-partitions -verify）。

  <p><i>(2) 确保进展：</i></p>
  <p>如果与传入写入速率相比阈值设置得太低，复制可能无法取得进展。这发生在：</p>
  <pre>max(BytesInPerSec) > throttle</pre>
  <p>
      BytesInPerSec是监控 producer 写入每个 broker 的写入吞吐量的指标。</p>
  <p>管理员可以在重新平衡期间使用以下指标监控复制是否正在取得进展：</p>

  <pre>kafka.server:type=FetcherLagMetrics,name=ConsumerLag,clientId=([-.\w]+),topic=([-.\w]+),partition=([0-9]+)</pre>

  <p>
  复制期间滞后数据应该不断减少。如果指标不降低，管理员应按上述方法增大节流阈值。
       </p>


  <h4><a id="quotas" href="#quotas">设置配额</a></h4>
  配额覆盖值和默认值可以在（user=user1, client-id=clientA），用户或客户端级别配置，如<a href="#design_quotas">此处</a>所述。
  默认情况下，客户端是无限制的配额。可以为每个（user, client-id），用户或客户端组设置自定义配额。
  <p>
  为（user = user1，client-id = clientA）配置自定义配额：
  <pre class="brush: bash;">
  > bin/kafka-configs.sh  --zookeeper localhost:2181 --alter --add-config 'producer_byte_rate=1024,consumer_byte_rate=2048,request_percentage=200' --entity-type users --entity-name user1 --entity-type clients --entity-name clientA
  Updated config for entity: user-principal 'user1', client-id 'clientA'.
  </pre>

   为user = user1配置自定义配额：
  <pre class="brush: bash;">
  > bin/kafka-configs.sh  --zookeeper localhost:2181 --alter --add-config 'producer_byte_rate=1024,consumer_byte_rate=2048,request_percentage=200' --entity-type users --entity-name user1
  Updated config for entity: user-principal 'user1'.
  </pre>

  为client-id=clientA配置自定义配额：
  <pre class="brush: bash;">
  > bin/kafka-configs.sh  --zookeeper localhost:2181 --alter --add-config 'producer_byte_rate=1024,consumer_byte_rate=2048,request_percentage=200' --entity-type clients --entity-name clientA
  Updated config for entity: client-id 'clientA'.
  </pre>

  可以通过指定 <i>--entity-default</i> 选项而不是<i>--entity-name</i>来为每个（user, client-id），用户或客户端ID组设置默认配额。
  <p>
  为user = userA配置默认客户端配额：
  <pre class="brush: bash;">
  > bin/kafka-configs.sh  --zookeeper localhost:2181 --alter --add-config 'producer_byte_rate=1024,consumer_byte_rate=2048,request_percentage=200' --entity-type users --entity-name user1 --entity-type clients --entity-default
  Updated config for entity: user-principal 'user1', default client-id.
  </pre>

  为用户配置默认配额：
  <pre class="brush: bash;">
  > bin/kafka-configs.sh  --zookeeper localhost:2181 --alter --add-config 'producer_byte_rate=1024,consumer_byte_rate=2048,request_percentage=200' --entity-type users --entity-default
  Updated config for entity: default user-principal.
  </pre>

  配置client-id的默认配额：
  <pre class="brush: bash;">
  > bin/kafka-configs.sh  --zookeeper localhost:2181 --alter --add-config 'producer_byte_rate=1024,consumer_byte_rate=2048,request_percentage=200' --entity-type clients --entity-default
  Updated config for entity: default client-id.
  </pre>

   以下是如何描述给定（user, client-id）的配额：
  <pre class="brush: bash;">
  > bin/kafka-configs.sh  --zookeeper localhost:2181 --describe --entity-type users --entity-name user1 --entity-type clients --entity-name clientA
  Configs for user-principal 'user1', client-id 'clientA' are producer_byte_rate=1024,consumer_byte_rate=2048,request_percentage=200
  </pre>
  描述给定用户的配额：
  <pre class="brush: bash;">
  > bin/kafka-configs.sh  --zookeeper localhost:2181 --describe --entity-type users --entity-name user1
  Configs for user-principal 'user1' are producer_byte_rate=1024,consumer_byte_rate=2048,request_percentage=200
  </pre>
  描述给定 client-id的配额：
  <pre class="brush: bash;">
  > bin/kafka-configs.sh  --zookeeper localhost:2181 --describe --entity-type clients --entity-name clientA
  Configs for client-id 'clientA' are producer_byte_rate=1024,consumer_byte_rate=2048,request_percentage=200
  </pre>
  如果未指定实体名称，则描述指定类型的所有实体。 例如，描述所有用户：
  <pre class="brush: bash;">
  > bin/kafka-configs.sh  --zookeeper localhost:2181 --describe --entity-type users
  Configs for user-principal 'user1' are producer_byte_rate=1024,consumer_byte_rate=2048,request_percentage=200
  Configs for default user-principal are producer_byte_rate=1024,consumer_byte_rate=2048,request_percentage=200
  </pre>
  对于(user, client)也是同样的:
  <pre class="brush: bash;">
  > bin/kafka-configs.sh  --zookeeper localhost:2181 --describe --entity-type users --entity-type clients
  Configs for user-principal 'user1', default client-id are producer_byte_rate=1024,consumer_byte_rate=2048,request_percentage=200
  Configs for user-principal 'user1', client-id 'clientA' are producer_byte_rate=1024,consumer_byte_rate=2048,request_percentage=200
  </pre>
  <p>
  通过在 broker 上设置这些配置，可以适用于所有client-id的默认配额。只有在Zookeeper中未配置配额覆盖或默认配置时才应用这些属性。默认情况下，每个 client-id 都会收到一个无限制的配额。以下设置每个 producer 和 consumer 客户端的默认配额为10MB/sec。
  <pre class="brush: text;">
    quota.producer.default=10485760
    quota.consumer.default=10485760
  </pre>
  请注意，这些属性已被弃用，并可能在未来版本中删除。使用kafka-configs.sh配置的默认值优先于这些属性。

  <h3><a id="datacenters" href="#datacenters">6.2 数据中心</a></h3>

  有一些部署需要维护一个跨越多个数据中心的数据管道。对此，我们推荐的方法是在每个拥有众多应用实例的数据中心内部署一个本地Kafka集群，在每个数据中心内只与本地的kafka集群进行交互，然后各集群之间通过镜像进行同步，（请参阅<a href="#basic_ops_mirror_maker">镜像制作工具< / a>了解怎么做到这一点）。.
  <p>
  这种部署模式允许数据中心充当一个独立的实体，并允许我们能够集中的管理和调节数据中心之间的复制。在这种部署模式下，即使数据中心间的链路不可用，每个设施也可以独立运行：当发生这种情况时，镜像会落后，直到链路恢复正常并追上时为止。
  <p>
  如果应用程序需要所有数据的全局视图，你可以提供一个聚合数据的集群，使用镜像将所有数据中心的本地集群镜像聚合起来。聚合集群用于需要全部数据集的应用程序读取。
  <p>
  这并是不唯一的部署模式，可以通过广域网读取或者写入到远程的Kafka集群，但是这显然会增加获取集群的延时。
  <p>
  Kafka能在生产端和消费端很轻易的批处理数据，所以即使在高延时的连接中也可以实现高吞吐量。为此.虽然我们可能需要在生产端，消费端还有broker端增加TCP 套接字缓冲区大小，修改如下参数配置<code>socket.send.buffer.bytes</code> 和 <code>socket.receive.buffer.bytes</code>。具体请参见<a href="http://en.wikipedia.org/wiki/Bandwidth-delay_product">这里</a>。
  <p>
  通常不建议在高延时链路的情况下部署一个跨越多个数据中心的Kafka集群。这将对Kafka写入和ZooKeeper写入产生非常高的复制延时，当各位置节点之间的网络不可用时，Kafka和ZooKeeper也将不保证可用

  <h3><a id="config" href="#config">6.3 Kafka 配置</a></h3>

  <h4><a id="clientconfig" href="#clientconfig">重要的客户端配置</a></h4>
  最重要的老的 scala 版本的 producer 配置
  <ul>
      <li>acks</li>
      <li>compression</li>
      <li>sync vs async production</li>
      <li>batch size (for async producers)</li>
  </ul>
  最重要的新的 Java 版本的 producer 配置
  <ul>
      <li>acks</li>
      <li>compression</li>
      <li>batch size</li>
  </ul>
  最重要的 consumer 配置是 fetch size。
  <p>
  所有的配置请查阅 <a href="#configuration">configuration</a> 章节。
  <p>
  <h4><a id="prodconfig" href="#prodconfig">一个生产服务器配置</a></h4>
  以下是生产服务器配置示例：
  <pre class="brush: text;">
  # ZooKeeper
  zookeeper.connect=[list of ZooKeeper servers]

  # Log configuration
  num.partitions=8
  default.replication.factor=3
  log.dir=[List of directories. Kafka should have its own dedicated disk(s) or SSD(s).]

  # Other configurations
  broker.id=[An integer. Start with 0 and increment by 1 for each new broker.]
  listeners=[list of listeners]
  auto.create.topics.enable=false
  min.insync.replicas=2
  queued.max.requests=[number of concurrent requests]
  </pre>

  我们的客户端配置在不同的使用场景下需要相应的变化。

  <h3><a id="java" href="#java">6.4 Java 版本</a></h3>

  从安全角度来看，我们建议您使用JDK 1.8的最新发布版本，因为较早的免费版本已经披露了安全漏洞。

  LinkedIn目前正在使用G1垃圾收集器运行JDK1.8 u5（希望升级到更新的版本）。如果您决定使用G1（当前默认值），并且您仍然使用JDK1.7，请确保您使用的是u51或者以上版本。LinkedIn已经在测试中试用了u21，但是在该版本中，GC方面存在着一些问题。

  LinkedIn的调整如下所示:
  <pre class="brush: text;">
  -Xmx6g -Xms6g -XX:MetaspaceSize=96m -XX:+UseG1GC
  -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:G1HeapRegionSize=16M
  -XX:MinMetaspaceFreeRatio=50 -XX:MaxMetaspaceFreeRatio=80
  </pre>

  作为参考，下面是LinkedIn最繁忙的群集（峰值）之一的统计数据：
  <ul>
      <li>60 brokers</li>
      <li>50k partitions (replication factor 2)</li>
      <li>800k messages/sec in</li>
      <li>300 MB/sec inbound, 1 GB/sec+ outbound</li>
  </ul>

  该调整看起来相当激进，但是在集群中的所有broker的GC暂停时间90%都在大约21ms，并且每秒钟的yong GC少于一次。

  <h3><a id="hwandos" href="#hwandos">6.5 硬件和操作系统</a></h3>
  我们正在使用双四核24GB的Intel Xeon 机器。
  <p>
  您需要足够的内存来缓存活动的readers和writers。您可以通过假设您希望缓存30秒，将您的内存需求计算为write_throughput * 30来进行内存需求的后期估计。
  <p>
  磁盘的吞吐量很重要。我们有8x7200转的SATA硬盘。通常磁盘的吞吐量是瓶颈，磁盘是越多越好。您能不能从更昂贵的磁盘中受益取决于你的刷新配置（如果您经常强制刷新，那么更高转速的SAS硬盘可能更好）。

  <h4><a id="os" href="#os">OS</a></h4>
  Kafka应该在任何Unix系统运行良好，并且已经在Linux和Solaris上进行了测试。
  <p>
  我们已经发现了在windows运行的一些问题，目前windows还不是一个理想的支持平台，虽然我们很乐意改变这个问题。
  <p>
  Kafka不需要太多的操作系统层面的调优，但是有两个潜在重要的操作系统级别的配置：
  <ul>
      <li>文件描述符限制: Kafka把文件描述符用于日志段和打开连接。如果一个broker上有许多分区，则考虑broker至少(number_of_partitions)*(partition_size/segment_size) 个文件描述符来跟踪所有的日志段和broker所创建的连接。我们推荐每一个broker一开始至少配置100000个文件描述符。
      <li>最大套接字缓冲区大小：可以增加以实现数据中心之间的高性能数据传输，如<a href="http://www.psc.edu/index.php/networking/641-tcp-tune">此处所述</a>。
  </ul>
  <p>

  <h4><a id="diskandfs" href="#diskandfs">磁盘和文件系统</a></h4>
  我们建议使用多个驱动器以获得良好的吞吐量，并且为了确保良好的延迟，不应该与应用程序日志或其他的操作系统文件系统活动共享用于Kafka数据的相同驱动器。您可以将这些驱动器RAID成单个卷或格式，并且把每个驱动器挂载到它自己的目录。Kafka的副本冗余功能可以由RAID或者应用程序级别提供，可以折衷选择实现。
  <p>
  如果您配置了多个数据目录，那么分区将被循环分配到数据目录。每个分区只属于一个数据目录，如果分区间的数据不均衡，则可能导致磁盘间的负载不均衡。
  <p>
  RAID可以更好地平衡磁盘之间的负载（尽管似乎并不总是如此），因为它在较低的级别上进行平衡负载。 RAID的主要缺点是通常会大幅度影响写入性能并且降低可用磁盘空间。
  <p>
  RAID的另一个潜在好处是能够容忍磁盘故障。 然而，我们的经验是，重建RAID阵列是I/O密集型操作以至于服务器不可用，所以这不提供太多的实际可用性改进。

  <h4><a id="appvsosflush" href="#appvsosflush">应用程序 vs. OS 刷新管理</a></h4>
  Kafka总是立即将所有数据写入文件系统，并支持配置刷新策略的功能，该策略控制何时将数据从OS缓存中强制刷新到磁盘上。该刷新策略可以控制在一段时间之后或者在写入一定数量的消息之后把数据持久化到磁盘。这里有几个可选配置项。
  <p>
  Kafka最总必须调用fsync指令才能知道数据已经被刷新。当从任何不被fsync所知的日志段崩溃中恢复时，Kafka将通过每个消息的CRC来检查其完整性，并且在启动时执行的恢复过程中会重建相应的偏移量索引文件。
  <p>
  请注意，Kafka中的持久性并不需要将数据同步到磁盘，因为失败的节点将始终从其副本中恢复。
  <p>
  我们建议使用完全禁用应用程序fsync的默认刷新设置。这意味着依靠操作系统和Kafka自己的后台完成的刷新操作。这种设置对大多数用途是最好的选择：无需调整，巨大的吞吐量和延时，以及完全的恢复保证。我们认为通过复制提供的保证比同步到本地磁盘更好，但是一些偏执的人仍然可能喜欢让OS，Kafka和应用程序级别的fsync策略都得到支持。
  <p>
  使用应用程序级别刷新设置的缺点是它的磁盘使用模式效率低下（它是操作系统在重新排序时没有什么回旋余地），并且在大多数Linux文件系统block中fsync写入文件时会引入延时，而后台刷新则会做更多粒度的页面级锁定。
  <p>
  通常你不需要对文件系统进行任何低级别的调整，但是，在接下来的几节中，我们也会介绍其中的一些内容，以防万一。

  <h4><a id="linuxflush" href="#linuxflush">理解Linux操作系统刷新行为</a></h4>

  在Linux中，写入文件系统的数据在<a href="http://en.wikipedia.org/wiki/Page_cache">pagecache</a>中保存，直到必须写入磁盘（由于应用程序级别的fsync或操作系统自己的刷新策略）。数据的刷新是通过一组叫做pdflush的后台线程来完成的（或者在2.6.32内核中的“flusher threads”）。
  <p>
  pdflush有一个可配置的策略，可以控制在缓存中可以维护多少脏数据，以及多久时间之前必须将数据写回到磁盘。
  该策略详情请参阅<a href="http://web.archive.org/web/20160518040713/http://www.westnet.com/~gsmith/content/linux-pdflush.htm">这里</a>。
  当Pdflush无法跟上数据写入的速率时，最终会导致写入过程阻塞发生写入延时来减慢数据的堆积。
  <p>
  您可以通过执行下面命令来查看当前OS内存使用状态
  <pre class="brush: bash;"> &gt; cat /proc/meminfo </pre>
  这些值的含义在上面的链接中有描述。
    <p>
  相对于进程内缓存，使用 pagecache 来存储将被写入到磁盘的数据有几个优势：
  <ul>
    <li>I/O 调度器将一批连续的小写入转换成为更大的物理写入，从而提高吞吐量。
    <li>I/O调度器将尝试重新排序写入操作，以尽量减少磁盘磁头的移动，从而提高吞吐量I/O
    <li>它会自动使用机器上的所有可用内存
  </ul>

  <h4><a id="filesystems" href="#filesystems">文件系统的选择</a></h4>
  <p>Kafka在磁盘上使用常规的文件，不依赖于特定的文件系统。 然而，使用最多的两个文件系统是EXT4和XFS。 从历史上看，EXT4使用更多，但最近对XFS文件系统的改进已经表明它对Kafka的负载具有更好的性能，而且不会影响稳定性。</p>
  <p>通过尝试各种文件系统的创建和挂载选项，在具有重要消息负载的集群上执行对比测试。Kafka监测的主要指标是“Request Local Time”，它表示追加操作的时间。XFS的本地时间更短（160ms比250ms +最好的EXT4配置），以及更低的平均等待时间。 随着磁盘性能变化，XFS的性能也表现出较小的波动。</p>
  <h5><a id="generalfs" href="#generalfs">一般文件系统注意事项</a></h5>
    对于用于数据目录的任何文件系统，在Linux系统上，建议在挂载时使用以下选项：
    <ul>
    <li>noatime：此选项禁止在读取文件时更新文件的atime（上次访问时间）属性。 这可以消除大量的文件系统写入，特别是在引导consumer的情况下。 Kafka根本不依赖atime属性，因此禁用这个属性是安全的。</li>
  </ul>
  <h5><a id="xfs" href="#xfs">XFS 注意事项</a></h5>
    XFS文件系统具有大量的自动调整功能，因此无需在文件系统创建时或挂载时对默认设置进行任何更改。 唯一值得考虑的调整参数是：
  <ul>
    <li>largeio: 这会影响统计调用报告的首选I/O大小。 虽然这可以允许在更大的磁盘写入时获得更高的性能，但是实际上它对性能的影响很小或者没有影响。</li>
    <li>nobarrier: 对于具有 battery-backed cache 的底层设备，此选项可以通过禁用定期写入刷新来提供更多的性能。 但是，如果底层设备运行良好，则会向文件系统报告不需要刷新，此选项不起作用。</li>
  </ul>
  <h5><a id="ext4" href="#ext4">EXT4 注意事项</a></h5>
    EXT4是Kafka数据目录的文件系统的一个可选择的选项，但是为了获得最高的性能需要调整几个挂载选项。 另外，这些选项在故障情况下通常是不安全的，并且会导致更多的数据丢失和损坏。 对于单个 broker 失败，这不是一个问题，因为可以擦除磁盘，并从集群重建副本。 但在诸如停电等多故障情况下，这可能意味着不容易恢复损坏的底层文件系统（数据）。 以下选项可以调整：
  <ul>
    <li>data=writeback:Ext4默认为data = ordered，这会导致某些写入操作上有很强的顺序性。Kafka不需要这样的顺序，因为它在所有未刷新的日志上进行非常偏执的数据恢复。此设置消除了排序约束，似乎显著减少了延迟。
    <li>Disabling journaling: Journaling 是一个折衷：在服务器崩溃之后，它会使重新启动更快，但会引入大量额外的锁定，从而增加写入性能的差异。那些不关心重启时间，想要减少写入延迟尖峰的主要来源，可以完全关闭Journaling。
    <li>commit=num_secs: 这调整了ext4向其元数据日志提交的频率。将其设置为较低的值可以减少崩溃期间未刷新数据的丢失。将其设置为更高的值则将提高吞吐量。
    <li>nobh: 当使用 data=writeback 模式时，此设置控制额外的排序保证。 Kafka应该是安全的，因为我们不依赖写入顺序并提高吞吐量和延迟。
    <li>delalloc: 延迟分配意味着文件系统避免分配任何 block 直到物理写入发生。这使得ext4可以在很大程度上分配连续区域而不是较小的页面，并有助于确保数据连续写入。这个功能非常适合吞吐量。它似乎涉及到文件系统中的一些锁定，这增加了一些延迟差异。
  </ul>

  <h3><a id="monitoring" href="#monitoring">6.6 Monitoring</a></h3>

  Kafka uses Yammer Metrics for metrics reporting in the server and Scala clients. The Java clients use Kafka Metrics, a built-in metrics registry that minimizes transitive dependencies pulled into client applications. Both expose metrics via JMX and can be configured to report stats using pluggable stats reporters to hook up to your monitoring system.
  <p>
  The easiest way to see the available metrics is to fire up jconsole and point it at a running kafka client or server; this will allow browsing all metrics with JMX.
  <p>
  We do graphing and alerting on the following metrics:
  <table class="data-table">
  <tbody><tr>
        <th>Description</th>
        <th>Mbean name</th>
        <th>Normal value</th>
      </tr>
      <tr>
        <td>Message in rate</td>
        <td>kafka.server:type=BrokerTopicMetrics,name=MessagesInPerSec</td>
        <td></td>
      </tr>
      <tr>
        <td>Byte in rate</td>
        <td>kafka.server:type=BrokerTopicMetrics,name=BytesInPerSec</td>
        <td></td>
      </tr>
      <tr>
        <td>Request rate</td>
        <td>kafka.network:type=RequestMetrics,name=RequestsPerSec,request={Produce|FetchConsumer|FetchFollower}</td>
        <td></td>
      </tr>
      <tr>
        <td>Byte out rate</td>
        <td>kafka.server:type=BrokerTopicMetrics,name=BytesOutPerSec</td>
        <td></td>
      </tr>
      <tr>
        <td>Log flush rate and time</td>
        <td>kafka.log:type=LogFlushStats,name=LogFlushRateAndTimeMs</td>
        <td></td>
      </tr>
      <tr>
        <td># of under replicated partitions (|ISR| &lt |all replicas|)</td>
        <td>kafka.server:type=ReplicaManager,name=UnderReplicatedPartitions</td>
        <td>0</td>
      </tr>
      <tr>
        <td># of under minIsr partitions (|ISR| &lt min.insync.replicas)</td>
        <td>kafka.server:type=ReplicaManager,name=UnderMinIsrPartitionCount</td>
        <td>0</td>
      </tr>
      <tr>
        <td># of offline log directories</td>
        <td>kafka.log:type=LogManager,name=OfflineLogDirectoryCount</td>
        <td>0</td>
      </tr>
      <tr>
        <td>Is controller active on broker</td>
        <td>kafka.controller:type=KafkaController,name=ActiveControllerCount</td>
        <td>only one broker in the cluster should have 1</td>
      </tr>
      <tr>
        <td>Leader election rate</td>
        <td>kafka.controller:type=ControllerStats,name=LeaderElectionRateAndTimeMs</td>
        <td>non-zero when there are broker failures</td>
      </tr>
      <tr>
        <td>Unclean leader election rate</td>
        <td>kafka.controller:type=ControllerStats,name=UncleanLeaderElectionsPerSec</td>
        <td>0</td>
      </tr>
      <tr>
        <td>Partition counts</td>
        <td>kafka.server:type=ReplicaManager,name=PartitionCount</td>
        <td>mostly even across brokers</td>
      </tr>
      <tr>
        <td>Leader replica counts</td>
        <td>kafka.server:type=ReplicaManager,name=LeaderCount</td>
        <td>mostly even across brokers</td>
      </tr>
      <tr>
        <td>ISR shrink rate</td>
        <td>kafka.server:type=ReplicaManager,name=IsrShrinksPerSec</td>
        <td>If a broker goes down, ISR for some of the partitions will
    shrink. When that broker is up again, ISR will be expanded
    once the replicas are fully caught up. Other than that, the
    expected value for both ISR shrink rate and expansion rate is 0. </td>
      </tr>
      <tr>
        <td>ISR expansion rate</td>
        <td>kafka.server:type=ReplicaManager,name=IsrExpandsPerSec</td>
        <td>See above</td>
      </tr>
      <tr>
        <td>Max lag in messages btw follower and leader replicas</td>
        <td>kafka.server:type=ReplicaFetcherManager,name=MaxLag,clientId=Replica</td>
        <td>lag should be proportional to the maximum batch size of a produce request.</td>
      </tr>
      <tr>
        <td>Lag in messages per follower replica</td>
        <td>kafka.server:type=FetcherLagMetrics,name=ConsumerLag,clientId=([-.\w]+),topic=([-.\w]+),partition=([0-9]+)</td>
        <td>lag should be proportional to the maximum batch size of a produce request.</td>
      </tr>
      <tr>
        <td>Requests waiting in the producer purgatory</td>
        <td>kafka.server:type=DelayedOperationPurgatory,name=PurgatorySize,delayedOperation=Produce</td>
        <td>non-zero if ack=-1 is used</td>
      </tr>
      <tr>
        <td>Requests waiting in the fetch purgatory</td>
        <td>kafka.server:type=DelayedOperationPurgatory,name=PurgatorySize,delayedOperation=Fetch</td>
        <td>size depends on fetch.wait.max.ms in the consumer</td>
      </tr>
      <tr>
        <td>Request total time</td>
        <td>kafka.network:type=RequestMetrics,name=TotalTimeMs,request={Produce|FetchConsumer|FetchFollower}</td>
        <td>broken into queue, local, remote and response send time</td>
      </tr>
      <tr>
        <td>Time the request waits in the request queue</td>
        <td>kafka.network:type=RequestMetrics,name=RequestQueueTimeMs,request={Produce|FetchConsumer|FetchFollower}</td>
        <td></td>
      </tr>
      <tr>
        <td>Time the request is processed at the leader</td>
        <td>kafka.network:type=RequestMetrics,name=LocalTimeMs,request={Produce|FetchConsumer|FetchFollower}</td>
        <td></td>
      </tr>
      <tr>
        <td>Time the request waits for the follower</td>
        <td>kafka.network:type=RequestMetrics,name=RemoteTimeMs,request={Produce|FetchConsumer|FetchFollower}</td>
        <td>non-zero for produce requests when ack=-1</td>
      </tr>
      <tr>
          <td>Time the request waits in the response queue</td>
          <td>kafka.network:type=RequestMetrics,name=ResponseQueueTimeMs,request={Produce|FetchConsumer|FetchFollower}</td>
          <td></td>
      </tr>
      <tr>
        <td>Time to send the response</td>
        <td>kafka.network:type=RequestMetrics,name=ResponseSendTimeMs,request={Produce|FetchConsumer|FetchFollower}</td>
        <td></td>
      </tr>
      <tr>
        <td>Number of messages the consumer lags behind the producer by. Published by the consumer, not broker.</td>
        <td>
          <p><em>Old consumer:</em> kafka.consumer:type=ConsumerFetcherManager,name=MaxLag,clientId=([-.\w]+)</p>
          <p><em>New consumer:</em> kafka.consumer:type=consumer-fetch-manager-metrics,client-id={client-id} Attribute: records-lag-max</p>
        </td>
        <td></td>
      </tr>
      <tr>
        <td>The average fraction of time the network processors are idle</td>
        <td>kafka.network:type=SocketServer,name=NetworkProcessorAvgIdlePercent</td>
        <td>between 0 and 1, ideally &gt 0.3</td>
      </tr>
      <tr>
        <td>The average fraction of time the request handler threads are idle</td>
        <td>kafka.server:type=KafkaRequestHandlerPool,name=RequestHandlerAvgIdlePercent</td>
        <td>between 0 and 1, ideally &gt 0.3</td>
      </tr>
      <tr>
        <td>Bandwidth quota metrics per (user, client-id), user or client-id</td>
        <td>kafka.server:type={Produce|Fetch},user=([-.\w]+),client-id=([-.\w]+)</td>
        <td>Two attributes. throttle-time indicates the amount of time in ms the client was throttled. Ideally = 0.
            byte-rate indicates the data produce/consume rate of the client in bytes/sec.
            For (user, client-id) quotas, both user and client-id are specified. If per-client-id quota is applied to the client, user is not specified. If per-user quota is applied, client-id is not specified.</td>
      </tr>
      <tr>
        <td>Request quota metrics per (user, client-id), user or client-id</td>
        <td>kafka.server:type=Request,user=([-.\w]+),client-id=([-.\w]+)</td>
        <td>Two attributes. throttle-time indicates the amount of time in ms the client was throttled. Ideally = 0.
            request-time indicates the percentage of time spent in broker network and I/O threads to process requests from client group.
            For (user, client-id) quotas, both user and client-id are specified. If per-client-id quota is applied to the client, user is not specified. If per-user quota is applied, client-id is not specified.</td>
      </tr>
      <tr>
        <td>Requests exempt from throttling</td>
        <td>kafka.server:type=Request</td>
        <td>exempt-throttle-time indicates the percentage of time spent in broker network and I/O threads to process requests
            that are exempt from throttling.</td>
      </tr>
  </tbody></table>

  <h4><a id="selector_monitoring" href="#selector_monitoring">Common monitoring metrics for producer/consumer/connect/streams</a></h4>

  The following metrics are available on producer/consumer/connector/streams instances.  For specific metrics, please see following sections.

  <table class="data-table">
    <tbody>
      <tr>
        <th>Metric/Attribute name</th>
        <th>Description</th>
        <th>Mbean name</th>
      </tr>
      <tr>
        <td>connection-close-rate</td>
        <td>Connections closed per second in the window.</td>
        <td>kafka.[producer|consumer|connect]:type=[producer|consumer|connect]-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>connection-creation-rate</td>
        <td>New connections established per second in the window.</td>
        <td>kafka.[producer|consumer|connect]:type=[producer|consumer|connect]-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>network-io-rate</td>
        <td>The average number of network operations (reads or writes) on all connections per second.</td>
        <td>kafka.[producer|consumer|connect]:type=[producer|consumer|connect]-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>outgoing-byte-rate</td>
        <td>The average number of outgoing bytes sent per second to all servers.</td>
        <td>kafka.[producer|consumer|connect]:type=[producer|consumer|connect]-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>request-rate</td>
        <td>The average number of requests sent per second.</td>
        <td>kafka.[producer|consumer|connect]:type=[producer|consumer|connect]-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>request-size-avg</td>
        <td>The average size of all requests in the window.</td>
        <td>kafka.[producer|consumer|connect]:type=[producer|consumer|connect]-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>request-size-max</td>
        <td>The maximum size of any request sent in the window.</td>
        <td>kafka.[producer|consumer|connect]:type=[producer|consumer|connect]-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>incoming-byte-rate</td>
        <td>Bytes/second read off all sockets.</td>
        <td>kafka.[producer|consumer|connect]:type=[producer|consumer|connect]-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>response-rate</td>
        <td>Responses received sent per second.</td>
        <td>kafka.[producer|consumer|connect]:type=[producer|consumer|connect]-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>select-rate</td>
        <td>Number of times the I/O layer checked for new I/O to perform per second.</td>
        <td>kafka.[producer|consumer|connect]:type=[producer|consumer|connect]-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>io-wait-time-ns-avg</td>
        <td>The average length of time the I/O thread spent waiting for a socket ready for reads or writes in nanoseconds.</td>
        <td>kafka.[producer|consumer|connect]:type=[producer|consumer|connect]-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>io-wait-ratio</td>
        <td>The fraction of time the I/O thread spent waiting.</td>
        <td>kafka.[producer|consumer|connect]:type=[producer|consumer|connect]-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>io-time-ns-avg</td>
        <td>The average length of time for I/O per select call in nanoseconds.</td>
        <td>kafka.[producer|consumer|connect]:type=[producer|consumer|connect]-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>io-ratio</td>
        <td>The fraction of time the I/O thread spent doing I/O.</td>
        <td>kafka.[producer|consumer|connect]:type=[producer|consumer|connect]-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>connection-count</td>
        <td>The current number of active connections.</td>
        <td>kafka.[producer|consumer|connect]:type=[producer|consumer|connect]-metrics,client-id=([-.\w]+)</td>
      </tr>
    </tbody>
  </table>

  <h4><a id="common_node_monitoring" href="#common_node_monitoring">Common Per-broker metrics for producer/consumer/connect/streams</a></h4>

  The following metrics are available on producer/consumer/connector/streams instances.  For specific metrics, please see following sections.

  <table class="data-table">
    <tbody>
      <tr>
        <th>Metric/Attribute name</th>
        <th>Description</th>
        <th>Mbean name</th>
      </tr>
      <tr>
        <td>outgoing-byte-rate</td>
        <td>The average number of outgoing bytes sent per second for a node.</td>
        <td>kafka.producer:type=[consumer|producer|connect]-node-metrics,client-id=([-.\w]+),node-id=([0-9]+)</td>
      </tr>
      <tr>
        <td>request-rate</td>
        <td>The average number of requests sent per second for a node.</td>
        <td>kafka.producer:type=[consumer|producer|connect]-node-metrics,client-id=([-.\w]+),node-id=([0-9]+)</td>
      </tr>
      <tr>
        <td>request-size-avg</td>
        <td>The average size of all requests in the window for a node.</td>
        <td>kafka.producer:type=[consumer|producer|connect]-node-metrics,client-id=([-.\w]+),node-id=([0-9]+)</td>
      </tr>
      <tr>
        <td>request-size-max</td>
        <td>The maximum size of any request sent in the window for a node.</td>
        <td>kafka.producer:type=[consumer|producer|connect]-node-metrics,client-id=([-.\w]+),node-id=([0-9]+)</td>
      </tr>
      <tr>
        <td>incoming-byte-rate</td>
        <td>The average number of responses received per second for a node.</td>
        <td>kafka.producer:type=[consumer|producer|connect]-node-metrics,client-id=([-.\w]+),node-id=([0-9]+)</td>
      </tr>
      <tr>
        <td>request-latency-avg</td>
        <td>The average request latency in ms for a node.</td>
        <td>kafka.producer:type=[consumer|producer|connect]-node-metrics,client-id=([-.\w]+),node-id=([0-9]+)</td>
      </tr>
      <tr>
        <td>request-latency-max</td>
        <td>The maximum request latency in ms for a node.</td>
        <td>kafka.producer:type=[consumer|producer|connect]-node-metrics,client-id=([-.\w]+),node-id=([0-9]+)</td>
      </tr>
      <tr>
        <td>response-rate</td>
        <td>Responses received sent per second for a node.</td>
        <td>kafka.producer:type=[consumer|producer|connect]-node-metrics,client-id=([-.\w]+),node-id=([0-9]+)</td>
      </tr>
    </tbody>
  </table>

  <h4><a id="producer_monitoring" href="#producer_monitoring">Producer monitoring</a></h4>

  The following metrics are available on producer instances.

  <table class="data-table">
  <tbody><tr>
        <th>Metric/Attribute name</th>
        <th>Description</th>
        <th>Mbean name</th>
      </tr>
        <tr>
        <td>waiting-threads</td>
        <td>The number of user threads blocked waiting for buffer memory to enqueue their records.</td>
        <td>kafka.producer:type=producer-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>buffer-total-bytes</td>
        <td>The maximum amount of buffer memory the client can use (whether or not it is currently used).</td>
        <td>kafka.producer:type=producer-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>buffer-available-bytes</td>
        <td>The total amount of buffer memory that is not being used (either unallocated or in the free list).</td>
        <td>kafka.producer:type=producer-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>bufferpool-wait-time</td>
        <td>The fraction of time an appender waits for space allocation.</td>
        <td>kafka.producer:type=producer-metrics,client-id=([-.\w]+)</td>
      </tr>

  </tbody></table>

  <h5><a id="producer_sender_monitoring" href="#producer_sender_monitoring">Producer Sender Metrics</a></h5>

  <!--#include virtual="generated/producer_metrics.html" -->


  <h4><a id="new_consumer_monitoring" href="#new_consumer_monitoring">New consumer monitoring</a></h4>

  The following metrics are available on new consumer instances.

  <h5><a id="new_consumer_group_monitoring" href="#new_consumer_group_monitoring">Consumer Group Metrics</a></h5>
  <table class="data-table">
    <tbody>
      <tr>
        <th>Metric/Attribute name</th>
        <th>Description</th>
        <th>Mbean name</th>
      </tr>
      <tr>
        <td>commit-latency-avg</td>
        <td>The average time taken for a commit request</td>
        <td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>commit-latency-max</td>
        <td>The max time taken for a commit request</td>
        <td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>commit-rate</td>
        <td>The number of commit calls per second</td>
        <td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>assigned-partitions</td>
        <td>The number of partitions currently assigned to this consumer</td>
        <td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>heartbeat-response-time-max</td>
        <td>The max time taken to receive a response to a heartbeat request</td>
        <td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>heartbeat-rate</td>
        <td>The average number of heartbeats per second</td>
        <td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>join-time-avg</td>
        <td>The average time taken for a group rejoin</td>
        <td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>join-time-max</td>
        <td>The max time taken for a group rejoin</td>
        <td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>join-rate</td>
        <td>The number of group joins per second</td>
        <td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>sync-time-avg</td>
        <td>The average time taken for a group sync</td>
        <td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>sync-time-max</td>
        <td>The max time taken for a group sync</td>
        <td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>sync-rate</td>
        <td>The number of group syncs per second</td>
        <td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>last-heartbeat-seconds-ago</td>
        <td>The number of seconds since the last controller heartbeat</td>
        <td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td>
      </tr>
    </tbody>
  </table>

  <h5><a id="new_consumer_fetch_monitoring" href="#new_consumer_fetch_monitoring">Consumer Fetch Metrics</a></h5>

  <!--#include virtual="generated/consumer_metrics.html" -->

  <h4><a id="connect_monitoring" href="#connect_monitoring">Connect Monitoring</a></h4>

  A Connect worker process contains all the producer and consumer metrics as well as metrics specific to Connect.
  The worker process itself has a number of metrics, while each connector and task have additional metrics.

  <!--#include virtual="generated/connect_metrics.html" -->

  <h4><a id="kafka_streams_monitoring" href="#kafka_streams_monitoring">Streams Monitoring</a></h4>

  A Kafka Streams instance contains all the producer and consumer metrics as well as additional metrics specific to streams.
  By default Kafka Streams has metrics with two recording levels: debug and info. The debug level records all metrics, while
  the info level records only the thread-level metrics.

  <p>
    Note that the metrics have a 3-layer hierarchy. At the top level there are per-thread metrics. Each thread has tasks, with their
    own metrics. Each task has a number of processor nodes, with their own metrics. Each task also has a number of state stores
    and record caches, all with their own metrics.
  </p>
  
  Use the following configuration option to specify which metrics
  you want collected:

<pre>metrics.recording.level="info"</pre>

<h5><a id="kafka_streams_thread_monitoring" href="#kafka_streams_thread_monitoring">Thread Metrics</a></h5>
All the following metrics have a recording level of ``info``:
<table class="data-table">
    <tbody>
      <tr>
        <th>Metric/Attribute name</th>
        <th>Description</th>
        <th>Mbean name</th>
      </tr>
       <tr>
        <td>commit-latency-avg</td>
        <td>The average execution time in ms for committing, across all running tasks of this thread.</td>
        <td>kafka.streams:type=stream-metrics,client-id=([-.\w]+)</td>
       </tr>
        <tr>
        <td>commit-latency-max</td>
        <td>The maximum execution time in ms for committing across all running tasks of this thread.</td>
        <td>kafka.streams:type=stream-metrics,client-id=([-.\w]+)</td>
       </tr>
       <tr>
        <td>poll-latency-avg</td>
        <td>The average execution time in ms for polling, across all running tasks of this thread.</td>
        <td>kafka.streams:type=stream-metrics,client-id=([-.\w]+)</td>
       </tr>
        <tr>
        <td>poll-latency-max</td>
        <td>The maximum execution time in ms for polling across all running tasks of this thread.</td>
        <td>kafka.streams:type=stream-metrics,client-id=([-.\w]+)</td>
       </tr>
       <tr>
        <td>process-latency-avg</td>
        <td>The average execution time in ms for processing, across all running tasks of this thread.</td>
        <td>kafka.streams:type=stream-metrics,client-id=([-.\w]+)</td>
       </tr>
       <tr>
        <td>process-latency-max</td>
        <td>The maximum execution time in ms for processing across all running tasks of this thread.</td>
        <td>kafka.streams:type=stream-metrics,client-id=([-.\w]+)</td>
       </tr>
       <tr>
        <td>punctuate-latency-avg</td>
        <td>The average execution time in ms for punctuating, across all running tasks of this thread.</td>
        <td>kafka.streams:type=stream-metrics,client-id=([-.\w]+)</td>
       </tr>
       <tr>
        <td>punctuate-latency-max</td>
        <td>The maximum execution time in ms for punctuating across all running tasks of this thread.</td>
        <td>kafka.streams:type=stream-metrics,client-id=([-.\w]+)</td>
       </tr>
       <tr>
        <td>commit-rate</td>
        <td>The average number of commits per second across all tasks.</td>
        <td>kafka.streams:type=stream-metrics,client-id=([-.\w]+)</td>
       </tr>
       <tr>
        <td>poll-rate</td>
        <td>The average number of polls per second across all tasks.</td>
        <td>kafka.streams:type=stream-metrics,client-id=([-.\w]+)</td>
       </tr>
       <tr>
        <td>process-rate</td>
        <td>The average number of process calls per second across all tasks.</td>
        <td>kafka.streams:type=stream-metrics,client-id=([-.\w]+)</td>
       </tr>
       <tr>
        <td>punctuate-rate</td>
        <td>The average number of punctuates per second across all tasks.</td>
        <td>kafka.streams:type=stream-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>task-created-rate</td>
        <td>The average number of newly created tasks per second.</td>
        <td>kafka.streams:type=stream-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>task-closed-rate</td>
        <td>The average number of tasks closed per second.</td>
        <td>kafka.streams:type=stream-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>skipped-records-rate</td>
        <td>The average number of skipped records per second. </td>
        <td>kafka.streams:type=stream-metrics,client-id=([-.\w]+)</td>
      </tr>
 </tbody>
</table>

<h5><a id="kafka_streams_task_monitoring" href="#kafka_streams_task_monitoring">Task Metrics</a></h5>
All the following metrics have a recording level of ``debug``:
 <table class="data-table">
      <tbody>
      <tr>
        <th>Metric/Attribute name</th>
        <th>Description</th>
        <th>Mbean name</th>
      </tr>
      <tr>
        <td>commit-latency-avg</td>
        <td>The average commit time in ns for this task. </td>
        <td>kafka.streams:type=stream-task-metrics,client-id=([-.\w]+),task-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>commit-latency-max</td>
        <td>The maximum commit time in ns for this task. </td>
        <td>kafka.streams:type=stream-task-metrics,client-id=([-.\w]+),task-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>commit-rate</td>
        <td>The average number of commit calls per second. </td>
        <td>kafka.streams:type=stream-task-metrics,client-id=([-.\w]+),task-id=([-.\w]+)</td>
      </tr>
 </tbody>
</table>

 <h5><a id="kafka_streams_node_monitoring" href="#kafka_streams_node_monitoring">Processor Node Metrics</a></h5>
All the following metrics have a recording level of ``debug``:
 <table class="data-table">
      <tbody>
      <tr>
        <th>Metric/Attribute name</th>
        <th>Description</th>
        <th>Mbean name</th>
      </tr>
      <tr>
        <td>process-latency-avg</td>
        <td>The average process execution time in ns. </td>
        <td>kafka.streams:type=stream-processor-node-metrics,client-id=([-.\w]+),task-id=([-.\w]+),processor-node-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>process-latency-max</td>
        <td>The maximum process execution time in ns. </td>
        <td>kafka.streams:type=stream-processor-node-metrics,client-id=([-.\w]+),task-id=([-.\w]+),processor-node-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>punctuate-latency-avg</td>
        <td>The average punctuate execution time in ns. </td>
        <td>kafka.streams:type=stream-processor-node-metrics,client-id=([-.\w]+),task-id=([-.\w]+),processor-node-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>punctuate-latency-max</td>
        <td>The maximum punctuate execution time in ns. </td>
        <td>kafka.streams:type=stream-processor-node-metrics,client-id=([-.\w]+),task-id=([-.\w]+),processor-node-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>create-latency-avg</td>
        <td>The average create execution time in ns. </td>
        <td>kafka.streams:type=stream-processor-node-metrics,client-id=([-.\w]+),task-id=([-.\w]+),processor-node-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>create-latency-max</td>
        <td>The maximum create execution time in ns. </td>
        <td>kafka.streams:type=stream-processor-node-metrics,client-id=([-.\w]+),task-id=([-.\w]+),processor-node-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>destroy-latency-avg</td>
        <td>The average destroy execution time in ns. </td>
        <td>kafka.streams:type=stream-processor-node-metrics,client-id=([-.\w]+),task-id=([-.\w]+),processor-node-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>destroy-latency-max</td>
        <td>The maximum destroy execution time in ns. </td>
        <td>kafka.streams:type=stream-processor-node-metrics,client-id=([-.\w]+),task-id=([-.\w]+),processor-node-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>process-rate</td>
        <td>The average number of process operations per second. </td>
        <td>kafka.streams:type=stream-processor-node-metrics,client-id=([-.\w]+),task-id=([-.\w]+),processor-node-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>punctuate-rate</td>
        <td>The average number of punctuate operations per second. </td>
        <td>kafka.streams:type=stream-processor-node-metrics,client-id=([-.\w]+),task-id=([-.\w]+),processor-node-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>create-rate</td>
        <td>The average number of create operations per second. </td>
        <td>kafka.streams:type=stream-processor-node-metrics,client-id=([-.\w]+),task-id=([-.\w]+),processor-node-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>destroy-rate</td>
        <td>The average number of destroy operations per second. </td>
        <td>kafka.streams:type=stream-processor-node-metrics,client-id=([-.\w]+),task-id=([-.\w]+),processor-node-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>forward-rate</td>
        <td>The average rate of records being forwarded downstream, from source nodes only, per second. </td>
        <td>kafka.streams:type=stream-processor-node-metrics,client-id=([-.\w]+),task-id=([-.\w]+),processor-node-id=([-.\w]+)</td>
      </tr>
 </tbody>
 </table>

 <h5><a id="kafka_streams_store_monitoring" href="#kafka_streams_store_monitoring">State Store Metrics</a></h5>
All the following metrics have a recording level of ``debug``:

 <table class="data-table">
      <tbody>
      <tr>
        <th>Metric/Attribute name</th>
        <th>Description</th>
        <th>Mbean name</th>
      </tr>
      <tr>
        <td>put-latency-avg</td>
        <td>The average put execution time in ns. </td>
        <td>kafka.streams:type=stream-[store-type]-state-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-type]-state-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>put-latency-max</td>
        <td>The maximum put execution time in ns. </td>
        <td>kafka.streams:type=stream-[store-type]-state-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-type]-state-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>put-if-absent-latency-avg</td>
        <td>The average put-if-absent execution time in ns. </td>
        <td>kafka.streams:type=stream-[store-type]-state-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-type]-state-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>put-if-absent-latency-max</td>
        <td>The maximum put-if-absent execution time in ns. </td>
        <td>kafka.streams:type=stream-[store-type]-state-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-type]-state-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>get-latency-avg</td>
        <td>The average get execution time in ns. </td>
        <td>kafka.streams:type=stream-[store-type]-state-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-type]-state-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>get-latency-max</td>
        <td>The maximum get execution time in ns. </td>
        <td>kafka.streams:type=stream-[store-type]-state-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-type]-state-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>delete-latency-avg</td>
        <td>The average delete execution time in ns. </td>
        <td>kafka.streams:type=stream-[store-type]-state-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-type]-state-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>delete-latency-max</td>
        <td>The maximum delete execution time in ns. </td>
        <td>kafka.streams:type=stream-[store-type]-state-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-type]-state-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>put-all-latency-avg</td>
        <td>The average put-all execution time in ns. </td>
        <td>kafka.streams:type=stream-[store-type]-state-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-type]-state-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>put-all-latency-max</td>
        <td>The maximum put-all execution time in ns. </td>
        <td>kafka.streams:type=stream-[store-type]-state-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-type]-state-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>all-latency-avg</td>
        <td>The average all operation execution time in ns. </td>
        <td>kafka.streams:type=stream-[store-type]-state-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-type]-state-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>all-latency-max</td>
        <td>The maximum all operation execution time in ns. </td>
        <td>kafka.streams:type=stream-[store-type]-state-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-type]-state-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>range-latency-avg</td>
        <td>The average range execution time in ns. </td>
        <td>kafka.streams:type=stream-[store-type]-state-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-type]-state-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>range-latency-max</td>
        <td>The maximum range execution time in ns. </td>
        <td>kafka.streams:type=stream-[store-type]-state-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-type]-state-id=([-.\w]+)</td>
      </tr>
       <tr>
        <td>flush-latency-avg</td>
        <td>The average flush execution time in ns. </td>
        <td>kafka.streams:type=stream-[store-type]-state-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-type]-state-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>flush-latency-max</td>
        <td>The maximum flush execution time in ns. </td>
        <td>kafka.streams:type=stream-[store-type]-state-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-type]-state-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>restore-latency-avg</td>
        <td>The average restore execution time in ns. </td>
        <td>kafka.streams:type=stream-[store-type]-state-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-type]-state-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>restore-latency-max</td>
        <td>The maximum restore execution time in ns. </td>
        <td>kafka.streams:type=stream-[store-type]-state-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-type]-state-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>put-rate</td>
        <td>The average put rate for this store.</td>
        <td>kafka.streams:type=stream-[store-type]-state-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-type]-state-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>put-if-absent-rate</td>
        <td>The average put-if-absent rate for this store.</td>
        <td>kafka.streams:type=stream-[store-type]-state-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-type]-state-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>get-rate</td>
        <td>The average get rate for this store.</td>
        <td>kafka.streams:type=stream-[store-type]-state-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-type]-state-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>delete-rate</td>
        <td>The average delete rate for this store.</td>
        <td>kafka.streams:type=stream-[store-type]-state-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-type]-state-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>put-all-rate</td>
        <td>The average put-all rate for this store.</td>
        <td>kafka.streams:type=stream-[store-type]-state-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-type]-state-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>all-rate</td>
        <td>The average all operation rate for this store.</td>
        <td>kafka.streams:type=stream-[store-type]-state-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-type]-state-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>range-rate</td>
        <td>The average range rate for this store.</td>
        <td>kafka.streams:type=stream-[store-type]-state-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-type]-state-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>flush-rate</td>
        <td>The average flush rate for this store.</td>
        <td>kafka.streams:type=stream-[store-type]-state-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-type]-state-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>restore-rate</td>
        <td>The average restore rate for this store.</td>
        <td>kafka.streams:type=stream-[store-type]-state-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-type]-state-id=([-.\w]+)</td>
      </tr>
    </tbody>
 </table>

  <h5><a id="kafka_streams_cache_monitoring" href="#kafka_streams_cache_monitoring">Record Cache Metrics</a></h5>
All the following metrics have a recording level of ``debug``:

  <table class="data-table">
      <tbody>
      <tr>
        <th>Metric/Attribute name</th>
        <th>Description</th>
        <th>Mbean name</th>
      </tr>
      <tr>
        <td>hitRatio-avg</td>
        <td>The average cache hit ratio defined as the ratio of cache read hits over the total cache read requests. </td>
        <td>kafka.streams:type=stream-record-cache-metrics,client-id=([-.\w]+),task-id=([-.\w]+),record-cache-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>hitRatio-min</td>
        <td>The mininum cache hit ratio. </td>
        <td>kafka.streams:type=stream-record-cache-metrics,client-id=([-.\w]+),task-id=([-.\w]+),record-cache-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>hitRatio-max</td>
        <td>The maximum cache hit ratio. </td>
        <td>kafka.streams:type=stream-record-cache-metrics,client-id=([-.\w]+),task-id=([-.\w]+),record-cache-id=([-.\w]+)</td>
      </tr>
    </tbody>
 </table>

  <h4><a id="others_monitoring" href="#others_monitoring">Others</a></h4>

  We recommend monitoring GC time and other stats and various server stats such as CPU utilization, I/O service time, etc.

  On the client side, we recommend monitoring the message/byte rate (global and per topic), request rate/size/time, and on the consumer side, max lag in messages among all partitions and min fetch request rate. For a consumer to keep up, max lag needs to be less than a threshold and min fetch rate needs to be larger than 0.

  <h4><a id="basic_ops_audit" href="#basic_ops_audit">Audit</a></h4>
  The final alerting we do is on the correctness of the data delivery. We audit that every message that is sent is consumed by all consumers and measure the lag for this to occur. For important topics we alert if a certain completeness is not achieved in a certain time period. The details of this are discussed in KAFKA-260.

  <h3><a id="zk" href="#zk">6.7 ZooKeeper</a></h3>

  <h4><a id="zkversion" href="#zkversion">稳定版本</a></h4>
  当前稳定分支是3.4，最新版本是3.4.9。

  <h4><a id="zkops" href="#zkops">Operationalizing ZooKeeper</a></h4>
  在操作上，我们有一下符合规范的ZooKeeper安装方式：
  <ul>
    <li>在物理/硬件/网络上的冗余：尽量不要把他们放在同一个机架上，合适的硬件配置（但不要过分），尽量保持电源，网络等。一个典型的ZooKeeper集群有5或7台服务器，分别允许宕机2台和3台服务器。如果你想部署一个小型集群，3台服务器也可以部署，但是要记住，在这种情况下你只能宕机1台服务器。</li>
    <li>
      I/O隔离：如果你有大量的写入操作流入，你几乎肯定会把事务日志放在一组特定的磁盘上。写入事物日志是同步的（但为了性能会分批写入），因此并发写入会明显影响性能。数据快照是异步落盘，因此通常可以与操作系统和消息日志文件共享磁盘性能。你可以配置dataLogDir参数单独为服务器配置磁盘组。</li>
    <li>应用隔离：除非你真的了解其他应用的运行模式，否则不要和ZooKeeper安装在一起，最好是单独部署运行ZooKeeper（尽管ZooKeeper可以均衡的利用硬件资源）。</li>
    <li>谨慎使用虚拟化：他的运行状况取决于你的集群架构，读写模式和SLA，即便是由虚拟化层引入的微小开销也可能造成ZooKeeper的中断，毕竟ZooKeeper对此十分敏感。</li>
    <li>ZooKeeper配置: 他是java运行的，首先确保你给他分配足够的堆空间（我们通常配置3-5G，但这是根据我们现有数据实际情况来定的）。不幸的是，我们没有一个好的固定公式来确定他的值，但是要记住分配个ZooKeeper的堆空间越大，快照也就越大，从而会影响快照的恢复时间。实际上，如果快照变得太大（几个G），那你能需要增加initlimit参数的值，以便为服务器提供足够的时间来恢复并加入集群。</li>
    <li>监控:JMX和4个字母的命令（ZooKeeper提供的一系列命令，如：conf,cons,dump等）非常有用，他们在某些功能上重复了（这种情况下我们更喜欢4lw命令，他们似乎更容易预测情况，至少，他们和基础设施监控兼容性更好）</li>
    <li>不要过度构建集群：大型集群，尤其是大量写入的情况下，意味着大量的集群内部通信（集群成员节点的写入和后续的仲裁更新），但是过小的将集群将承担不必要的风险。添加更多的服务器可以增加集群的读取能力。</li>
  </ul>
  总体来看，我们应尽量保持zookeeper尽可能小的处理负载（标准增长容量规划） 并尽可能的简单。与官方版本相比，我们尽量对配置和应用布局不做什么更改，尽可能保持官方原版。基于这些原因，我们倾向于跳过操作系统打包的版本。因为为了有更好的表现，它倾向于把关注点放在可能“混乱”的标准系统层上。
</script>

<div class="p-ops"></div>
