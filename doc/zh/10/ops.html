<!--
 Licensed to the Apache Software Foundation (ASF) under one or more
 contributor license agreements.  See the NOTICE file distributed with
 this work for additional information regarding copyright ownership.
 The ASF licenses this file to You under the Apache License, Version 2.0
 (the "License"); you may not use this file except in compliance with
 the License.  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

 Unless required by applicable law or agreed to in writing, software
 distributed under the License is distributed on an "AS IS" BASIS,
 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 See the License for the specific language governing permissions and
 limitations under the License.
-->
<script id="ops-template" type="text/x-handlebars-template">
  
  Here is some information on actually running Kafka as a production system based on usage and experience at LinkedIn. Please send us any additional tips you know of.

  <h3><a id="basic_ops" href="#basic_ops">6.1 Basic Kafka Operations</a></h3>

  This section will review the most common operations you will perform on your Kafka cluster. All of the tools reviewed in this section are available under the <code>bin/</code> directory of the Kafka distribution and each tool will print details on all possible commandline options if it is run with no arguments.

  <h4><a id="basic_ops_add_topic" href="#basic_ops_add_topic">Adding and removing topics</a></h4>

  You have the option of either adding topics manually or having them be created automatically when data is first published to a non-existent topic. If topics are auto-created then you may want to tune the default <a href="#topicconfigs">topic configurations</a> used for auto-created topics.
  <p>
  Topics are added and modified using the topic tool:
  <pre class="brush: bash;">
  &gt; bin/kafka-topics.sh --zookeeper zk_host:port/chroot --create --topic my_topic_name
        --partitions 20 --replication-factor 3 --config x=y
  </pre>
  The replication factor controls how many servers will replicate each message that is written. If you have a replication factor of 3 then up to 2 servers can fail before you will lose access to your data. We recommend you use a replication factor of 2 or 3 so that you can transparently bounce machines without interrupting data consumption.
  <p>
  The partition count controls how many logs the topic will be sharded into. There are several impacts of the partition count. First each partition must fit entirely on a single server. So if you have 20 partitions the full data set (and read and write load) will be handled by no more than 20 servers (not counting replicas). Finally the partition count impacts the maximum parallelism of your consumers. This is discussed in greater detail in the <a href="#intro_consumers">concepts section</a>.
  <p>
  Each sharded partition log is placed into its own folder under the Kafka log directory. The name of such folders consists of the topic name, appended by a dash (-) and the partition id. Since a typical folder name can not be over 255 characters long, there will be a limitation on the length of topic names. We assume the number of partitions will not ever be above 100,000. Therefore, topic names cannot be longer than 249 characters. This leaves just enough room in the folder name for a dash and a potentially 5 digit long partition id.
  <p>
  The configurations added on the command line override the default settings the server has for things like the length of time data should be retained. The complete set of per-topic configurations is documented <a href="#topicconfigs">here</a>.

  <h4><a id="basic_ops_modify_topic" href="#basic_ops_modify_topic">Modifying topics</a></h4>

  You can change the configuration or partitioning of a topic using the same topic tool.
  <p>
  To add partitions you can do
  <pre class="brush: bash;">
  &gt; bin/kafka-topics.sh --zookeeper zk_host:port/chroot --alter --topic my_topic_name
        --partitions 40
  </pre>
  Be aware that one use case for partitions is to semantically partition data, and adding partitions doesn't change the partitioning of existing data so this may disturb consumers if they rely on that partition. That is if data is partitioned by <code>hash(key) % number_of_partitions</code> then this partitioning will potentially be shuffled by adding partitions but Kafka will not attempt to automatically redistribute data in any way.
  <p>
  To add configs:
  <pre class="brush: bash;">
  &gt; bin/kafka-configs.sh --zookeeper zk_host:port/chroot --entity-type topics --entity-name my_topic_name --alter --add-config x=y
  </pre>
  To remove a config:
  <pre class="brush: bash;">
  &gt; bin/kafka-configs.sh --zookeeper zk_host:port/chroot --entity-type topics --entity-name my_topic_name --alter --delete-config x
  </pre>
  And finally deleting a topic:
  <pre class="brush: bash;">
  &gt; bin/kafka-topics.sh --zookeeper zk_host:port/chroot --delete --topic my_topic_name
  </pre>
  <p>
  Kafka does not currently support reducing the number of partitions for a topic.
  <p>
  Instructions for changing the replication factor of a topic can be found <a href="#basic_ops_increase_replication_factor">here</a>.

  <h4><a id="basic_ops_restarting" href="#basic_ops_restarting">Graceful shutdown</a></h4>

  The Kafka cluster will automatically detect any broker shutdown or failure and elect new leaders for the partitions on that machine. This will occur whether a server fails or it is brought down intentionally for maintenance or configuration changes. For the latter cases Kafka supports a more graceful mechanism for stopping a server than just killing it.

  When a server is stopped gracefully it has two optimizations it will take advantage of:
  <ol>
      <li>It will sync all its logs to disk to avoid needing to do any log recovery when it restarts (i.e. validating the checksum for all messages in the tail of the log). Log recovery takes time so this speeds up intentional restarts.
      <li>It will migrate any partitions the server is the leader for to other replicas prior to shutting down. This will make the leadership transfer faster and minimize the time each partition is unavailable to a few milliseconds.
  </ol>

  Syncing the logs will happen automatically whenever the server is stopped other than by a hard kill, but the controlled leadership migration requires using a special setting:
  <pre class="brush: text;">
      controlled.shutdown.enable=true
  </pre>
  Note that controlled shutdown will only succeed if <i>all</i> the partitions hosted on the broker have replicas (i.e. the replication factor is greater than 1 <i>and</i> at least one of these replicas is alive). This is generally what you want since shutting down the last replica would make that topic partition unavailable.

  <h4><a id="basic_ops_leader_balancing" href="#basic_ops_leader_balancing">Balancing leadership</a></h4>

  Whenever a broker stops or crashes leadership for that broker's partitions transfers to other replicas. This means that by default when the broker is restarted it will only be a follower for all its partitions, meaning it will not be used for client reads and writes.
  <p>
  To avoid this imbalance, Kafka has a notion of preferred replicas. If the list of replicas for a partition is 1,5,9 then node 1 is preferred as the leader to either node 5 or 9 because it is earlier in the replica list. You can have the Kafka cluster try to restore leadership to the restored replicas by running the command:
  <pre class="brush: bash;">
  &gt; bin/kafka-preferred-replica-election.sh --zookeeper zk_host:port/chroot
  </pre>

  Since running this command can be tedious you can also configure Kafka to do this automatically by setting the following configuration:
  <pre class="brush: text;">
      auto.leader.rebalance.enable=true
  </pre>

  <h4><a id="basic_ops_racks" href="#basic_ops_racks">Balancing Replicas Across Racks</a></h4>
  The rack awareness feature spreads replicas of the same partition across different racks. This extends the guarantees Kafka provides for broker-failure to cover rack-failure, limiting the risk of data loss should all the brokers on a rack fail at once. The feature can also be applied to other broker groupings such as availability zones in EC2.
  <p></p>
  You can specify that a broker belongs to a particular rack by adding a property to the broker config:
  <pre class="brush: text;">   broker.rack=my-rack-id</pre>
  When a topic is <a href="#basic_ops_add_topic">created</a>, <a href="#basic_ops_modify_topic">modified</a> or replicas are <a href="#basic_ops_cluster_expansion">redistributed</a>, the rack constraint will be honoured, ensuring replicas span as many racks as they can (a partition will span min(#racks, replication-factor) different racks).
  <p></p>
  The algorithm used to assign replicas to brokers ensures that the number of leaders per broker will be constant, regardless of how brokers are distributed across racks. This ensures balanced throughput.
  <p></p>
  However if racks are assigned different numbers of brokers, the assignment of replicas will not be even. Racks with fewer brokers will get more replicas, meaning they will use more storage and put more resources into replication. Hence it is sensible to configure an equal number of brokers per rack.

  <h4><a id="basic_ops_mirror_maker" href="#basic_ops_mirror_maker">Mirroring data between clusters</a></h4>

  We refer to the process of replicating data <i>between</i> Kafka clusters "mirroring" to avoid confusion with the replication that happens amongst the nodes in a single cluster. Kafka comes with a tool for mirroring data between Kafka clusters. The tool consumes from a source cluster and produces to a destination cluster.

  A common use case for this kind of mirroring is to provide a replica in another datacenter. This scenario will be discussed in more detail in the next section.
  <p>
  You can run many such mirroring processes to increase throughput and for fault-tolerance (if one process dies, the others will take overs the additional load).
  <p>
  Data will be read from topics in the source cluster and written to a topic with the same name in the destination cluster. In fact the mirror maker is little more than a Kafka consumer and producer hooked together.
  <p>
  The source and destination clusters are completely independent entities: they can have different numbers of partitions and the offsets will not be the same. For this reason the mirror cluster is not really intended as a fault-tolerance mechanism (as the consumer position will be different); for that we recommend using normal in-cluster replication. The mirror maker process will, however, retain and use the message key for partitioning so order is preserved on a per-key basis.
  <p>
  Here is an example showing how to mirror a single topic (named <i>my-topic</i>) from an input cluster:
  <pre class="brush: bash;">
  &gt; bin/kafka-mirror-maker.sh
        --consumer.config consumer.properties
        --producer.config producer.properties --whitelist my-topic
  </pre>
  Note that we specify the list of topics with the <code>--whitelist</code> option. This option allows any regular expression using <a href="http://docs.oracle.com/javase/7/docs/api/java/util/regex/Pattern.html">Java-style regular expressions</a>. So you could mirror two topics named <i>A</i> and <i>B</i> using <code>--whitelist 'A|B'</code>. Or you could mirror <i>all</i> topics using <code>--whitelist '*'</code>. Make sure to quote any regular expression to ensure the shell doesn't try to expand it as a file path. For convenience we allow the use of ',' instead of '|' to specify a list of topics.
  <p>
  Sometimes it is easier to say what it is that you <i>don't</i> want. Instead of using <code>--whitelist</code> to say what you want
  to mirror you can use <code>--blacklist</code> to say what to exclude. This also takes a regular expression argument.
  However, <code>--blacklist</code> is not supported when the new consumer has been enabled (i.e. when <code>bootstrap.servers</code>
  has been defined in the consumer configuration).
  <p>
  Combining mirroring with the configuration <code>auto.create.topics.enable=true</code> makes it possible to have a replica cluster that will automatically create and replicate all data in a source cluster even as new topics are added.

  <h4><a id="basic_ops_consumer_lag" href="#basic_ops_consumer_lag">Checking consumer position</a></h4>
  Sometimes it's useful to see the position of your consumers. We have a tool that will show the position of all consumers in a consumer group as well as how far behind the end of the log they are. To run this tool on a consumer group named <i>my-group</i> consuming a topic named <i>my-topic</i> would look like this:
  <pre class="brush: bash;">
  &gt; bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group my-group

  Note: This will only show information about consumers that use the Java consumer API (non-ZooKeeper-based consumers).
  
  TOPIC                          PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG        CONSUMER-ID                                       HOST                           CLIENT-ID
  my-topic                       0          2               4               2          consumer-1-029af89c-873c-4751-a720-cefd41a669d6   /127.0.0.1                     consumer-1
  my-topic                       1          2               3               1          consumer-1-029af89c-873c-4751-a720-cefd41a669d6   /127.0.0.1                     consumer-1
  my-topic                       2          2               3               1          consumer-2-42c1abd4-e3b2-425d-a8bb-e1ea49b29bb2   /127.0.0.1                     consumer-2
  </pre>

  This tool also works with ZooKeeper-based consumers:
  <pre class="brush: bash;">
  &gt; bin/kafka-consumer-groups.sh --zookeeper localhost:2181 --describe --group my-group

  Note: This will only show information about consumers that use ZooKeeper (not those using the Java consumer API).

  TOPIC                          PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG        CONSUMER-ID
  my-topic                       0          2               4               2          my-group_consumer-1
  my-topic                       1          2               3               1          my-group_consumer-1
  my-topic                       2          2               3               1          my-group_consumer-2
  </pre>

  <h4><a id="basic_ops_consumer_group" href="#basic_ops_consumer_group">Managing Consumer Groups</a></h4>

  With the ConsumerGroupCommand tool, we can list, describe, or delete consumer groups. Note that deletion is only available when the group metadata is stored in
  ZooKeeper. When using the <a href="http://kafka.apache.org/documentation.html#newconsumerapi">new consumer API</a> (where
  the broker handles coordination of partition handling and rebalance), the group is deleted when the last committed offset for that group expires.

  For example, to list all consumer groups across all topics:

  <pre class="brush: bash;">
  &gt; bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --list

  test-consumer-group
  </pre>

  To view offsets, as mentioned earlier, we "describe" the consumer group like this:

  <pre class="brush: bash;">
  &gt; bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group test-consumer-group

  TOPIC                          PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG        CONSUMER-ID                                       HOST                           CLIENT-ID
  test-foo                       0          1               3               2          consumer-1-a5d61779-4d04-4c50-a6d6-fb35d942642d   /127.0.0.1                     consumer-1
  </pre>

  If you are using the old high-level consumer and storing the group metadata in ZooKeeper (i.e. <code>offsets.storage=zookeeper</code>), pass
  <code>--zookeeper</code> instead of <code>bootstrap-server</code>:

  <pre class="brush: bash;">
  &gt; bin/kafka-consumer-groups.sh --zookeeper localhost:2181 --list
  </pre>

  <h4><a id="basic_ops_cluster_expansion" href="#basic_ops_cluster_expansion">Expanding your cluster</a></h4>

  Adding servers to a Kafka cluster is easy, just assign them a unique broker id and start up Kafka on your new servers. However these new servers will not automatically be assigned any data partitions, so unless partitions are moved to them they won't be doing any work until new topics are created. So usually when you add machines to your cluster you will want to migrate some existing data to these machines.
  <p>
  The process of migrating data is manually initiated but fully automated. Under the covers what happens is that Kafka will add the new server as a follower of the partition it is migrating and allow it to fully replicate the existing data in that partition. When the new server has fully replicated the contents of this partition and joined the in-sync replica one of the existing replicas will delete their partition's data.
  <p>
  The partition reassignment tool can be used to move partitions across brokers. An ideal partition distribution would ensure even data load and partition sizes across all brokers. The partition reassignment tool does not have the capability to automatically study the data distribution in a Kafka cluster and move partitions around to attain an even load distribution. As such, the admin has to figure out which topics or partitions should be moved around.
  <p>
  The partition reassignment tool can run in 3 mutually exclusive modes:
  <ul>
  <li>--generate: In this mode, given a list of topics and a list of brokers, the tool generates a candidate reassignment to move all partitions of the specified topics to the new brokers. This option merely provides a convenient way to generate a partition reassignment plan given a list of topics and target brokers.</li>
  <li>--execute: In this mode, the tool kicks off the reassignment of partitions based on the user provided reassignment plan. (using the --reassignment-json-file option). This can either be a custom reassignment plan hand crafted by the admin or provided by using the --generate option</li>
  <li>--verify: In this mode, the tool verifies the status of the reassignment for all partitions listed during the last --execute. The status can be either of successfully completed, failed or in progress</li>
  </ul>
  <h5><a id="basic_ops_automigrate" href="#basic_ops_automigrate">Automatically migrating data to new machines</a></h5>
  The partition reassignment tool can be used to move some topics off of the current set of brokers to the newly added brokers. This is typically useful while expanding an existing cluster since it is easier to move entire topics to the new set of brokers, than moving one partition at a time. When used to do this, the user should provide a list of topics that should be moved to the new set of brokers and a target list of new brokers. The tool then evenly distributes all partitions for the given list of topics across the new set of brokers. During this move, the replication factor of the topic is kept constant. Effectively the replicas for all partitions for the input list of topics are moved from the old set of brokers to the newly added brokers.
  <p>
  For instance, the following example will move all partitions for topics foo1,foo2 to the new set of brokers 5,6. At the end of this move, all partitions for topics foo1 and foo2 will <i>only</i> exist on brokers 5,6.
  <p>
  Since the tool accepts the input list of topics as a json file, you first need to identify the topics you want to move and create the json file as follows:
  <pre class="brush: bash;">
  > cat topics-to-move.json
  {"topics": [{"topic": "foo1"},
              {"topic": "foo2"}],
  "version":1
  }
  </pre>
  Once the json file is ready, use the partition reassignment tool to generate a candidate assignment:
  <pre class="brush: bash;">
  > bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --topics-to-move-json-file topics-to-move.json --broker-list "5,6" --generate
  Current partition replica assignment

  {"version":1,
  "partitions":[{"topic":"foo1","partition":2,"replicas":[1,2]},
                {"topic":"foo1","partition":0,"replicas":[3,4]},
                {"topic":"foo2","partition":2,"replicas":[1,2]},
                {"topic":"foo2","partition":0,"replicas":[3,4]},
                {"topic":"foo1","partition":1,"replicas":[2,3]},
                {"topic":"foo2","partition":1,"replicas":[2,3]}]
  }

  Proposed partition reassignment configuration

  {"version":1,
  "partitions":[{"topic":"foo1","partition":2,"replicas":[5,6]},
                {"topic":"foo1","partition":0,"replicas":[5,6]},
                {"topic":"foo2","partition":2,"replicas":[5,6]},
                {"topic":"foo2","partition":0,"replicas":[5,6]},
                {"topic":"foo1","partition":1,"replicas":[5,6]},
                {"topic":"foo2","partition":1,"replicas":[5,6]}]
  }
  </pre>
  <p>
  The tool generates a candidate assignment that will move all partitions from topics foo1,foo2 to brokers 5,6. Note, however, that at this point, the partition movement has not started, it merely tells you the current assignment and the proposed new assignment. The current assignment should be saved in case you want to rollback to it. The new assignment should be saved in a json file (e.g. expand-cluster-reassignment.json) to be input to the tool with the --execute option as follows:
  <pre class="brush: bash;">
  > bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file expand-cluster-reassignment.json --execute
  Current partition replica assignment

  {"version":1,
  "partitions":[{"topic":"foo1","partition":2,"replicas":[1,2]},
                {"topic":"foo1","partition":0,"replicas":[3,4]},
                {"topic":"foo2","partition":2,"replicas":[1,2]},
                {"topic":"foo2","partition":0,"replicas":[3,4]},
                {"topic":"foo1","partition":1,"replicas":[2,3]},
                {"topic":"foo2","partition":1,"replicas":[2,3]}]
  }

  Save this to use as the --reassignment-json-file option during rollback
  Successfully started reassignment of partitions
  {"version":1,
  "partitions":[{"topic":"foo1","partition":2,"replicas":[5,6]},
                {"topic":"foo1","partition":0,"replicas":[5,6]},
                {"topic":"foo2","partition":2,"replicas":[5,6]},
                {"topic":"foo2","partition":0,"replicas":[5,6]},
                {"topic":"foo1","partition":1,"replicas":[5,6]},
                {"topic":"foo2","partition":1,"replicas":[5,6]}]
  }
  </pre>
  <p>
  Finally, the --verify option can be used with the tool to check the status of the partition reassignment. Note that the same expand-cluster-reassignment.json (used with the --execute option) should be used with the --verify option:
  <pre class="brush: bash;">
  > bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file expand-cluster-reassignment.json --verify
  Status of partition reassignment:
  Reassignment of partition [foo1,0] completed successfully
  Reassignment of partition [foo1,1] is in progress
  Reassignment of partition [foo1,2] is in progress
  Reassignment of partition [foo2,0] completed successfully
  Reassignment of partition [foo2,1] completed successfully
  Reassignment of partition [foo2,2] completed successfully
  </pre>

  <h5><a id="basic_ops_partitionassignment" href="#basic_ops_partitionassignment">Custom partition assignment and migration</a></h5>
  The partition reassignment tool can also be used to selectively move replicas of a partition to a specific set of brokers. When used in this manner, it is assumed that the user knows the reassignment plan and does not require the tool to generate a candidate reassignment, effectively skipping the --generate step and moving straight to the --execute step
  <p>
  For instance, the following example moves partition 0 of topic foo1 to brokers 5,6 and partition 1 of topic foo2 to brokers 2,3:
  <p>
  The first step is to hand craft the custom reassignment plan in a json file:
  <pre class="brush: bash;">
  > cat custom-reassignment.json
  {"version":1,"partitions":[{"topic":"foo1","partition":0,"replicas":[5,6]},{"topic":"foo2","partition":1,"replicas":[2,3]}]}
  </pre>
  Then, use the json file with the --execute option to start the reassignment process:
  <pre class="brush: bash;">
  > bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file custom-reassignment.json --execute
  Current partition replica assignment

  {"version":1,
  "partitions":[{"topic":"foo1","partition":0,"replicas":[1,2]},
                {"topic":"foo2","partition":1,"replicas":[3,4]}]
  }

  Save this to use as the --reassignment-json-file option during rollback
  Successfully started reassignment of partitions
  {"version":1,
  "partitions":[{"topic":"foo1","partition":0,"replicas":[5,6]},
                {"topic":"foo2","partition":1,"replicas":[2,3]}]
  }
  </pre>
  <p>
  The --verify option can be used with the tool to check the status of the partition reassignment. Note that the same expand-cluster-reassignment.json (used with the --execute option) should be used with the --verify option:
  <pre class="brush: bash;">
  > bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file custom-reassignment.json --verify
  Status of partition reassignment:
  Reassignment of partition [foo1,0] completed successfully
  Reassignment of partition [foo2,1] completed successfully
  </pre>

  <h4><a id="basic_ops_decommissioning_brokers" href="#basic_ops_decommissioning_brokers">Decommissioning brokers</a></h4>
  The partition reassignment tool does not have the ability to automatically generate a reassignment plan for decommissioning brokers yet. As such, the admin has to come up with a reassignment plan to move the replica for all partitions hosted on the broker to be decommissioned, to the rest of the brokers. This can be relatively tedious as the reassignment needs to ensure that all the replicas are not moved from the decommissioned broker to only one other broker. To make this process effortless, we plan to add tooling support for decommissioning brokers in the future.

  <h4><a id="basic_ops_increase_replication_factor" href="#basic_ops_increase_replication_factor">Increasing replication factor</a></h4>
  Increasing the replication factor of an existing partition is easy. Just specify the extra replicas in the custom reassignment json file and use it with the --execute option to increase the replication factor of the specified partitions.
  <p>
  For instance, the following example increases the replication factor of partition 0 of topic foo from 1 to 3. Before increasing the replication factor, the partition's only replica existed on broker 5. As part of increasing the replication factor, we will add more replicas on brokers 6 and 7.
  <p>
  The first step is to hand craft the custom reassignment plan in a json file:
  <pre class="brush: bash;">
  > cat increase-replication-factor.json
  {"version":1,
  "partitions":[{"topic":"foo","partition":0,"replicas":[5,6,7]}]}
  </pre>
  Then, use the json file with the --execute option to start the reassignment process:
  <pre class="brush: bash;">
  > bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file increase-replication-factor.json --execute
  Current partition replica assignment

  {"version":1,
  "partitions":[{"topic":"foo","partition":0,"replicas":[5]}]}

  Save this to use as the --reassignment-json-file option during rollback
  Successfully started reassignment of partitions
  {"version":1,
  "partitions":[{"topic":"foo","partition":0,"replicas":[5,6,7]}]}
  </pre>
  <p>
  The --verify option can be used with the tool to check the status of the partition reassignment. Note that the same increase-replication-factor.json (used with the --execute option) should be used with the --verify option:
  <pre class="brush: bash;">
  > bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file increase-replication-factor.json --verify
  Status of partition reassignment:
  Reassignment of partition [foo,0] completed successfully
  </pre>
  You can also verify the increase in replication factor with the kafka-topics tool:
  <pre class="brush: bash;">
  > bin/kafka-topics.sh --zookeeper localhost:2181 --topic foo --describe
  Topic:foo	PartitionCount:1	ReplicationFactor:3	Configs:
    Topic: foo	Partition: 0	Leader: 5	Replicas: 5,6,7	Isr: 5,6,7
  </pre>

  <h4><a id="rep-throttle" href="#rep-throttle">Limiting Bandwidth Usage during Data Migration</a></h4>
  Kafka lets you apply a throttle to replication traffic, setting an upper bound on the bandwidth used to move replicas from machine to machine. This is useful when rebalancing a cluster, bootstrapping a new broker or adding or removing brokers, as it limits the impact these data-intensive operations will have on users.
  <p></p>
  There are two interfaces that can be used to engage a throttle. The simplest, and safest, is to apply a throttle when invoking the kafka-reassign-partitions.sh, but kafka-configs.sh can also be used to view and alter the throttle values directly.
  <p></p>
  So for example, if you were to execute a rebalance, with the below command, it would move partitions at no more than 50MB/s.
  <pre class="brush: bash;">$ bin/kafka-reassign-partitions.sh --zookeeper myhost:2181--execute --reassignment-json-file bigger-cluster.json —throttle 50000000</pre>
  When you execute this script you will see the throttle engage:
  <pre class="brush: bash;">
  The throttle limit was set to 50000000 B/s
  Successfully started reassignment of partitions.</pre>
  <p>Should you wish to alter the throttle, during a rebalance, say to increase the throughput so it completes quicker, you can do this by re-running the execute command passing the same reassignment-json-file:</p>
  <pre class="brush: bash;">$ bin/kafka-reassign-partitions.sh --zookeeper localhost:2181  --execute --reassignment-json-file bigger-cluster.json --throttle 700000000
  There is an existing assignment running.
  The throttle limit was set to 700000000 B/s</pre>

  <p>Once the rebalance completes the administrator can check the status of the rebalance using the --verify option.
      If the rebalance has completed, the throttle will be removed via the --verify command. It is important that
      administrators remove the throttle in a timely manner once rebalancing completes by running the command with
      the --verify option. Failure to do so could cause regular replication traffic to be throttled. </p>
  <p>When the --verify option is executed, and the reassignment has completed, the script will confirm that the throttle was removed:</p>

  <pre class="brush: bash;">
  > bin/kafka-reassign-partitions.sh --zookeeper localhost:2181  --verify --reassignment-json-file bigger-cluster.json
  Status of partition reassignment:
  Reassignment of partition [my-topic,1] completed successfully
  Reassignment of partition [mytopic,0] completed successfully
  Throttle was removed.</pre>

  <p>The administrator can also validate the assigned configs using the kafka-configs.sh. There are two pairs of throttle
      configuration used to manage the throttling process. The throttle value itself. This is configured, at a broker
      level, using the dynamic properties: </p>

  <pre class="brush: text;">leader.replication.throttled.rate
  follower.replication.throttled.rate</pre>

  <p>There is also an enumerated set of throttled replicas: </p>

  <pre class="brush: text;">leader.replication.throttled.replicas
  follower.replication.throttled.replicas</pre>

  <p>Which are configured per topic. All four config values are automatically assigned by kafka-reassign-partitions.sh
      (discussed below). </p>
  <p>To view the throttle limit configuration:</p>

  <pre class="brush: bash;">
  > bin/kafka-configs.sh --describe --zookeeper localhost:2181 --entity-type brokers
  Configs for brokers '2' are leader.replication.throttled.rate=700000000,follower.replication.throttled.rate=700000000
  Configs for brokers '1' are leader.replication.throttled.rate=700000000,follower.replication.throttled.rate=700000000</pre>

  <p>This shows the throttle applied to both leader and follower side of the replication protocol. By default both sides
      are assigned the same throttled throughput value. </p>

  <p>To view the list of throttled replicas:</p>

  <pre class="brush: bash;">
  > bin/kafka-configs.sh --describe --zookeeper localhost:2181 --entity-type topics
  Configs for topic 'my-topic' are leader.replication.throttled.replicas=1:102,0:101,
      follower.replication.throttled.replicas=1:101,0:102</pre>

  <p>Here we see the leader throttle is applied to partition 1 on broker 102 and partition 0 on broker 101. Likewise the
      follower throttle is applied to partition 1 on
      broker 101 and partition 0 on broker 102. </p>

  <p>By default kafka-reassign-partitions.sh will apply the leader throttle to all replicas that exist before the
      rebalance, any one of which might be leader.
      It will apply the follower throttle to all move destinations. So if there is a partition with replicas on brokers
      101,102, being reassigned to 102,103, a leader throttle,
      for that partition, would be applied to 101,102 and a follower throttle would be applied to 103 only. </p>


  <p>If required, you can also use the --alter switch on kafka-configs.sh to alter the throttle configurations manually.
  </p>

  <h5>Safe usage of throttled replication</h5>

  <p>Some care should be taken when using throttled replication. In particular:</p>

  <p><i>(1) Throttle Removal:</i></p>
  The throttle should be removed in a timely manner once reassignment completes (by running kafka-reassign-partitions
  —verify).

  <p><i>(2) Ensuring Progress:</i></p>
  <p>If the throttle is set too low, in comparison to the incoming write rate, it is possible for replication to not
      make progress. This occurs when:</p>
  <pre>max(BytesInPerSec) > throttle</pre>
  <p>
      Where BytesInPerSec is the metric that monitors the write throughput of producers into each broker. </p>
  <p>The administrator can monitor whether replication is making progress, during the rebalance, using the metric:</p>

  <pre>kafka.server:type=FetcherLagMetrics,name=ConsumerLag,clientId=([-.\w]+),topic=([-.\w]+),partition=([0-9]+)</pre>

  <p>The lag should constantly decrease during replication.  If the metric does not decrease the administrator should
      increase the
      throttle throughput as described above. </p>


  <h4><a id="quotas" href="#quotas">Setting quotas</a></h4>
  Quotas overrides and defaults may be configured at (user, client-id), user or client-id levels as described <a href="#design_quotas">here</a>.
  By default, clients receive an unlimited quota.

  It is possible to set custom quotas for each (user, client-id), user or client-id group.
  <p>
  Configure custom quota for (user=user1, client-id=clientA):
  <pre class="brush: bash;">
  > bin/kafka-configs.sh  --zookeeper localhost:2181 --alter --add-config 'producer_byte_rate=1024,consumer_byte_rate=2048,request_percentage=200' --entity-type users --entity-name user1 --entity-type clients --entity-name clientA
  Updated config for entity: user-principal 'user1', client-id 'clientA'.
  </pre>

  Configure custom quota for user=user1:
  <pre class="brush: bash;">
  > bin/kafka-configs.sh  --zookeeper localhost:2181 --alter --add-config 'producer_byte_rate=1024,consumer_byte_rate=2048,request_percentage=200' --entity-type users --entity-name user1
  Updated config for entity: user-principal 'user1'.
  </pre>

  Configure custom quota for client-id=clientA:
  <pre class="brush: bash;">
  > bin/kafka-configs.sh  --zookeeper localhost:2181 --alter --add-config 'producer_byte_rate=1024,consumer_byte_rate=2048,request_percentage=200' --entity-type clients --entity-name clientA
  Updated config for entity: client-id 'clientA'.
  </pre>

  It is possible to set default quotas for each (user, client-id), user or client-id group by specifying <i>--entity-default</i> option instead of <i>--entity-name</i>.
  <p>
  Configure default client-id quota for user=userA:
  <pre class="brush: bash;">
  > bin/kafka-configs.sh  --zookeeper localhost:2181 --alter --add-config 'producer_byte_rate=1024,consumer_byte_rate=2048,request_percentage=200' --entity-type users --entity-name user1 --entity-type clients --entity-default
  Updated config for entity: user-principal 'user1', default client-id.
  </pre>

  Configure default quota for user:
  <pre class="brush: bash;">
  > bin/kafka-configs.sh  --zookeeper localhost:2181 --alter --add-config 'producer_byte_rate=1024,consumer_byte_rate=2048,request_percentage=200' --entity-type users --entity-default
  Updated config for entity: default user-principal.
  </pre>

  Configure default quota for client-id:
  <pre class="brush: bash;">
  > bin/kafka-configs.sh  --zookeeper localhost:2181 --alter --add-config 'producer_byte_rate=1024,consumer_byte_rate=2048,request_percentage=200' --entity-type clients --entity-default
  Updated config for entity: default client-id.
  </pre>

  Here's how to describe the quota for a given (user, client-id):
  <pre class="brush: bash;">
  > bin/kafka-configs.sh  --zookeeper localhost:2181 --describe --entity-type users --entity-name user1 --entity-type clients --entity-name clientA
  Configs for user-principal 'user1', client-id 'clientA' are producer_byte_rate=1024,consumer_byte_rate=2048,request_percentage=200
  </pre>
  Describe quota for a given user:
  <pre class="brush: bash;">
  > bin/kafka-configs.sh  --zookeeper localhost:2181 --describe --entity-type users --entity-name user1
  Configs for user-principal 'user1' are producer_byte_rate=1024,consumer_byte_rate=2048,request_percentage=200
  </pre>
  Describe quota for a given client-id:
  <pre class="brush: bash;">
  > bin/kafka-configs.sh  --zookeeper localhost:2181 --describe --entity-type clients --entity-name clientA
  Configs for client-id 'clientA' are producer_byte_rate=1024,consumer_byte_rate=2048,request_percentage=200
  </pre>
  If entity name is not specified, all entities of the specified type are described. For example, describe all users:
  <pre class="brush: bash;">
  > bin/kafka-configs.sh  --zookeeper localhost:2181 --describe --entity-type users
  Configs for user-principal 'user1' are producer_byte_rate=1024,consumer_byte_rate=2048,request_percentage=200
  Configs for default user-principal are producer_byte_rate=1024,consumer_byte_rate=2048,request_percentage=200
  </pre>
  Similarly for (user, client):
  <pre class="brush: bash;">
  > bin/kafka-configs.sh  --zookeeper localhost:2181 --describe --entity-type users --entity-type clients
  Configs for user-principal 'user1', default client-id are producer_byte_rate=1024,consumer_byte_rate=2048,request_percentage=200
  Configs for user-principal 'user1', client-id 'clientA' are producer_byte_rate=1024,consumer_byte_rate=2048,request_percentage=200
  </pre>
  <p>
  It is possible to set default quotas that apply to all client-ids by setting these configs on the brokers. These properties are applied only if quota overrides or defaults are not configured in Zookeeper. By default, each client-id receives an unlimited quota. The following sets the default quota per producer and consumer client-id to 10MB/sec.
  <pre class="brush: text;">
    quota.producer.default=10485760
    quota.consumer.default=10485760
  </pre>
  Note that these properties are being deprecated and may be removed in a future release. Defaults configured using kafka-configs.sh take precedence over these properties.

  <h3><a id="datacenters" href="#datacenters">6.2 数据中心</a></h3>

  有一些部署需要维护一个跨越多个数据中心的数据管道。对此，我们推荐的方法是在每个拥有众多应用实例的数据中心内部署一个本地Kafka集群，在每个数据中心内只与本地的kafka集群进行交互，然后各集群之间通过镜像进行同步，（请参阅<a href="#basic_ops_mirror_maker">镜像制作工具< / a>了解怎么做到这一点）。.
  <p>
  这种部署模式允许数据中心充当一个独立的实体，并允许我们能够集中的管理和调节数据中心之间的复制。在这种部署模式下，即使数据中心间的链路不可用，每个设施也可以独立运行：当发生这种情况时，镜像会落后，直到链路恢复正常并追上时为止。
  <p>
    对于需要所有数据的全局视图的应用程序，你可以使用镜像来提供具有来自所有<i>数据中心</i>的本地集群镜像的聚合数据的集群。聚合集群用于需要全部数据集的应用程序读取。
  <p>
  这并是不唯一的部署模式，可以通过广域网读取或者写入到远程的Kafka集群，但是这显然会增加获取集群的延时。
  <p>
  Kafka能在生产端和消费端很轻易的批处理数据，所以即使在高延时的连接中也可以实现高吞吐量。为此.虽然我们可能需要在生产端，消费端还有broker端增加TCP 套接字缓冲区大小，修改如下参数配置<code>socket.send.buffer.bytes</code> 和 <code>socket.receive.buffer.bytes</code>。具体请参见<a href="http://en.wikipedia.org/wiki/Bandwidth-delay_product">这里</a>。
  <p>
  通常不建议在高延时链路的情况下部署一个跨越多个数据中心的Kafka集群。这将对Kafka写入和ZooKeeper写入产生非常高的复制延时，当各位置节点之间的网络不可用时，Kafka和ZooKeeper也将不保证可用

  <h3><a id="config" href="#config">6.3 Kafka 配置</a></h3>

  <h4><a id="clientconfig" href="#clientconfig">重要的客户端配置</a></h4>
  最重要的旧scala producer configurations control
  <ul>
      <li>acks</li>
      <li>compression</li>
      <li>sync vs async production</li>
      <li>batch size (for async producers)</li>
  </ul>
  最重要的新 Java producer configurations control
  <ul>
      <li>acks</li>
      <li>compression</li>
      <li>batch size</li>
  </ul>
  最重要的消费端配置是 fetch size。
  <p>
  所有的配置请查阅 <a href="#configuration">configuration</a> 章节。
  <p>
  <h4><a id="prodconfig" href="#prodconfig">一个生产服务器配置</a></h4>
  以下是生产服务器配置示例：
  <pre class="brush: text;">
  # ZooKeeper
  zookeeper.connect=[list of ZooKeeper servers]

  # Log configuration
  num.partitions=8
  default.replication.factor=3
  log.dir=[List of directories. Kafka should have its own dedicated disk(s) or SSD(s).]

  # Other configurations
  broker.id=[An integer. Start with 0 and increment by 1 for each new broker.]
  listeners=[list of listeners]
  auto.create.topics.enable=false
  min.insync.replicas=2
  queued.max.requests=[number of concurrent requests]
  </pre>

  我们的客户端配置在不同的使用场景下需要相应的变化。

  <h3><a id="java" href="#java">6.4 Java 版本</a></h3>

  从安全角度来看，我们建议您使用JDK 1.8的最新发布版本，因为较早的免费版本已经披露了安全漏洞。

  LinkedIn目前正在使用G1垃圾收集器运行JDK1.8 u5（希望升级到更新的版本）。如果您决定使用G1（当前默认值），并且您仍然使用JDK1.7，请确保您使用的是u51或者以上版本。LinkedIn已经在测试中试用了u21，但是在该版本中，GC方面存在着一些问题。

  LinkedIn的调整如下所示:
  <pre class="brush: text;">
  -Xmx6g -Xms6g -XX:MetaspaceSize=96m -XX:+UseG1GC
  -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:G1HeapRegionSize=16M
  -XX:MinMetaspaceFreeRatio=50 -XX:MaxMetaspaceFreeRatio=80
  </pre>

  作为参考，下面是LinkedIn最繁忙的群集（峰值）之一的统计数据：
  <ul>
      <li>60 brokers</li>
      <li>50k partitions (replication factor 2)</li>
      <li>800k messages/sec in</li>
      <li>300 MB/sec inbound, 1 GB/sec+ outbound</li>
  </ul>

  该调整看起来相当激进，但是在集群中的所有broker的GC暂停时间90%都在大约21ms，并且每秒钟的yong GC少于一次。

  <h3><a id="hwandos" href="#hwandos">6.5 硬件和操作系统</a></h3>
  我们正在使用双四核24GB的Intel Xeon 机器。
  <p>
  您需要足够的内存来缓存活动的readers和writers。您可以通过假设您希望缓存30秒，将您的内存需求计算为write_throughput * 30来进行内存需求的后期估计。
  <p>
  磁盘的吞吐量很重要。我们有8x7200转的SATA硬盘。通常磁盘的吞吐量是瓶颈，磁盘是越多越好。您能不能从更昂贵的磁盘中受益取决于你的刷新配置（如果您经常强制刷新，那么更高转速的SAS硬盘可能更好）。

  <h4><a id="os" href="#os">OS</a></h4>
  Kafka应该在任何Unix系统运行良好，并且已经在Linux和Solaris上进行了测试。
  <p>
  我们已经发现了在windows运行的一些问题，目前windows还不是一个理想的支持平台，虽然我们很乐意改变这个问题。
  <p>
  Kafka不需要太多的操作系统层面的调优，但是有两个潜在重要的操作系统级别的配置：
  <ul>
      <li>文件描述符限制: Kafka把文件描述符用于日志段和打开连接。如果一个broker上有许多分区，则考虑broker至少(number_of_partitions)*(partition_size/segment_size) 个文件描述符来跟踪所有的日志段和broker所创建的连接。我们推荐每一个broker一开始至少配置100000个文件描述符。
      <li>最大套接字缓冲区大小：可以增加以实现数据中心之间的高性能数据传输，如<a href="http://www.psc.edu/index.php/networking/641-tcp-tune">此处所述</a>。
  </ul>
  <p>

  <h4><a id="diskandfs" href="#diskandfs">磁盘和文件系统</a></h4>
  我们建议使用多个驱动器以获得良好的吞吐量，并且为了确保良好的延迟，不应该与应用程序日志或其他的操作系统文件系统活动共享用于Kafka数据的相同驱动器。您可以将这些驱动器RAID成单个卷或格式，并且把每个驱动器挂载到它自己的目录。Kafka的副本冗余功能可以由RAID或者应用程序级别提供，可以折衷选择实现。
  <p>
  如果您配置了多个数据目录，那么分区将被循环分配到数据目录。每个分区只属于一个数据目录，如果分区间的数据不均衡，则可能导致磁盘间的负载不均衡。
  <p>
  RAID可以更好地平衡磁盘之间的负载（尽管似乎并不总是如此），因为它在较低的级别上进行平衡负载。 RAID的主要缺点是通常会大幅度影响写入性能并且降低可用磁盘空间。
  <p>
  RAID的另一个潜在好处是能够容忍磁盘故障。 然而，我们的经验是，重建RAID阵列是I/O密集型操作以至于服务器不可用，所以这不提供太多的实际可用性改进。

  <h4><a id="appvsosflush" href="#appvsosflush">应用程序 vs. OS 刷新管理</a></h4>
  Kafka总是立即将所有数据写入文件系统，并支持配置刷新策略的功能，该策略控制何时将数据从OS缓存中强制刷新到磁盘上。该刷新策略可以控制在一段时间之后或者在写入一定数量的消息之后把数据持久化到磁盘。这里有几个可选配置项。
  <p>
  Kafka最总必须调用fsync指令才能知道数据已经被刷新。当从任何不被fsync所知的日志段崩溃中恢复时，Kafka将通过每个消息的CRC来检查其完整性，并且在启动时执行的恢复过程中会重建相应的偏移量索引文件。
  <p>
  请注意，Kafka中的持久性并不需要将数据同步到磁盘，因为失败的节点将始终从其副本中恢复。
  <p>
  我们建议使用完全禁用应用程序fsync的默认刷新设置。这意味着依靠操作系统和Kafka自己的后台完成的刷新操作。这种设置对大多数用途是最好的选择：无需调整，巨大的吞吐量和延时，以及完全的恢复保证。我们认为通过复制提供的保证比同步到本地磁盘更好，但是一些偏执的人仍然可能喜欢让OS，Kafka和应用程序级别的fsync策略都得到支持。
  <p>
  使用应用程序级别刷新设置的缺点是它的磁盘使用模式效率低下（它是操作系统在重新排序时没有什么回旋余地），并且在大多数Linux文件系统block中fsync写入文件时会引入延时，而后台刷新则会做更多粒度的页面级锁定。
  <p>
  通常你不需要对文件系统进行任何低级别的调整，但是，在接下来的几节中，我们也会介绍其中的一些内容，以防万一。

  <h4><a id="linuxflush" href="#linuxflush">理解Linux操作系统刷新行为</a></h4>

  在Linux中，写入文件系统的数据在<a href="http://en.wikipedia.org/wiki/Page_cache">页面缓存</a>中保存，直到必须写入磁盘（由于应用程序级别的fsync或操作系统自己的刷新策略）。数据的刷新是通过一组叫做pdflush的后台线程来完成的（或者在2.6.32内核中的“flusher threads”）。
  <p>
  pdflush有一个可配置的策略，可以控制在缓存中可以维护多少脏数据，以及多久时间之前必须将数据写回到磁盘。
  该策略详情请参阅<a href="http://web.archive.org/web/20160518040713/http://www.westnet.com/~gsmith/content/linux-pdflush.htm">这里</a>。
  当Pdflush无法跟上数据写入的速率时，最终会导致写入过程阻塞发生写入延时来减慢数据的堆积。
  <p>
  您可以通过执行下面命令来查看当前OS内存使用状态
  <pre class="brush: bash;"> &gt; cat /proc/meminfo </pre>
  这些值的含义在上面的链接中有描述。
    <p>
  相对于进程内缓存，使用页面缓存来存储将被写入到磁盘的数据有几个优势：
  <ul>
    <li>I/O 调度器将一批连续的小写入转换成为更大的物理写入，从而提高吞吐量。
    <li>I/O调度器将尝试重新排序写入操作，以尽量减少磁盘磁头的移动，从而提高吞吐量I/O
    <li>它会自动使用机器上的所有可用内存
  </ul>

  <h4><a id="filesystems" href="#filesystems">文件系统的选择</a></h4>
  <p>Kafka在磁盘上使用常规的文件，不依赖于特定的文件系统。 然而，使用最多的两个文件系统是EXT4和XFS。 从历史上看，EXT4使用更多，但最近对XFS文件系统的改进已经表明它对Kafka的负载具有更好的性能，而且不会影响稳定性。</p>
  <p>通过尝试各种文件系统的创建和挂载选项，在具有重要消息负载的集群上执行对比测试。Kafka监测的主要指标是“Request Local Time”，它表示追加操作的时间。XFS的本地时间更短（160ms比250ms +最好的EXT4配置），以及更低的平均等待时间。 随着磁盘性能变化，XFS的性能也表现出较小的波动。</p>
  <h5><a id="generalfs" href="#generalfs">一般文件系统注意事项</a></h5>
    对于用于数据目录的任何文件系统，在Linux系统上，建议在挂载时使用以下选项：
    <ul>
    <li>noatime：此选项禁止在读取文件时更新文件的atime（上次访问时间）属性。 这可以消除大量的文件系统写入，特别是在引导consumer的情况下。 Kafka根本不依赖atime属性，因此禁用这个属性是安全的。</li>
  </ul>
  <h5><a id="xfs" href="#xfs">XFS 注意事项</a></h5>
    XFS文件系统具有大量的自动调整功能，因此无需在文件系统创建时或挂载时对默认设置进行任何更改。 唯一值得考虑的调整参数是：
  <ul>
    <li>largeio: 这会影响统计调用报告的首选I/O大小。 虽然这可以允许在更大的磁盘写入时获得更高的性能，但是实际上它对性能的影响很小或者没有影响。</li>
    <li>nobarrier: 对于具有 battery-backed cache 的底层设备，此选项可以通过禁用定期写入刷新来提供更多的性能。 但是，如果底层设备运行良好，则会向文件系统报告不需要刷新，此选项不起作用。</li>
  </ul>
  <h5><a id="ext4" href="#ext4">EXT4 注意事项</a></h5>
    EXT4是Kafka数据目录的文件系统的一个可选择的选项，但是为了获得最高的性能需要调整几个挂载选项。 另外，这些选项在故障情况下通常是不安全的，并且会导致更多的数据丢失和损坏。 对于单个 broker 失败，这不是一个问题，因为可以擦除磁盘，并从集群重建副本。 但在诸如停电等多故障情况下，这可能意味着不容易恢复损坏的底层文件系统（数据）。 以下选项可以调整：
  <ul>
    <li>data=writeback:Ext4默认为data = ordered，这会导致某些写入操作上有很强的顺序性。Kafka不需要这样的顺序，因为它在所有未刷新的日志上进行非常偏执的数据恢复。此设置消除了排序约束，似乎显著减少了延迟。
    <li>Disabling journaling: Journaling 是一个折衷：在服务器崩溃之后，它会使重新启动更快，但会引入大量额外的锁定，从而增加写入性能的差异。那些不关心重启时间，想要减少写入延迟尖峰的主要来源，可以完全关闭Journaling。
    <li>commit=num_secs: 这调整了ext4向其元数据日志提交的频率。将其设置为较低的值可以减少崩溃期间未刷新数据的丢失。将其设置为更高的值则将提高吞吐量。
    <li>nobh: 当使用 data=writeback 模式时，此设置控制额外的排序保证。 Kafka应该是安全的，因为我们不依赖写入顺序并提高吞吐量和延迟。
    <li>delalloc: 延迟分配意味着文件系统避免分配任何 block 直到物理写入发生。这使得ext4可以在很大程度上分配连续区域而不是较小的页面，并有助于确保数据连续写入。这个功能非常适合吞吐量。它似乎涉及到文件系统中的一些锁定，这增加了一些延迟差异。
  </ul>

  <h3><a id="monitoring" href="#monitoring">6.6 Monitoring</a></h3>

  Kafka uses Yammer Metrics for metrics reporting in the server and Scala clients. The Java clients use Kafka Metrics, a built-in metrics registry that minimizes transitive dependencies pulled into client applications. Both expose metrics via JMX and can be configured to report stats using pluggable stats reporters to hook up to your monitoring system.
  <p>
  The easiest way to see the available metrics is to fire up jconsole and point it at a running kafka client or server; this will allow browsing all metrics with JMX.
  <p>
  We do graphing and alerting on the following metrics:
  <table class="data-table">
  <tbody><tr>
        <th>Description</th>
        <th>Mbean name</th>
        <th>Normal value</th>
      </tr>
      <tr>
        <td>Message in rate</td>
        <td>kafka.server:type=BrokerTopicMetrics,name=MessagesInPerSec</td>
        <td></td>
      </tr>
      <tr>
        <td>Byte in rate</td>
        <td>kafka.server:type=BrokerTopicMetrics,name=BytesInPerSec</td>
        <td></td>
      </tr>
      <tr>
        <td>Request rate</td>
        <td>kafka.network:type=RequestMetrics,name=RequestsPerSec,request={Produce|FetchConsumer|FetchFollower}</td>
        <td></td>
      </tr>
      <tr>
        <td>Byte out rate</td>
        <td>kafka.server:type=BrokerTopicMetrics,name=BytesOutPerSec</td>
        <td></td>
      </tr>
      <tr>
        <td>Log flush rate and time</td>
        <td>kafka.log:type=LogFlushStats,name=LogFlushRateAndTimeMs</td>
        <td></td>
      </tr>
      <tr>
        <td># of under replicated partitions (|ISR| &lt |all replicas|)</td>
        <td>kafka.server:type=ReplicaManager,name=UnderReplicatedPartitions</td>
        <td>0</td>
      </tr>
      <tr>
        <td># of under minIsr partitions (|ISR| &lt min.insync.replicas)</td>
        <td>kafka.server:type=ReplicaManager,name=UnderMinIsrPartitionCount</td>
        <td>0</td>
      </tr>
      <tr>
        <td># of offline log directories</td>
        <td>kafka.log:type=LogManager,name=OfflineLogDirectoryCount</td>
        <td>0</td>
      </tr>
      <tr>
        <td>Is controller active on broker</td>
        <td>kafka.controller:type=KafkaController,name=ActiveControllerCount</td>
        <td>only one broker in the cluster should have 1</td>
      </tr>
      <tr>
        <td>Leader election rate</td>
        <td>kafka.controller:type=ControllerStats,name=LeaderElectionRateAndTimeMs</td>
        <td>non-zero when there are broker failures</td>
      </tr>
      <tr>
        <td>Unclean leader election rate</td>
        <td>kafka.controller:type=ControllerStats,name=UncleanLeaderElectionsPerSec</td>
        <td>0</td>
      </tr>
      <tr>
        <td>Partition counts</td>
        <td>kafka.server:type=ReplicaManager,name=PartitionCount</td>
        <td>mostly even across brokers</td>
      </tr>
      <tr>
        <td>Leader replica counts</td>
        <td>kafka.server:type=ReplicaManager,name=LeaderCount</td>
        <td>mostly even across brokers</td>
      </tr>
      <tr>
        <td>ISR shrink rate</td>
        <td>kafka.server:type=ReplicaManager,name=IsrShrinksPerSec</td>
        <td>If a broker goes down, ISR for some of the partitions will
    shrink. When that broker is up again, ISR will be expanded
    once the replicas are fully caught up. Other than that, the
    expected value for both ISR shrink rate and expansion rate is 0. </td>
      </tr>
      <tr>
        <td>ISR expansion rate</td>
        <td>kafka.server:type=ReplicaManager,name=IsrExpandsPerSec</td>
        <td>See above</td>
      </tr>
      <tr>
        <td>Max lag in messages btw follower and leader replicas</td>
        <td>kafka.server:type=ReplicaFetcherManager,name=MaxLag,clientId=Replica</td>
        <td>lag should be proportional to the maximum batch size of a produce request.</td>
      </tr>
      <tr>
        <td>Lag in messages per follower replica</td>
        <td>kafka.server:type=FetcherLagMetrics,name=ConsumerLag,clientId=([-.\w]+),topic=([-.\w]+),partition=([0-9]+)</td>
        <td>lag should be proportional to the maximum batch size of a produce request.</td>
      </tr>
      <tr>
        <td>Requests waiting in the producer purgatory</td>
        <td>kafka.server:type=DelayedOperationPurgatory,name=PurgatorySize,delayedOperation=Produce</td>
        <td>non-zero if ack=-1 is used</td>
      </tr>
      <tr>
        <td>Requests waiting in the fetch purgatory</td>
        <td>kafka.server:type=DelayedOperationPurgatory,name=PurgatorySize,delayedOperation=Fetch</td>
        <td>size depends on fetch.wait.max.ms in the consumer</td>
      </tr>
      <tr>
        <td>Request total time</td>
        <td>kafka.network:type=RequestMetrics,name=TotalTimeMs,request={Produce|FetchConsumer|FetchFollower}</td>
        <td>broken into queue, local, remote and response send time</td>
      </tr>
      <tr>
        <td>Time the request waits in the request queue</td>
        <td>kafka.network:type=RequestMetrics,name=RequestQueueTimeMs,request={Produce|FetchConsumer|FetchFollower}</td>
        <td></td>
      </tr>
      <tr>
        <td>Time the request is processed at the leader</td>
        <td>kafka.network:type=RequestMetrics,name=LocalTimeMs,request={Produce|FetchConsumer|FetchFollower}</td>
        <td></td>
      </tr>
      <tr>
        <td>Time the request waits for the follower</td>
        <td>kafka.network:type=RequestMetrics,name=RemoteTimeMs,request={Produce|FetchConsumer|FetchFollower}</td>
        <td>non-zero for produce requests when ack=-1</td>
      </tr>
      <tr>
          <td>Time the request waits in the response queue</td>
          <td>kafka.network:type=RequestMetrics,name=ResponseQueueTimeMs,request={Produce|FetchConsumer|FetchFollower}</td>
          <td></td>
      </tr>
      <tr>
        <td>Time to send the response</td>
        <td>kafka.network:type=RequestMetrics,name=ResponseSendTimeMs,request={Produce|FetchConsumer|FetchFollower}</td>
        <td></td>
      </tr>
      <tr>
        <td>Number of messages the consumer lags behind the producer by. Published by the consumer, not broker.</td>
        <td>
          <p><em>Old consumer:</em> kafka.consumer:type=ConsumerFetcherManager,name=MaxLag,clientId=([-.\w]+)</p>
          <p><em>New consumer:</em> kafka.consumer:type=consumer-fetch-manager-metrics,client-id={client-id} Attribute: records-lag-max</p>
        </td>
        <td></td>
      </tr>
      <tr>
        <td>The average fraction of time the network processors are idle</td>
        <td>kafka.network:type=SocketServer,name=NetworkProcessorAvgIdlePercent</td>
        <td>between 0 and 1, ideally &gt 0.3</td>
      </tr>
      <tr>
        <td>The average fraction of time the request handler threads are idle</td>
        <td>kafka.server:type=KafkaRequestHandlerPool,name=RequestHandlerAvgIdlePercent</td>
        <td>between 0 and 1, ideally &gt 0.3</td>
      </tr>
      <tr>
        <td>Bandwidth quota metrics per (user, client-id), user or client-id</td>
        <td>kafka.server:type={Produce|Fetch},user=([-.\w]+),client-id=([-.\w]+)</td>
        <td>Two attributes. throttle-time indicates the amount of time in ms the client was throttled. Ideally = 0.
            byte-rate indicates the data produce/consume rate of the client in bytes/sec.
            For (user, client-id) quotas, both user and client-id are specified. If per-client-id quota is applied to the client, user is not specified. If per-user quota is applied, client-id is not specified.</td>
      </tr>
      <tr>
        <td>Request quota metrics per (user, client-id), user or client-id</td>
        <td>kafka.server:type=Request,user=([-.\w]+),client-id=([-.\w]+)</td>
        <td>Two attributes. throttle-time indicates the amount of time in ms the client was throttled. Ideally = 0.
            request-time indicates the percentage of time spent in broker network and I/O threads to process requests from client group.
            For (user, client-id) quotas, both user and client-id are specified. If per-client-id quota is applied to the client, user is not specified. If per-user quota is applied, client-id is not specified.</td>
      </tr>
      <tr>
        <td>Requests exempt from throttling</td>
        <td>kafka.server:type=Request</td>
        <td>exempt-throttle-time indicates the percentage of time spent in broker network and I/O threads to process requests
            that are exempt from throttling.</td>
      </tr>
  </tbody></table>

  <h4><a id="selector_monitoring" href="#selector_monitoring">Common monitoring metrics for producer/consumer/connect/streams</a></h4>

  The following metrics are available on producer/consumer/connector/streams instances.  For specific metrics, please see following sections.

  <table class="data-table">
    <tbody>
      <tr>
        <th>Metric/Attribute name</th>
        <th>Description</th>
        <th>Mbean name</th>
      </tr>
      <tr>
        <td>connection-close-rate</td>
        <td>Connections closed per second in the window.</td>
        <td>kafka.[producer|consumer|connect]:type=[producer|consumer|connect]-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>connection-creation-rate</td>
        <td>New connections established per second in the window.</td>
        <td>kafka.[producer|consumer|connect]:type=[producer|consumer|connect]-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>network-io-rate</td>
        <td>The average number of network operations (reads or writes) on all connections per second.</td>
        <td>kafka.[producer|consumer|connect]:type=[producer|consumer|connect]-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>outgoing-byte-rate</td>
        <td>The average number of outgoing bytes sent per second to all servers.</td>
        <td>kafka.[producer|consumer|connect]:type=[producer|consumer|connect]-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>request-rate</td>
        <td>The average number of requests sent per second.</td>
        <td>kafka.[producer|consumer|connect]:type=[producer|consumer|connect]-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>request-size-avg</td>
        <td>The average size of all requests in the window.</td>
        <td>kafka.[producer|consumer|connect]:type=[producer|consumer|connect]-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>request-size-max</td>
        <td>The maximum size of any request sent in the window.</td>
        <td>kafka.[producer|consumer|connect]:type=[producer|consumer|connect]-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>incoming-byte-rate</td>
        <td>Bytes/second read off all sockets.</td>
        <td>kafka.[producer|consumer|connect]:type=[producer|consumer|connect]-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>response-rate</td>
        <td>Responses received sent per second.</td>
        <td>kafka.[producer|consumer|connect]:type=[producer|consumer|connect]-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>select-rate</td>
        <td>Number of times the I/O layer checked for new I/O to perform per second.</td>
        <td>kafka.[producer|consumer|connect]:type=[producer|consumer|connect]-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>io-wait-time-ns-avg</td>
        <td>The average length of time the I/O thread spent waiting for a socket ready for reads or writes in nanoseconds.</td>
        <td>kafka.[producer|consumer|connect]:type=[producer|consumer|connect]-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>io-wait-ratio</td>
        <td>The fraction of time the I/O thread spent waiting.</td>
        <td>kafka.[producer|consumer|connect]:type=[producer|consumer|connect]-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>io-time-ns-avg</td>
        <td>The average length of time for I/O per select call in nanoseconds.</td>
        <td>kafka.[producer|consumer|connect]:type=[producer|consumer|connect]-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>io-ratio</td>
        <td>The fraction of time the I/O thread spent doing I/O.</td>
        <td>kafka.[producer|consumer|connect]:type=[producer|consumer|connect]-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>connection-count</td>
        <td>The current number of active connections.</td>
        <td>kafka.[producer|consumer|connect]:type=[producer|consumer|connect]-metrics,client-id=([-.\w]+)</td>
      </tr>
    </tbody>
  </table>

  <h4><a id="common_node_monitoring" href="#common_node_monitoring">Common Per-broker metrics for producer/consumer/connect/streams</a></h4>

  The following metrics are available on producer/consumer/connector/streams instances.  For specific metrics, please see following sections.

  <table class="data-table">
    <tbody>
      <tr>
        <th>Metric/Attribute name</th>
        <th>Description</th>
        <th>Mbean name</th>
      </tr>
      <tr>
        <td>outgoing-byte-rate</td>
        <td>The average number of outgoing bytes sent per second for a node.</td>
        <td>kafka.producer:type=[consumer|producer|connect]-node-metrics,client-id=([-.\w]+),node-id=([0-9]+)</td>
      </tr>
      <tr>
        <td>request-rate</td>
        <td>The average number of requests sent per second for a node.</td>
        <td>kafka.producer:type=[consumer|producer|connect]-node-metrics,client-id=([-.\w]+),node-id=([0-9]+)</td>
      </tr>
      <tr>
        <td>request-size-avg</td>
        <td>The average size of all requests in the window for a node.</td>
        <td>kafka.producer:type=[consumer|producer|connect]-node-metrics,client-id=([-.\w]+),node-id=([0-9]+)</td>
      </tr>
      <tr>
        <td>request-size-max</td>
        <td>The maximum size of any request sent in the window for a node.</td>
        <td>kafka.producer:type=[consumer|producer|connect]-node-metrics,client-id=([-.\w]+),node-id=([0-9]+)</td>
      </tr>
      <tr>
        <td>incoming-byte-rate</td>
        <td>The average number of responses received per second for a node.</td>
        <td>kafka.producer:type=[consumer|producer|connect]-node-metrics,client-id=([-.\w]+),node-id=([0-9]+)</td>
      </tr>
      <tr>
        <td>request-latency-avg</td>
        <td>The average request latency in ms for a node.</td>
        <td>kafka.producer:type=[consumer|producer|connect]-node-metrics,client-id=([-.\w]+),node-id=([0-9]+)</td>
      </tr>
      <tr>
        <td>request-latency-max</td>
        <td>The maximum request latency in ms for a node.</td>
        <td>kafka.producer:type=[consumer|producer|connect]-node-metrics,client-id=([-.\w]+),node-id=([0-9]+)</td>
      </tr>
      <tr>
        <td>response-rate</td>
        <td>Responses received sent per second for a node.</td>
        <td>kafka.producer:type=[consumer|producer|connect]-node-metrics,client-id=([-.\w]+),node-id=([0-9]+)</td>
      </tr>
    </tbody>
  </table>

  <h4><a id="producer_monitoring" href="#producer_monitoring">Producer monitoring</a></h4>

  The following metrics are available on producer instances.

  <table class="data-table">
  <tbody><tr>
        <th>Metric/Attribute name</th>
        <th>Description</th>
        <th>Mbean name</th>
      </tr>
        <tr>
        <td>waiting-threads</td>
        <td>The number of user threads blocked waiting for buffer memory to enqueue their records.</td>
        <td>kafka.producer:type=producer-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>buffer-total-bytes</td>
        <td>The maximum amount of buffer memory the client can use (whether or not it is currently used).</td>
        <td>kafka.producer:type=producer-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>buffer-available-bytes</td>
        <td>The total amount of buffer memory that is not being used (either unallocated or in the free list).</td>
        <td>kafka.producer:type=producer-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>bufferpool-wait-time</td>
        <td>The fraction of time an appender waits for space allocation.</td>
        <td>kafka.producer:type=producer-metrics,client-id=([-.\w]+)</td>
      </tr>

  </tbody></table>

  <h5><a id="producer_sender_monitoring" href="#producer_sender_monitoring">Producer Sender Metrics</a></h5>

  <!--#include virtual="generated/producer_metrics.html" -->


  <h4><a id="new_consumer_monitoring" href="#new_consumer_monitoring">New consumer monitoring</a></h4>

  The following metrics are available on new consumer instances.

  <h5><a id="new_consumer_group_monitoring" href="#new_consumer_group_monitoring">Consumer Group Metrics</a></h5>
  <table class="data-table">
    <tbody>
      <tr>
        <th>Metric/Attribute name</th>
        <th>Description</th>
        <th>Mbean name</th>
      </tr>
      <tr>
        <td>commit-latency-avg</td>
        <td>The average time taken for a commit request</td>
        <td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>commit-latency-max</td>
        <td>The max time taken for a commit request</td>
        <td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>commit-rate</td>
        <td>The number of commit calls per second</td>
        <td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>assigned-partitions</td>
        <td>The number of partitions currently assigned to this consumer</td>
        <td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>heartbeat-response-time-max</td>
        <td>The max time taken to receive a response to a heartbeat request</td>
        <td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>heartbeat-rate</td>
        <td>The average number of heartbeats per second</td>
        <td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>join-time-avg</td>
        <td>The average time taken for a group rejoin</td>
        <td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>join-time-max</td>
        <td>The max time taken for a group rejoin</td>
        <td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>join-rate</td>
        <td>The number of group joins per second</td>
        <td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>sync-time-avg</td>
        <td>The average time taken for a group sync</td>
        <td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>sync-time-max</td>
        <td>The max time taken for a group sync</td>
        <td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>sync-rate</td>
        <td>The number of group syncs per second</td>
        <td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>last-heartbeat-seconds-ago</td>
        <td>The number of seconds since the last controller heartbeat</td>
        <td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td>
      </tr>
    </tbody>
  </table>

  <h5><a id="new_consumer_fetch_monitoring" href="#new_consumer_fetch_monitoring">Consumer Fetch Metrics</a></h5>

  <!--#include virtual="generated/consumer_metrics.html" -->

  <h4><a id="connect_monitoring" href="#connect_monitoring">Connect Monitoring</a></h4>

  A Connect worker process contains all the producer and consumer metrics as well as metrics specific to Connect.
  The worker process itself has a number of metrics, while each connector and task have additional metrics.

  <!--#include virtual="generated/connect_metrics.html" -->

  <h4><a id="kafka_streams_monitoring" href="#kafka_streams_monitoring">Streams Monitoring</a></h4>

  A Kafka Streams instance contains all the producer and consumer metrics as well as additional metrics specific to streams.
  By default Kafka Streams has metrics with two recording levels: debug and info. The debug level records all metrics, while
  the info level records only the thread-level metrics.

  <p>
    Note that the metrics have a 3-layer hierarchy. At the top level there are per-thread metrics. Each thread has tasks, with their
    own metrics. Each task has a number of processor nodes, with their own metrics. Each task also has a number of state stores
    and record caches, all with their own metrics.
  </p>
  
  Use the following configuration option to specify which metrics
  you want collected:

<pre>metrics.recording.level="info"</pre>

<h5><a id="kafka_streams_thread_monitoring" href="#kafka_streams_thread_monitoring">Thread Metrics</a></h5>
All the following metrics have a recording level of ``info``:
<table class="data-table">
    <tbody>
      <tr>
        <th>Metric/Attribute name</th>
        <th>Description</th>
        <th>Mbean name</th>
      </tr>
       <tr>
        <td>commit-latency-avg</td>
        <td>The average execution time in ms for committing, across all running tasks of this thread.</td>
        <td>kafka.streams:type=stream-metrics,client-id=([-.\w]+)</td>
       </tr>
        <tr>
        <td>commit-latency-max</td>
        <td>The maximum execution time in ms for committing across all running tasks of this thread.</td>
        <td>kafka.streams:type=stream-metrics,client-id=([-.\w]+)</td>
       </tr>
       <tr>
        <td>poll-latency-avg</td>
        <td>The average execution time in ms for polling, across all running tasks of this thread.</td>
        <td>kafka.streams:type=stream-metrics,client-id=([-.\w]+)</td>
       </tr>
        <tr>
        <td>poll-latency-max</td>
        <td>The maximum execution time in ms for polling across all running tasks of this thread.</td>
        <td>kafka.streams:type=stream-metrics,client-id=([-.\w]+)</td>
       </tr>
       <tr>
        <td>process-latency-avg</td>
        <td>The average execution time in ms for processing, across all running tasks of this thread.</td>
        <td>kafka.streams:type=stream-metrics,client-id=([-.\w]+)</td>
       </tr>
       <tr>
        <td>process-latency-max</td>
        <td>The maximum execution time in ms for processing across all running tasks of this thread.</td>
        <td>kafka.streams:type=stream-metrics,client-id=([-.\w]+)</td>
       </tr>
       <tr>
        <td>punctuate-latency-avg</td>
        <td>The average execution time in ms for punctuating, across all running tasks of this thread.</td>
        <td>kafka.streams:type=stream-metrics,client-id=([-.\w]+)</td>
       </tr>
       <tr>
        <td>punctuate-latency-max</td>
        <td>The maximum execution time in ms for punctuating across all running tasks of this thread.</td>
        <td>kafka.streams:type=stream-metrics,client-id=([-.\w]+)</td>
       </tr>
       <tr>
        <td>commit-rate</td>
        <td>The average number of commits per second across all tasks.</td>
        <td>kafka.streams:type=stream-metrics,client-id=([-.\w]+)</td>
       </tr>
       <tr>
        <td>poll-rate</td>
        <td>The average number of polls per second across all tasks.</td>
        <td>kafka.streams:type=stream-metrics,client-id=([-.\w]+)</td>
       </tr>
       <tr>
        <td>process-rate</td>
        <td>The average number of process calls per second across all tasks.</td>
        <td>kafka.streams:type=stream-metrics,client-id=([-.\w]+)</td>
       </tr>
       <tr>
        <td>punctuate-rate</td>
        <td>The average number of punctuates per second across all tasks.</td>
        <td>kafka.streams:type=stream-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>task-created-rate</td>
        <td>The average number of newly created tasks per second.</td>
        <td>kafka.streams:type=stream-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>task-closed-rate</td>
        <td>The average number of tasks closed per second.</td>
        <td>kafka.streams:type=stream-metrics,client-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>skipped-records-rate</td>
        <td>The average number of skipped records per second. </td>
        <td>kafka.streams:type=stream-metrics,client-id=([-.\w]+)</td>
      </tr>
 </tbody>
</table>

<h5><a id="kafka_streams_task_monitoring" href="#kafka_streams_task_monitoring">Task Metrics</a></h5>
All the following metrics have a recording level of ``debug``:
 <table class="data-table">
      <tbody>
      <tr>
        <th>Metric/Attribute name</th>
        <th>Description</th>
        <th>Mbean name</th>
      </tr>
      <tr>
        <td>commit-latency-avg</td>
        <td>The average commit time in ns for this task. </td>
        <td>kafka.streams:type=stream-task-metrics,client-id=([-.\w]+),task-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>commit-latency-max</td>
        <td>The maximum commit time in ns for this task. </td>
        <td>kafka.streams:type=stream-task-metrics,client-id=([-.\w]+),task-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>commit-rate</td>
        <td>The average number of commit calls per second. </td>
        <td>kafka.streams:type=stream-task-metrics,client-id=([-.\w]+),task-id=([-.\w]+)</td>
      </tr>
 </tbody>
</table>

 <h5><a id="kafka_streams_node_monitoring" href="#kafka_streams_node_monitoring">Processor Node Metrics</a></h5>
All the following metrics have a recording level of ``debug``:
 <table class="data-table">
      <tbody>
      <tr>
        <th>Metric/Attribute name</th>
        <th>Description</th>
        <th>Mbean name</th>
      </tr>
      <tr>
        <td>process-latency-avg</td>
        <td>The average process execution time in ns. </td>
        <td>kafka.streams:type=stream-processor-node-metrics,client-id=([-.\w]+),task-id=([-.\w]+),processor-node-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>process-latency-max</td>
        <td>The maximum process execution time in ns. </td>
        <td>kafka.streams:type=stream-processor-node-metrics,client-id=([-.\w]+),task-id=([-.\w]+),processor-node-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>punctuate-latency-avg</td>
        <td>The average punctuate execution time in ns. </td>
        <td>kafka.streams:type=stream-processor-node-metrics,client-id=([-.\w]+),task-id=([-.\w]+),processor-node-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>punctuate-latency-max</td>
        <td>The maximum punctuate execution time in ns. </td>
        <td>kafka.streams:type=stream-processor-node-metrics,client-id=([-.\w]+),task-id=([-.\w]+),processor-node-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>create-latency-avg</td>
        <td>The average create execution time in ns. </td>
        <td>kafka.streams:type=stream-processor-node-metrics,client-id=([-.\w]+),task-id=([-.\w]+),processor-node-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>create-latency-max</td>
        <td>The maximum create execution time in ns. </td>
        <td>kafka.streams:type=stream-processor-node-metrics,client-id=([-.\w]+),task-id=([-.\w]+),processor-node-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>destroy-latency-avg</td>
        <td>The average destroy execution time in ns. </td>
        <td>kafka.streams:type=stream-processor-node-metrics,client-id=([-.\w]+),task-id=([-.\w]+),processor-node-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>destroy-latency-max</td>
        <td>The maximum destroy execution time in ns. </td>
        <td>kafka.streams:type=stream-processor-node-metrics,client-id=([-.\w]+),task-id=([-.\w]+),processor-node-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>process-rate</td>
        <td>The average number of process operations per second. </td>
        <td>kafka.streams:type=stream-processor-node-metrics,client-id=([-.\w]+),task-id=([-.\w]+),processor-node-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>punctuate-rate</td>
        <td>The average number of punctuate operations per second. </td>
        <td>kafka.streams:type=stream-processor-node-metrics,client-id=([-.\w]+),task-id=([-.\w]+),processor-node-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>create-rate</td>
        <td>The average number of create operations per second. </td>
        <td>kafka.streams:type=stream-processor-node-metrics,client-id=([-.\w]+),task-id=([-.\w]+),processor-node-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>destroy-rate</td>
        <td>The average number of destroy operations per second. </td>
        <td>kafka.streams:type=stream-processor-node-metrics,client-id=([-.\w]+),task-id=([-.\w]+),processor-node-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>forward-rate</td>
        <td>The average rate of records being forwarded downstream, from source nodes only, per second. </td>
        <td>kafka.streams:type=stream-processor-node-metrics,client-id=([-.\w]+),task-id=([-.\w]+),processor-node-id=([-.\w]+)</td>
      </tr>
 </tbody>
 </table>

 <h5><a id="kafka_streams_store_monitoring" href="#kafka_streams_store_monitoring">State Store Metrics</a></h5>
All the following metrics have a recording level of ``debug``:

 <table class="data-table">
      <tbody>
      <tr>
        <th>Metric/Attribute name</th>
        <th>Description</th>
        <th>Mbean name</th>
      </tr>
      <tr>
        <td>put-latency-avg</td>
        <td>The average put execution time in ns. </td>
        <td>kafka.streams:type=stream-[store-type]-state-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-type]-state-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>put-latency-max</td>
        <td>The maximum put execution time in ns. </td>
        <td>kafka.streams:type=stream-[store-type]-state-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-type]-state-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>put-if-absent-latency-avg</td>
        <td>The average put-if-absent execution time in ns. </td>
        <td>kafka.streams:type=stream-[store-type]-state-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-type]-state-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>put-if-absent-latency-max</td>
        <td>The maximum put-if-absent execution time in ns. </td>
        <td>kafka.streams:type=stream-[store-type]-state-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-type]-state-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>get-latency-avg</td>
        <td>The average get execution time in ns. </td>
        <td>kafka.streams:type=stream-[store-type]-state-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-type]-state-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>get-latency-max</td>
        <td>The maximum get execution time in ns. </td>
        <td>kafka.streams:type=stream-[store-type]-state-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-type]-state-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>delete-latency-avg</td>
        <td>The average delete execution time in ns. </td>
        <td>kafka.streams:type=stream-[store-type]-state-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-type]-state-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>delete-latency-max</td>
        <td>The maximum delete execution time in ns. </td>
        <td>kafka.streams:type=stream-[store-type]-state-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-type]-state-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>put-all-latency-avg</td>
        <td>The average put-all execution time in ns. </td>
        <td>kafka.streams:type=stream-[store-type]-state-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-type]-state-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>put-all-latency-max</td>
        <td>The maximum put-all execution time in ns. </td>
        <td>kafka.streams:type=stream-[store-type]-state-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-type]-state-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>all-latency-avg</td>
        <td>The average all operation execution time in ns. </td>
        <td>kafka.streams:type=stream-[store-type]-state-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-type]-state-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>all-latency-max</td>
        <td>The maximum all operation execution time in ns. </td>
        <td>kafka.streams:type=stream-[store-type]-state-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-type]-state-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>range-latency-avg</td>
        <td>The average range execution time in ns. </td>
        <td>kafka.streams:type=stream-[store-type]-state-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-type]-state-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>range-latency-max</td>
        <td>The maximum range execution time in ns. </td>
        <td>kafka.streams:type=stream-[store-type]-state-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-type]-state-id=([-.\w]+)</td>
      </tr>
       <tr>
        <td>flush-latency-avg</td>
        <td>The average flush execution time in ns. </td>
        <td>kafka.streams:type=stream-[store-type]-state-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-type]-state-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>flush-latency-max</td>
        <td>The maximum flush execution time in ns. </td>
        <td>kafka.streams:type=stream-[store-type]-state-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-type]-state-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>restore-latency-avg</td>
        <td>The average restore execution time in ns. </td>
        <td>kafka.streams:type=stream-[store-type]-state-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-type]-state-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>restore-latency-max</td>
        <td>The maximum restore execution time in ns. </td>
        <td>kafka.streams:type=stream-[store-type]-state-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-type]-state-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>put-rate</td>
        <td>The average put rate for this store.</td>
        <td>kafka.streams:type=stream-[store-type]-state-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-type]-state-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>put-if-absent-rate</td>
        <td>The average put-if-absent rate for this store.</td>
        <td>kafka.streams:type=stream-[store-type]-state-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-type]-state-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>get-rate</td>
        <td>The average get rate for this store.</td>
        <td>kafka.streams:type=stream-[store-type]-state-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-type]-state-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>delete-rate</td>
        <td>The average delete rate for this store.</td>
        <td>kafka.streams:type=stream-[store-type]-state-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-type]-state-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>put-all-rate</td>
        <td>The average put-all rate for this store.</td>
        <td>kafka.streams:type=stream-[store-type]-state-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-type]-state-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>all-rate</td>
        <td>The average all operation rate for this store.</td>
        <td>kafka.streams:type=stream-[store-type]-state-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-type]-state-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>range-rate</td>
        <td>The average range rate for this store.</td>
        <td>kafka.streams:type=stream-[store-type]-state-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-type]-state-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>flush-rate</td>
        <td>The average flush rate for this store.</td>
        <td>kafka.streams:type=stream-[store-type]-state-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-type]-state-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>restore-rate</td>
        <td>The average restore rate for this store.</td>
        <td>kafka.streams:type=stream-[store-type]-state-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-type]-state-id=([-.\w]+)</td>
      </tr>
    </tbody>
 </table>

  <h5><a id="kafka_streams_cache_monitoring" href="#kafka_streams_cache_monitoring">Record Cache Metrics</a></h5>
All the following metrics have a recording level of ``debug``:

  <table class="data-table">
      <tbody>
      <tr>
        <th>Metric/Attribute name</th>
        <th>Description</th>
        <th>Mbean name</th>
      </tr>
      <tr>
        <td>hitRatio-avg</td>
        <td>The average cache hit ratio defined as the ratio of cache read hits over the total cache read requests. </td>
        <td>kafka.streams:type=stream-record-cache-metrics,client-id=([-.\w]+),task-id=([-.\w]+),record-cache-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>hitRatio-min</td>
        <td>The mininum cache hit ratio. </td>
        <td>kafka.streams:type=stream-record-cache-metrics,client-id=([-.\w]+),task-id=([-.\w]+),record-cache-id=([-.\w]+)</td>
      </tr>
      <tr>
        <td>hitRatio-max</td>
        <td>The maximum cache hit ratio. </td>
        <td>kafka.streams:type=stream-record-cache-metrics,client-id=([-.\w]+),task-id=([-.\w]+),record-cache-id=([-.\w]+)</td>
      </tr>
    </tbody>
 </table>

  <h4><a id="others_monitoring" href="#others_monitoring">Others</a></h4>

  We recommend monitoring GC time and other stats and various server stats such as CPU utilization, I/O service time, etc.

  On the client side, we recommend monitoring the message/byte rate (global and per topic), request rate/size/time, and on the consumer side, max lag in messages among all partitions and min fetch request rate. For a consumer to keep up, max lag needs to be less than a threshold and min fetch rate needs to be larger than 0.

  <h4><a id="basic_ops_audit" href="#basic_ops_audit">Audit</a></h4>
  The final alerting we do is on the correctness of the data delivery. We audit that every message that is sent is consumed by all consumers and measure the lag for this to occur. For important topics we alert if a certain completeness is not achieved in a certain time period. The details of this are discussed in KAFKA-260.

  <h3><a id="zk" href="#zk">6.7 ZooKeeper</a></h3>

  <h4><a id="zkversion" href="#zkversion">Stable version</a></h4>
  The current stable branch is 3.4 and the latest release of that branch is 3.4.9.

  <h4><a id="zkops" href="#zkops">Operationalizing ZooKeeper</a></h4>
  Operationally, we do the following for a healthy ZooKeeper installation:
  <ul>
    <li>Redundancy in the physical/hardware/network layout: try not to put them all in the same rack, decent (but don't go nuts) hardware, try to keep redundant power and network paths, etc. A typical ZooKeeper ensemble has 5 or 7 servers, which tolerates 2 and 3 servers down, respectively. If you have a small deployment, then using 3 servers is acceptable, but keep in mind that you'll only be able to tolerate 1 server down in this case. </li>
    <li>I/O segregation: if you do a lot of write type traffic you'll almost definitely want the transaction logs on a dedicated disk group. Writes to the transaction log are synchronous (but batched for performance), and consequently, concurrent writes can significantly affect performance. ZooKeeper snapshots can be one such a source of concurrent writes, and ideally should be written on a disk group separate from the transaction log. Snapshots are written to disk asynchronously, so it is typically ok to share with the operating system and message log files. You can configure a server to use a separate disk group with the dataLogDir parameter.</li>
    <li>Application segregation: Unless you really understand the application patterns of other apps that you want to install on the same box, it can be a good idea to run ZooKeeper in isolation (though this can be a balancing act with the capabilities of the hardware).</li>
    <li>Use care with virtualization: It can work, depending on your cluster layout and read/write patterns and SLAs, but the tiny overheads introduced by the virtualization layer can add up and throw off ZooKeeper, as it can be very time sensitive</li>
    <li>ZooKeeper configuration: It's java, make sure you give it 'enough' heap space (We usually run them with 3-5G, but that's mostly due to the data set size we have here). Unfortunately we don't have a good formula for it, but keep in mind that allowing for more ZooKeeper state means that snapshots can become large, and large snapshots affect recovery time. In fact, if the snapshot becomes too large (a few gigabytes), then you may need to increase the initLimit parameter to give enough time for servers to recover and join the ensemble.</li>
    <li>Monitoring: Both JMX and the 4 letter words (4lw) commands are very useful, they do overlap in some cases (and in those cases we prefer the 4 letter commands, they seem more predictable, or at the very least, they work better with the LI monitoring infrastructure)</li>
    <li>Don't overbuild the cluster: large clusters, especially in a write heavy usage pattern, means a lot of intracluster communication (quorums on the writes and subsequent cluster member updates), but don't underbuild it (and risk swamping the cluster). Having more servers adds to your read capacity.</li>
  </ul>
  Overall, we try to keep the ZooKeeper system as small as will handle the load (plus standard growth capacity planning) and as simple as possible. We try not to do anything fancy with the configuration or application layout as compared to the official release as well as keep it as self contained as possible. For these reasons, we tend to skip the OS packaged versions, since it has a tendency to try to put things in the OS standard hierarchy, which can be 'messy', for want of a better way to word it.
</script>

<div class="p-ops"></div>
